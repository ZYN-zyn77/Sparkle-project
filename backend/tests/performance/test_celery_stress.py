"""
Celery ä»»åŠ¡é˜Ÿåˆ—å‹åŠ›æµ‹è¯•

æµ‹è¯•ç›®æ ‡:
1. éªŒè¯ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„ç¨³å®šæ€§
2. æµ‹é‡ä»»åŠ¡æ‰§è¡Œååé‡
3. è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
4. éªŒè¯èµ„æºåˆ©ç”¨ç‡

æµ‹è¯•åœºæ™¯:
- åœºæ™¯1: å¿«é€Ÿä»»åŠ¡å¹¶å‘æµ‹è¯• (1000ä¸ªä»»åŠ¡)
- åœºæ™¯2: é•¿æ—¶ä»»åŠ¡å¹¶å‘æµ‹è¯• (100ä¸ªä»»åŠ¡)
- åœºæ™¯3: æ··åˆä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•
- åœºæ™¯4: å¼‚å¸¸å¤„ç†å’Œé‡è¯•æµ‹è¯•
- åœºæ™¯5: å†…å­˜æ³„æ¼æ£€æµ‹

ä½œè€…: Claude Code (Opus 4.5)
åˆ›å»ºæ—¶é—´: 2026-01-03
"""

import asyncio
import time
import psutil
import os
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from loguru import logger

import pytest
from app.core.celery_app import celery_app
from app.core.task_manager import task_manager


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    scenario: str
    task_count: int
    total_time: float
    success_count: int
    failed_count: int
    avg_latency_ms: float
    throughput_tasks_per_sec: float
    memory_usage_mb: float
    cpu_percent: float
    timestamp: str

    @property
    def success_rate(self) -> float:
        if self.task_count == 0:
            return 0.0
        return self.success_count / self.task_count

    def to_dict(self):
        return asdict(self)


class CeleryStressTester:
    """Celery å‹åŠ›æµ‹è¯•å™¨"""

    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
        self.process = psutil.Process(os.getpid())

    def get_system_stats(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        memory_info = self.process.memory_info()
        return {
            "memory_mb": memory_info.rss / 1024 / 1024,
            "cpu_percent": self.process.cpu_percent()
        }

    async def scenario_1_fast_tasks_concurrent(self, task_count: int = 1000) -> PerformanceMetrics:
        """
        åœºæ™¯1: å¿«é€Ÿä»»åŠ¡å¹¶å‘æµ‹è¯•
        æµ‹è¯•ç›®æ ‡: 1000ä¸ªå¿«é€Ÿä»»åŠ¡çš„æ‰§è¡Œååé‡
        """
        logger.info(f"ğŸš€ åœºæ™¯1: å¿«é€Ÿä»»åŠ¡å¹¶å‘æµ‹è¯• ({task_count} ä¸ªä»»åŠ¡)")

        # å¿«é€Ÿä»»åŠ¡å®šä¹‰
        async def quick_task(task_id: int):
            await asyncio.sleep(0.01)  # 10ms
            return f"task_{task_id}_completed"

        start_time = time.time()
        system_start = self.get_system_stats()

        # å¹¶å‘åˆ›å»ºä»»åŠ¡
        tasks = []
        for i in range(task_count):
            task = await task_manager.spawn(
                quick_task(i),
                task_name=f"stress_quick_{i}",
                user_id="stress_test_user"
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        end_time = time.time()
        system_end = self.get_system_stats()

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = task_count - success_count
        total_time = end_time - start_time

        metrics = PerformanceMetrics(
            scenario="å¿«é€Ÿä»»åŠ¡å¹¶å‘æµ‹è¯•",
            task_count=task_count,
            total_time=total_time,
            success_count=success_count,
            failed_count=failed_count,
            avg_latency_ms=(total_time / task_count) * 1000,
            throughput_tasks_per_sec=task_count / total_time,
            memory_usage_mb=system_end["memory_mb"],
            cpu_percent=system_end["cpu_percent"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"âœ… åœºæ™¯1 å®Œæˆ: {metrics.throughput_tasks_per_sec:.2f} tasks/sec")
        return metrics

    async def scenario_2_long_tasks_concurrent(self, task_count: int = 50) -> PerformanceMetrics:
        """
        åœºæ™¯2: é•¿æ—¶ä»»åŠ¡å¹¶å‘æµ‹è¯•
        æµ‹è¯•ç›®æ ‡: 50ä¸ªé•¿æ—¶ä»»åŠ¡çš„å¹¶å‘å¤„ç†èƒ½åŠ›
        """
        logger.info(f"ğŸš€ åœºæ™¯2: é•¿æ—¶ä»»åŠ¡å¹¶å‘æµ‹è¯• ({task_count} ä¸ªä»»åŠ¡)")

        # é•¿æ—¶ä»»åŠ¡å®šä¹‰ (æ¨¡æ‹ŸçœŸå®åœºæ™¯)
        async def long_task(task_id: int):
            await asyncio.sleep(0.5)  # 500ms
            # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
            result = sum(i * i for i in range(1000))
            return f"long_task_{task_id}_result_{result}"

        start_time = time.time()
        system_start = self.get_system_stats()

        # å¹¶å‘åˆ›å»ºä»»åŠ¡
        tasks = []
        for i in range(task_count):
            task = await task_manager.spawn(
                long_task(i),
                task_name=f"stress_long_{i}",
                user_id="stress_test_user"
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        end_time = time.time()
        system_end = self.get_system_stats()

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = task_count - success_count
        total_time = end_time - start_time

        metrics = PerformanceMetrics(
            scenario="é•¿æ—¶ä»»åŠ¡å¹¶å‘æµ‹è¯•",
            task_count=task_count,
            total_time=total_time,
            success_count=success_count,
            failed_count=failed_count,
            avg_latency_ms=(total_time / task_count) * 1000,
            throughput_tasks_per_sec=task_count / total_time,
            memory_usage_mb=system_end["memory_mb"],
            cpu_percent=system_end["cpu_percent"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"âœ… åœºæ™¯2 å®Œæˆ: {metrics.throughput_tasks_per_sec:.2f} tasks/sec")
        return metrics

    async def scenario_3_priority_queues(self) -> PerformanceMetrics:
        """
        åœºæ™¯3: æ··åˆä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•
        æµ‹è¯•ç›®æ ‡: éªŒè¯ä¼˜å…ˆçº§é˜Ÿåˆ—çš„è°ƒåº¦ç­–ç•¥
        """
        logger.info("ğŸš€ åœºæ™¯3: æ··åˆä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•")

        execution_order = []

        async def high_priority_task(task_id: int):
            execution_order.append(f"high_{task_id}")
            await asyncio.sleep(0.05)
            return f"high_{task_id}"

        async def default_priority_task(task_id: int):
            execution_order.append(f"default_{task_id}")
            await asyncio.sleep(0.05)
            return f"default_{task_id}"

        async def low_priority_task(task_id: int):
            execution_order.append(f"low_{task_id}")
            await asyncio.sleep(0.05)
            return f"low_{task_id}"

        start_time = time.time()

        # åˆ›å»ºæ··åˆä»»åŠ¡
        tasks = []

        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡
        for i in range(5):
            task = await task_manager.spawn(
                high_priority_task(i),
                task_name=f"high_prio_{i}",
                user_id="stress_test_user"
            )
            tasks.append(task)

        # é»˜è®¤ä¼˜å…ˆçº§ä»»åŠ¡
        for i in range(5):
            task = await task_manager.spawn(
                default_priority_task(i),
                task_name=f"default_prio_{i}",
                user_id="stress_test_user"
            )
            tasks.append(task)

        # ä½ä¼˜å…ˆçº§ä»»åŠ¡
        for i in range(5):
            task = await task_manager.spawn(
                low_priority_task(i),
                task_name=f"low_prio_{i}",
                user_id="stress_test_user"
            )
            tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

        end_time = time.time()

        metrics = PerformanceMetrics(
            scenario="æ··åˆä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•",
            task_count=15,
            total_time=end_time - start_time,
            success_count=15,
            failed_count=0,
            avg_latency_ms=(end_time - start_time) / 15 * 1000,
            throughput_tasks_per_sec=15 / (end_time - start_time),
            memory_usage_mb=self.get_system_stats()["memory_mb"],
            cpu_percent=self.get_system_stats()["cpu_percent"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"âœ… åœºæ™¯3 å®Œæˆ: æ‰§è¡Œé¡ºåº {execution_order}")
        return metrics

    async def scenario_4_exception_handling(self, task_count: int = 100) -> PerformanceMetrics:
        """
        åœºæ™¯4: å¼‚å¸¸å¤„ç†å’Œé‡è¯•æµ‹è¯•
        æµ‹è¯•ç›®æ ‡: éªŒè¯ç³»ç»Ÿåœ¨ä»»åŠ¡å¤±è´¥æ—¶çš„ç¨³å®šæ€§
        """
        logger.info(f"ğŸš€ åœºæ™¯4: å¼‚å¸¸å¤„ç†æµ‹è¯• ({task_count} ä¸ªä»»åŠ¡)")

        attempt_count = [0]

        async def flaky_task(task_id: int):
            attempt_count[0] += 1
            if attempt_count[0] % 5 == 0:  # æ¯5æ¬¡æˆåŠŸ1æ¬¡
                return f"task_{task_id}_success"
            raise Exception(f"Task {task_id} failed")

        start_time = time.time()

        # åˆ›å»ºå¸¦é‡è¯•çš„ä»»åŠ¡
        tasks = []
        for i in range(task_count):
            task = await task_manager.spawn_with_retry(
                lambda i=i: flaky_task(i),
                task_name=f"flaky_{i}",
                max_retries=3,
                retry_delay=0.05,
                user_id="stress_test_user"
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        end_time = time.time()

        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = task_count - success_count

        metrics = PerformanceMetrics(
            scenario="å¼‚å¸¸å¤„ç†å’Œé‡è¯•æµ‹è¯•",
            task_count=task_count,
            total_time=end_time - start_time,
            success_count=success_count,
            failed_count=failed_count,
            avg_latency_ms=(end_time - start_time) / task_count * 1000,
            throughput_tasks_per_sec=task_count / (end_time - start_time),
            memory_usage_mb=self.get_system_stats()["memory_mb"],
            cpu_percent=self.get_system_stats()["cpu_percent"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"âœ… åœºæ™¯4 å®Œæˆ: æˆåŠŸç‡ {success_count/task_count*100:.1f}%")
        return metrics

    async def scenario_5_memory_leak_detection(self, task_count: int = 1000) -> PerformanceMetrics:
        """
        åœºæ™¯5: å†…å­˜æ³„æ¼æ£€æµ‹
        æµ‹è¯•ç›®æ ‡: éªŒè¯é•¿æ—¶é—´è¿è¡Œæ˜¯å¦å¯¼è‡´å†…å­˜æ³„æ¼
        """
        logger.info(f"ğŸš€ åœºæ™¯5: å†…å­˜æ³„æ¼æ£€æµ‹ ({task_count} ä¸ªä»»åŠ¡)")

        memory_samples = []

        async def memory_intensive_task(task_id: int):
            # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
            data = list(range(1000))
            result = sum(data)
            return result

        start_time = time.time()
        initial_memory = self.get_system_stats()["memory_mb"]

        # åˆ†æ‰¹æ‰§è¡Œä»»åŠ¡ï¼Œç›‘æ§å†…å­˜å˜åŒ–
        batch_size = 100
        for batch in range(0, task_count, batch_size):
            tasks = []
            for i in range(batch, min(batch + batch_size, task_count)):
                task = await task_manager.spawn(
                    memory_intensive_task(i),
                    task_name=f"memory_{i}",
                    user_id="stress_test_user"
                )
                tasks.append(task)

            await asyncio.gather(*tasks, return_exceptions=True)

            # è®°å½•å†…å­˜ä½¿ç”¨
            current_memory = self.get_system_stats()["memory_mb"]
            memory_samples.append(current_memory)
            logger.info(f"  æ‰¹æ¬¡ {batch//batch_size + 1}: å†…å­˜ {current_memory:.2f} MB")

        end_time = time.time()
        final_memory = self.get_system_stats()["memory_mb"]

        # è®¡ç®—å†…å­˜å¢é•¿
        memory_growth = final_memory - initial_memory
        growth_rate = memory_growth / task_count  # MB per task

        metrics = PerformanceMetrics(
            scenario="å†…å­˜æ³„æ¼æ£€æµ‹",
            task_count=task_count,
            total_time=end_time - start_time,
            success_count=task_count,
            failed_count=0,
            avg_latency_ms=(end_time - start_time) / task_count * 1000,
            throughput_tasks_per_sec=task_count / (end_time - start_time),
            memory_usage_mb=final_memory,
            cpu_percent=self.get_system_stats()["cpu_percent"],
            timestamp=datetime.now().isoformat()
        )

        logger.info(f"âœ… åœºæ™¯5 å®Œæˆ: å†…å­˜å¢é•¿ {memory_growth:.2f} MB (é€Ÿç‡: {growth_rate:.4f} MB/task)")
        return metrics

    async def run_all_scenarios(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•åœºæ™¯"""
        logger.info("=" * 60)
        logger.info("ğŸ”¥ å¼€å§‹ Celery å‹åŠ›æµ‹è¯•")
        logger.info("=" * 60)

        results = {}

        # åœºæ™¯1: å¿«é€Ÿä»»åŠ¡å¹¶å‘
        results["scenario_1"] = await self.scenario_1_fast_tasks_concurrent(1000)

        # åœºæ™¯2: é•¿æ—¶ä»»åŠ¡å¹¶å‘
        results["scenario_2"] = await self.scenario_2_long_tasks_concurrent(50)

        # åœºæ™¯3: ä¼˜å…ˆçº§é˜Ÿåˆ—
        results["scenario_3"] = await self.scenario_3_priority_queues()

        # åœºæ™¯4: å¼‚å¸¸å¤„ç†
        results["scenario_4"] = await self.scenario_4_exception_handling(100)

        # åœºæ™¯5: å†…å­˜æ³„æ¼
        results["scenario_5"] = await self.scenario_5_memory_leak_detection(1000)

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(results)

        return results

    def _generate_report(self, results: Dict[str, PerformanceMetrics]):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š å‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 60)

        total_tasks = sum(m.task_count for m in results.values())
        total_time = sum(m.total_time for m in results.values())
        total_success = sum(m.success_count for m in results.values())
        total_failed = sum(m.failed_count for m in results.values())

        logger.info(f"æ€»ä»»åŠ¡æ•°: {total_tasks}")
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}s")
        logger.info(f"æˆåŠŸç‡: {total_success/total_tasks*100:.2f}%")
        logger.info(f"å¹³å‡ååé‡: {total_tasks/total_time:.2f} tasks/sec")

        logger.info("\nè¯¦ç»†ç»“æœ:")
        for name, metrics in results.items():
            logger.info(f"\n{name}:")
            logger.info(f"  ä»»åŠ¡æ•°: {metrics.task_count}")
            logger.info(f"  è€—æ—¶: {metrics.total_time:.2f}s")
            logger.info(f"  ååé‡: {metrics.throughput_tasks_per_sec:.2f} tasks/sec")
            logger.info(f"  æˆåŠŸç‡: {metrics.success_count/metrics.task_count*100:.1f}%")
            logger.info(f"  å†…å­˜: {metrics.memory_usage_mb:.2f} MB")
            logger.info(f"  CPU: {metrics.cpu_percent:.1f}%")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        import json
        report_data = {k: v.to_dict() for k, v in results.items()}
        report_data["summary"] = {
            "total_tasks": total_tasks,
            "total_time": total_time,
            "overall_success_rate": total_success/total_tasks,
            "overall_throughput": total_tasks/total_time
        }

        with open("/tmp/celery_stress_report.json", "w") as f:
            json.dump(report_data, f, indent=2)

        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: /tmp/celery_stress_report.json")


@pytest.mark.performance
@pytest.mark.asyncio
async def test_celery_stress_all_scenarios():
    """è¿è¡Œå®Œæ•´å‹åŠ›æµ‹è¯•"""
    tester = CeleryStressTester()
    results = await tester.run_all_scenarios()

    # éªŒè¯åŸºå‡†æŒ‡æ ‡
    scenario_1 = results["scenario_1"]
    assert scenario_1.throughput_tasks_per_sec > 50, "å¿«é€Ÿä»»åŠ¡ååé‡åº” > 50 tasks/sec"
    assert scenario_1.success_rate > 0.95, "æˆåŠŸç‡åº” > 95%"

    scenario_2 = results["scenario_2"]
    assert scenario_2.success_rate == 1.0, "é•¿æ—¶ä»»åŠ¡æˆåŠŸç‡åº”ä¸º 100%"

    scenario_5 = results["scenario_5"]
    memory_growth = scenario_5.memory_usage_mb - results["scenario_1"].memory_usage_mb
    assert memory_growth < 100, f"å†…å­˜å¢é•¿åº” < 100 MB, å®é™…: {memory_growth:.2f} MB"

    logger.info("âœ… æ‰€æœ‰å‹åŠ›æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_celery_stress_all_scenarios())
