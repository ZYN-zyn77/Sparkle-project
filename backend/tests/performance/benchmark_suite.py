"""
Celery æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶

æä¾›è¯¦ç»†çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”åˆ†æ

ä½œè€…: Claude Code (Opus 4.5)
åˆ›å»ºæ—¶é—´: 2026-01-03
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger

from app.core.celery_app import celery_app
from app.core.task_manager import task_manager


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    name: str
    iterations: int
    mean_ms: float
    median_ms: float
    p95_ms: float
    p99_ms: float
    min_ms: float
    max_ms: float
    std_dev_ms: float
    throughput_per_sec: float
    total_time: float

    def __str__(self):
        return (
            f"{self.name}:\n"
            f"  å¹³å‡: {self.mean_ms:.2f}ms | ä¸­ä½æ•°: {self.median_ms:.2f}ms\n"
            f"  P95: {self.p95_ms:.2f}ms | P99: {self.p99_ms:.2f}ms\n"
            f"  æœ€å°: {self.min_ms:.2f}ms | æœ€å¤§: {self.max_ms:.2f}ms\n"
            f"  æ ‡å‡†å·®: {self.std_dev_ms:.2f}ms | ååé‡: {self.throughput_per_sec:.2f} ops/s"
        )


class BenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    @staticmethod
    def calculate_percentile(data: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * (percentile / 100))
        if index >= len(sorted_data):
            index = len(sorted_data) - 1
        return sorted_data[index]

    @staticmethod
    def calculate_stats(times: List[float], total_time: float, iterations: int) -> BenchmarkResult:
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        return BenchmarkResult(
            name="",
            iterations=iterations,
            mean_ms=statistics.mean(times) * 1000,
            median_ms=statistics.median(times) * 1000,
            p95_ms=BenchmarkSuite.calculate_percentile(times, 95) * 1000,
            p99_ms=BenchmarkSuite.calculate_percentile(times, 99) * 1000,
            min_ms=min(times) * 1000,
            max_ms=max(times) * 1000,
            std_dev_ms=statistics.stdev(times) * 1000 if len(times) > 1 else 0,
            throughput_per_sec=iterations / total_time,
            total_time=total_time
        )

    async def benchmark_task_spawn_overhead(self, iterations: int = 1000) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•: ä»»åŠ¡åˆ›å»ºå¼€é”€"""
        logger.info(f"ğŸ” æµ‹è¯•ä»»åŠ¡åˆ›å»ºå¼€é”€ ({iterations} æ¬¡)")

        async def dummy_task():
            return "done"

        times = []
        start_time = time.time()

        for i in range(iterations):
            task_start = time.time()
            task = await task_manager.spawn(dummy_task(), task_name=f"bench_spawn_{i}")
            await task
            task_end = time.time()
            times.append(task_end - task_start)

        total_time = time.time() - start_time

        result = self.calculate_stats(times, total_time, iterations)
        result.name = "ä»»åŠ¡åˆ›å»ºå¼€é”€"
        return result

    async def benchmark_concurrent_spawn(self, concurrency: int = 100, iterations: int = 1000) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•: å¹¶å‘ä»»åŠ¡åˆ›å»º"""
        logger.info(f"ğŸ” æµ‹è¯•å¹¶å‘ä»»åŠ¡åˆ›å»º ({concurrency} å¹¶å‘, {iterations} æ€»æ•°)")

        async def quick_task(task_id: int):
            await asyncio.sleep(0.001)  # 1ms
            return task_id

        times = []
        start_time = time.time()

        # åˆ†æ‰¹æ‰§è¡Œä»¥æ§åˆ¶å¹¶å‘
        for batch_start in range(0, iterations, concurrency):
            batch_end = min(batch_start + concurrency, iterations)
            batch_size = batch_end - batch_start

            batch_start_time = time.time()
            tasks = []
            for i in range(batch_start, batch_end):
                task = await task_manager.spawn(quick_task(i), task_name=f"bench_conc_{i}")
                tasks.append(task)

            await asyncio.gather(*tasks, return_exceptions=True)
            batch_end_time = time.time()

            # è®°å½•æ¯ä¸ªä»»åŠ¡çš„å¹³å‡æ—¶é—´
            batch_time = batch_end_time - batch_start_time
            avg_task_time = batch_time / batch_size
            times.extend([avg_task_time] * batch_size)

        total_time = time.time() - start_time

        result = self.calculate_stats(times, total_time, iterations)
        result.name = f"å¹¶å‘ä»»åŠ¡åˆ›å»º ({concurrency}å¹¶å‘)"
        return result

    async def benchmark_task_manager_vs_raw_asyncio(self, iterations: int = 500) -> Dict[str, BenchmarkResult]:
        """åŸºå‡†æµ‹è¯•: TaskManager vs åŸç”Ÿ asyncio"""
        logger.info(f"ğŸ” æµ‹è¯• TaskManager vs åŸç”Ÿ asyncio ({iterations} æ¬¡)")

        async def test_task():
            await asyncio.sleep(0.01)
            return "result"

        # æµ‹è¯• TaskManager
        tm_times = []
        tm_start = time.time()

        for i in range(iterations):
            task_start = time.time()
            task = await task_manager.spawn(test_task(), task_name=f"tm_bench_{i}")
            await task
            tm_times.append(time.time() - task_start)

        tm_total = time.time() - tm_start

        # æµ‹è¯•åŸç”Ÿ asyncio
        raw_times = []
        raw_start = time.time()

        for i in range(iterations):
            task_start = time.time()
            task = asyncio.create_task(test_task())
            await task
            raw_times.append(time.time() - task_start)

        raw_total = time.time() - raw_start

        tm_result = self.calculate_stats(tm_times, tm_total, iterations)
        tm_result.name = "TaskManager"

        raw_result = self.calculate_stats(raw_times, raw_total, iterations)
        raw_result.name = "åŸç”Ÿ asyncio"

        return {
            "task_manager": tm_result,
            "raw_asyncio": raw_result
        }

    async def benchmark_celery_task_execution(self, iterations: int = 100) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•: Celery ä»»åŠ¡æ‰§è¡Œ"""
        logger.info(f"ğŸ” æµ‹è¯• Celery ä»»åŠ¡æ‰§è¡Œ ({iterations} æ¬¡)")

        from app.core.celery_tasks import health_check_task

        times = []
        start_time = time.time()

        for i in range(iterations):
            task_start = time.time()
            result = health_check_task.apply_async()
            # ç­‰å¾…å®Œæˆ
            while not result.ready():
                await asyncio.sleep(0.001)
            task_end = time.time()
            times.append(task_end - task_start)

        total_time = time.time() - start_time

        result = self.calculate_stats(times, total_time, iterations)
        result.name = "Celery ä»»åŠ¡æ‰§è¡Œ"
        return result

    async def benchmark_memory_efficiency(self, iterations: int = 1000) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•: å†…å­˜æ•ˆç‡"""
        logger.info(f"ğŸ” æµ‹è¯•å†…å­˜æ•ˆç‡ ({iterations} æ¬¡)")

        import psutil
        import os

        process = psutil.Process(os.getpid())

        async def memory_task(task_id: int):
            # åˆ›å»ºä¸´æ—¶æ•°æ®
            data = list(range(100))
            result = sum(data)
            return result

        # åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024

        # æ‰§è¡Œä»»åŠ¡
        tasks = []
        for i in range(iterations):
            task = await task_manager.spawn(memory_task(i), task_name=f"mem_bench_{i}")
            tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

        # æœ€ç»ˆå†…å­˜
        final_memory = process.memory_info().rss / 1024 / 1024

        # æ¸…ç†ä»»åŠ¡ç»Ÿè®¡ (ä¿ç•™æœ€è¿‘100ä¸ª)
        task_manager._stats = {
            k: v for k, v in list(task_manager._stats.items())[-100:]
        }

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()

        post_cleanup_memory = process.memory_info().rss / 1024 / 1024

        return {
            "name": "å†…å­˜æ•ˆç‡",
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "peak_memory_mb": final_memory,
            "memory_growth_mb": final_memory - initial_memory,
            "post_cleanup_memory_mb": post_cleanup_memory,
            "cleanup_freed_mb": final_memory - post_cleanup_memory,
            "memory_per_task_mb": (final_memory - initial_memory) / iterations
        }

    async def benchmark_queue_performance(self) -> Dict[str, BenchmarkResult]:
        """åŸºå‡†æµ‹è¯•: é˜Ÿåˆ—æ€§èƒ½"""
        logger.info("ğŸ” æµ‹è¯•é˜Ÿåˆ—æ€§èƒ½")

        # æµ‹è¯•ä¸åŒé˜Ÿåˆ—çš„æ€§èƒ½
        queue_results = {}

        for queue_name, queue_desc in [("high_priority", "é«˜ä¼˜å…ˆçº§"), ("default", "é»˜è®¤"), ("low_priority", "ä½ä¼˜å…ˆçº§")]:
            times = []
            start_time = time.time()

            async def queue_task():
                await asyncio.sleep(0.01)
                return "done"

            # ä½¿ç”¨ Celery ç›´æ¥æµ‹è¯•é˜Ÿåˆ—
            for i in range(100):
                task_start = time.time()
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ä½¿ç”¨ Celery çš„é˜Ÿåˆ—æœºåˆ¶
                task = await task_manager.spawn(queue_task(), task_name=f"queue_{queue_name}_{i}")
                await task
                times.append(time.time() - task_start)

            total_time = time.time() - start_time
            result = self.calculate_stats(times, total_time, 100)
            result.name = f"é˜Ÿåˆ— {queue_desc}"
            queue_results[queue_name] = result

        return queue_results

    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        logger.info("=" * 60)
        logger.info("ğŸ¯ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        logger.info("=" * 60)

        results = {}

        # 1. ä»»åŠ¡åˆ›å»ºå¼€é”€
        results["spawn_overhead"] = await self.benchmark_task_spawn_overhead(1000)

        # 2. å¹¶å‘ä»»åŠ¡åˆ›å»º
        results["concurrent_spawn_50"] = await self.benchmark_concurrent_spawn(50, 500)
        results["concurrent_spawn_100"] = await self.benchmark_concurrent_spawn(100, 1000)

        # 3. TaskManager vs Raw Asyncio
        results["comparison"] = await self.benchmark_task_manager_vs_raw_asyncio(500)

        # 4. Celery ä»»åŠ¡æ‰§è¡Œ
        results["celery_execution"] = await self.benchmark_celery_task_execution(100)

        # 5. å†…å­˜æ•ˆç‡
        results["memory"] = await self.benchmark_memory_efficiency(500)

        # 6. é˜Ÿåˆ—æ€§èƒ½
        results["queues"] = await self.benchmark_queue_performance()

        # ç”ŸæˆæŠ¥å‘Š
        self._print_report(results)

        return results

    def _print_report(self, results: Dict[str, Any]):
        """æ‰“å°åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 60)

        # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
        logger.info("\nã€åŸºç¡€æ€§èƒ½æŒ‡æ ‡ã€‘")
        logger.info(str(results["spawn_overhead"]))
        logger.info("")
        logger.info(str(results["concurrent_spawn_100"]))
        logger.info("")

        # å¹¶å‘å¯¹æ¯”
        logger.info("ã€å¹¶å‘æ€§èƒ½å¯¹æ¯”ã€‘")
        logger.info(f"50å¹¶å‘: {results['concurrent_spawn_50'].throughput_per_sec:.2f} ops/s")
        logger.info(f"100å¹¶å‘: {results['concurrent_spawn_100'].throughput_per_sec:.2f} ops/s")
        logger.info("")

        # TaskManager vs Raw Asyncio
        logger.info("ã€TaskManager vs åŸç”Ÿ asyncioã€‘")
        tm = results["comparison"]["task_manager"]
        raw = results["comparison"]["raw_asyncio"]
        overhead = ((tm.mean_ms - raw.mean_ms) / raw.mean_ms) * 100
        logger.info(f"TaskManager: {tm.mean_ms:.2f}ms (ååé‡: {tm.throughput_per_sec:.2f} ops/s)")
        logger.info(f"åŸç”Ÿ asyncio: {raw.mean_ms:.2f}ms (ååé‡: {raw.throughput_per_sec:.2f} ops/s)")
        logger.info(f"å¼€é”€: {overhead:.1f}%")
        logger.info("")

        # Celery æ‰§è¡Œ
        logger.info("ã€Celery ä»»åŠ¡æ‰§è¡Œã€‘")
        logger.info(str(results["celery_execution"]))
        logger.info("")

        # å†…å­˜æ•ˆç‡
        logger.info("ã€å†…å­˜æ•ˆç‡ã€‘")
        mem = results["memory"]
        logger.info(f"åˆå§‹å†…å­˜: {mem['initial_memory_mb']:.2f} MB")
        logger.info(f"å³°å€¼å†…å­˜: {mem['peak_memory_mb']:.2f} MB")
        logger.info(f"å†…å­˜å¢é•¿: {mem['memory_growth_mb']:.2f} MB")
        logger.info(f"æ¯ä»»åŠ¡å¢é•¿: {mem['memory_per_task_mb']:.4f} MB")
        logger.info(f"æ¸…ç†å: {mem['post_cleanup_memory_mb']:.2f} MB (é‡Šæ”¾: {mem['cleanup_freed_mb']:.2f} MB)")
        logger.info("")

        # é˜Ÿåˆ—æ€§èƒ½
        logger.info("ã€é˜Ÿåˆ—æ€§èƒ½ã€‘")
        for queue_name, result in results["queues"].items():
            logger.info(f"{result.name}: {result.throughput_per_sec:.2f} ops/s, P95: {result.p95_ms:.2f}ms")
        logger.info("")

        # æ€§èƒ½å»ºè®®
        logger.info("ã€æ€§èƒ½å»ºè®®ã€‘")
        if overhead > 20:
            logger.warning("âš ï¸  TaskManager å¼€é”€è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–")
        else:
            logger.info("âœ… TaskManager å¼€é”€åœ¨å¯æ¥å—èŒƒå›´")

        if results["memory"]["memory_growth_mb"] > 50:
            logger.warning("âš ï¸  å†…å­˜å¢é•¿æ˜æ˜¾ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼")
        else:
            logger.info("âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸")

        if results["concurrent_spawn_100"].throughput_per_sec < 100:
            logger.warning("âš ï¸  å¹¶å‘ååé‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ  Worker æ•°é‡")
        else:
            logger.info("âœ… å¹¶å‘æ€§èƒ½è‰¯å¥½")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        import json
        report_file = "/tmp/benchmark_report.json"
        with open(report_file, "w") as f:
            json.dump(results, f, default=str, indent=2)

        logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


if __name__ == "__main__":
    import asyncio
    suite = BenchmarkSuite()
    asyncio.run(suite.run_all_benchmarks())
