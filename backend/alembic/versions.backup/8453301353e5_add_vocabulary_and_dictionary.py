"""add_vocabulary_and_dictionary

Revision ID: 8453301353e5
Revises: 3682fd8c7978
Create Date: 2025-12-23 23:40:41.445998

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
import pgvector.sqlalchemy
from app.models.base import GUID


# revision identifiers, used by Alembic.
revision: str = '8453301353e5'
down_revision: Union[str, None] = '3682fd8c7978'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('dictionary_entries',
    sa.Column('word', sa.String(length=100), nullable=False),
    sa.Column('phonetic', sa.String(length=100), nullable=True),
    sa.Column('pos', sa.String(length=50), nullable=True),
    sa.Column('definitions', sa.JSON(), nullable=False),
    sa.Column('examples', sa.JSON(), nullable=True),
    sa.Column('source', sa.String(length=50), nullable=True),
    sa.Column('id', GUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('dictionary_entries', schema=None) as batch_op:
        batch_op.create_index('idx_dict_word', ['word'], unique=False)
        batch_op.create_index(batch_op.f('ix_dictionary_entries_deleted_at'), ['deleted_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_dictionary_entries_word'), ['word'], unique=True)

    op.create_table('word_books',
    sa.Column('user_id', GUID(), nullable=False),
    sa.Column('word', sa.String(length=100), nullable=False),
    sa.Column('phonetic', sa.String(length=100), nullable=True),
    sa.Column('definition', sa.Text(), nullable=False),
    sa.Column('mastery_level', sa.Integer(), nullable=True),
    sa.Column('next_review_at', sa.DateTime(), nullable=True),
    sa.Column('last_review_at', sa.DateTime(), nullable=True),
    sa.Column('review_count', sa.Integer(), nullable=True),
    sa.Column('context_sentence', sa.Text(), nullable=True),
    sa.Column('source_task_id', GUID(), nullable=True),
    sa.Column('tags', sa.JSON(), nullable=True),
    sa.Column('id', GUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('deleted_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['source_task_id'], ['tasks.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('user_id', 'word', name='uq_user_word')
    )
    with op.batch_alter_table('word_books', schema=None) as batch_op:
        batch_op.create_index('idx_wordbook_review', ['user_id', 'next_review_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_word_books_deleted_at'), ['deleted_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_word_books_user_id'), ['user_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_word_books_word'), ['word'], unique=False)

    with op.batch_alter_table('cognitive_fragments', schema=None) as batch_op:
        batch_op.alter_column('embedding',
               existing_type=sa.NUMERIC(precision=1536),
               type_=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
               existing_nullable=True)

    with op.batch_alter_table('knowledge_nodes', schema=None) as batch_op:
        batch_op.alter_column('embedding',
               existing_type=sa.NUMERIC(precision=1536),
               type_=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
               existing_nullable=True)

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('knowledge_nodes', schema=None) as batch_op:
        batch_op.alter_column('embedding',
               existing_type=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
               type_=sa.NUMERIC(precision=1536),
               existing_nullable=True)

    with op.batch_alter_table('cognitive_fragments', schema=None) as batch_op:
        batch_op.alter_column('embedding',
               existing_type=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
               type_=sa.NUMERIC(precision=1536),
               existing_nullable=True)

    with op.batch_alter_table('word_books', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_word_books_word'))
        batch_op.drop_index(batch_op.f('ix_word_books_user_id'))
        batch_op.drop_index(batch_op.f('ix_word_books_deleted_at'))
        batch_op.drop_index('idx_wordbook_review')

    op.drop_table('word_books')
    with op.batch_alter_table('dictionary_entries', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_dictionary_entries_word'))
        batch_op.drop_index(batch_op.f('ix_dictionary_entries_deleted_at'))
        batch_op.drop_index('idx_dict_word')

    op.drop_table('dictionary_entries')
    # ### end Alembic commands ###
