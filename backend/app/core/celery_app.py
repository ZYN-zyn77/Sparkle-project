"""
Sparkle Celery åº”ç”¨é…ç½®

æä¾›åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—,ç”¨äºå¤„ç†é•¿æ—¶ä»»åŠ¡:
- Embedding ç”Ÿæˆ
- æ‰¹é‡é”™é¢˜åˆ†æ
- æ•°æ®æ¸…ç†
- å®šæ—¶ä»»åŠ¡

ä½œè€…: Claude Code (Opus 4.5)
åˆ›å»ºæ—¶é—´: 2026-01-03
"""

import os
from celery import Celery
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

# =============================================================================
# Celery é…ç½®
# =============================================================================

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/1")
CELERY_BROKER_URL = os.getenv("CELERY_BROKER_URL", REDIS_URL)
CELERY_RESULT_BACKEND = os.getenv("CELERY_RESULT_BACKEND", REDIS_URL)

# Celery åº”ç”¨å®ä¾‹
celery_app = Celery(
    "sparkle",
    broker=CELERY_BROKER_URL,
    backend=CELERY_RESULT_BACKEND,
    include=[
        "app.core.celery_tasks",
    ]
)

# é…ç½®
celery_app.conf.update(
    # ä»»åŠ¡åºåˆ—åŒ–
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",

    # æ—¶åŒº
    timezone="Asia/Shanghai",
    enable_utc=True,

    # ä»»åŠ¡é…ç½®
    task_track_started=True,
    task_send_sent_event=True,

    # é‡è¯•é…ç½®
    task_reject_on_worker_lost=True,
    task_acks_late=True,

    # é˜Ÿåˆ—é…ç½®
    task_queues={
        "high_priority": {
            "exchange": "sparkle",
            "routing_key": "high",
            "priority": 0,
        },
        "default": {
            "exchange": "sparkle",
            "routing_key": "default",
            "priority": 5,
        },
        "low_priority": {
            "exchange": "sparkle",
            "routing_key": "low",
            "priority": 10,
        },
    },

    # é»˜è®¤è·¯ç”±
    task_routes={
        "app.core.celery_tasks.generate_embedding": {"queue": "high_priority"},
        "app.core.celery_tasks.batch_error_analysis": {"queue": "default"},
        "app.core.celery_tasks.cleanup_old_data": {"queue": "low_priority"},
    },

    # ç›‘æ§
    worker_send_task_events=True,

    # ç»“æœè¿‡æœŸæ—¶é—´ (24å°æ—¶)
    result_expires=86400,

    # æ—¥å¿—çº§åˆ«
    worker_log_level="INFO",
)

# Ensure tasks are registered when importing celery_app.
from app.core import celery_tasks  # noqa: F401

# =============================================================================
# ä»»åŠ¡å®šä¹‰
# =============================================================================

@celery_app.task(bind=True, max_retries=3, name="generate_embedding")
def generate_embedding(self, node_id: str, text: str, user_id: Optional[str] = None):
    """
    ç”ŸæˆèŠ‚ç‚¹ Embedding (é•¿æ—¶ä»»åŠ¡)

    Args:
        node_id: èŠ‚ç‚¹ID
        text: è¦ç”Ÿæˆembeddingçš„æ–‡æœ¬
        user_id: ç”¨æˆ·ID (ç”¨äºè¿½è¸ª)

    Returns:
        dict: åŒ…å«embeddingå’ŒçŠ¶æ€
    """
    import asyncio
    from app.db.session import AsyncSessionLocal
    from app.services.embedding_service import embedding_service
    from app.models.galaxy import KnowledgeNode
    from loguru import logger

    async def _generate():
        async with AsyncSessionLocal() as session:
            try:
                # ç”Ÿæˆ embedding
                embedding = await embedding_service.get_embedding(text)

                # æ›´æ–°èŠ‚ç‚¹
                node = await session.get(KnowledgeNode, node_id)
                if node:
                    node.embedding = embedding
                    session.add(node)
                    await session.commit()

                    logger.info(f"âœ… Celery: Generated embedding for node {node_id}")
                    return {
                        "status": "success",
                        "node_id": node_id,
                        "embedding_length": len(embedding)
                    }
                else:
                    raise ValueError(f"Node {node_id} not found")

            except Exception as e:
                logger.error(f"âŒ Celery: Failed to generate embedding for {node_id}: {e}")
                raise

    try:
        return asyncio.run(_generate())
    except Exception as exc:
        logger.error(f"Task failed, attempt {self.request.retries + 1}: {exc}")
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)


@celery_app.task(bind=True, max_retries=3, name="batch_error_analysis")
def batch_error_analysis(self, error_ids: List[str], user_id: str):
    """
    æ‰¹é‡é”™é¢˜åˆ†æ

    Args:
        error_ids: é”™é¢˜IDåˆ—è¡¨
        user_id: ç”¨æˆ·ID

    Returns:
        dict: åˆ†æç»“æœç»Ÿè®¡
    """
    import asyncio
    from uuid import UUID
    from app.db.session import AsyncSessionLocal
    from app.services.error_book_service import ErrorBookService
    from loguru import logger

    async def _analyze():
        async with AsyncSessionLocal() as session:
            service = ErrorBookService(session)
            results = []

            for error_id in error_ids:
                try:
                    error_uuid = UUID(error_id)
                    await service.analyze_and_link(error_uuid, UUID(user_id))
                    results.append({"error_id": error_id, "status": "success"})
                    logger.info(f"âœ… Celery: Analyzed error {error_id}")
                except Exception as e:
                    results.append({"error_id": error_id, "status": "failed", "error": str(e)})
                    logger.error(f"âŒ Celery: Failed to analyze error {error_id}: {e}")

            return {
                "total": len(error_ids),
                "success": sum(1 for r in results if r["status"] == "success"),
                "failed": sum(1 for r in results if r["status"] == "failed"),
                "results": results
            }

    try:
        return asyncio.run(_analyze())
    except Exception as exc:
        logger.error(f"Batch analysis failed: {exc}")
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)


@celery_app.task(bind=True, max_retries=3, name="cleanup_old_data")
def cleanup_old_data(self, days_to_keep: int = 30):
    """
    æ¸…ç†æ—§æ•°æ® (å®šæ—¶ä»»åŠ¡)

    Args:
        days_to_keep: ä¿ç•™å¤©æ•°

    Returns:
        dict: æ¸…ç†ç»Ÿè®¡
    """
    import asyncio
    from datetime import datetime, timedelta
    from app.db.session import AsyncSessionLocal
    from app.models.idempotency_key import IdempotencyKey
    from loguru import logger

    async def _cleanup():
        async with AsyncSessionLocal() as session:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)

            # æ¸…ç†è¿‡æœŸå¹‚ç­‰é”®
            result = await session.execute(
                IdempotencyKey.__table__.delete().where(
                    IdempotencyKey.created_at < cutoff_date
                )
            )
            deleted = result.rowcount

            await session.commit()

            logger.info(f"âœ… Celery: Cleaned up {deleted} old records")
            return {
                "status": "success",
                "deleted_records": deleted,
                "cutoff_date": cutoff_date.isoformat()
            }

    try:
        return asyncio.run(_cleanup())
    except Exception as exc:
        logger.error(f"Cleanup failed: {exc}")
        raise self.retry(exc=exc, countdown=60)


@celery_app.task(bind=True, max_retries=2, name="notify_user")
def notify_user(self, user_id: str, message: str, notification_type: str = "system"):
    """
    å‘é€ç”¨æˆ·é€šçŸ¥ (å¼‚æ­¥é€šçŸ¥)

    Args:
        user_id: ç”¨æˆ·ID
        message: æ¶ˆæ¯å†…å®¹
        notification_type: é€šçŸ¥ç±»å‹
    """
    import asyncio
    from app.db.session import AsyncSessionLocal
    from app.services.notification_service import NotificationService
    from loguru import logger

    async def _notify():
        async with AsyncSessionLocal() as session:
            service = NotificationService(session)
            try:
                await service.create_system_notification(
                    user_id=user_id,
                    message=message,
                    notification_type=notification_type
                )
                logger.info(f"âœ… Celery: Notification sent to {user_id}")
                return {"status": "success", "user_id": user_id}
            except Exception as e:
                logger.error(f"âŒ Celery: Failed to notify {user_id}: {e}")
                raise

    try:
        return asyncio.run(_notify())
    except Exception as exc:
        raise self.retry(exc=exc, countdown=10)


@celery_app.task(bind=True, name="daily_report")
def daily_report(self):
    """
    ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š (å®šæ—¶ä»»åŠ¡)

    Returns:
        dict: æŠ¥å‘Šæ‘˜è¦
    """
    import asyncio
    from app.db.session import AsyncSessionLocal
    from app.services.dashboard_service import DashboardService
    from loguru import logger

    async def _generate():
        async with AsyncSessionLocal() as session:
            service = DashboardService(session)
            try:
                # ç”ŸæˆæŠ¥å‘Š
                report = await service.generate_daily_report()
                logger.info("âœ… Celery: Daily report generated")
                return report
            except Exception as e:
                logger.error(f"âŒ Celery: Failed to generate daily report: {e}")
                raise

    try:
        return asyncio.run(_generate())
    except Exception as exc:
        raise self.retry(exc=exc, countdown=300)


# =============================================================================
# å‘¨æœŸä»»åŠ¡ (Beat Schedule)
# =============================================================================

celery_app.conf.beat_schedule = {
    # æ¯å¤©å‡Œæ™¨2ç‚¹æ¸…ç†æ—§æ•°æ®
    "cleanup-every-day": {
        "task": "cleanup_old_data",
        "schedule": 86400.0,  # 24å°æ—¶
        "args": (30,),  # ä¿ç•™30å¤©
        "options": {"queue": "low_priority"}
    },

    # æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆæ—¥æŠ¥
    "daily-report": {
        "task": "daily_report",
        "schedule": 86400.0,
        "args": (),
        "options": {"queue": "default"}
    },

    # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ç³»ç»Ÿå¥åº·
    "health-check": {
        "task": "app.core.celery_tasks.health_check_task",
        "schedule": 3600.0,
        "options": {"queue": "low_priority"}
    },
}


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def get_celery_status():
    """è·å– Celery çŠ¶æ€"""
    try:
        # æ£€æŸ¥ Broker è¿æ¥
        celery_app.broker_connection().ensure_connection(max_retries=1)

        # è·å–æ´»åŠ¨ worker
        inspect = celery_app.control.inspect()
        active_workers = inspect.active() or {}

        return {
            "status": "healthy",
            "broker": "connected",
            "active_workers": len(active_workers),
            "scheduled_tasks": len(celery_app.conf.beat_schedule),
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "broker": "disconnected",
            "error": str(e)
        }


def schedule_long_task(task_name: str, args: tuple = (), kwargs: dict = None, queue: str = "default"):
    """
    è°ƒåº¦é•¿æ—¶ä»»åŠ¡

    Args:
        task_name: ä»»åŠ¡åç§°
        args: ä»»åŠ¡å‚æ•°
        kwargs: ä»»åŠ¡å…³é”®å­—å‚æ•°
        queue: é˜Ÿåˆ—åç§°

    Returns:
        task_id: ä»»åŠ¡ID
    """
    if kwargs is None:
        kwargs = {}

    try:
        task = celery_app.send_task(
            task_name,
            args=args,
            kwargs=kwargs,
            queue=queue
        )
    except Exception as exc:
        raise RuntimeError(f"Broker connection error: {exc}") from exc

    logger.info(f"ğŸ“… Scheduled task: {task_name} (ID: {task.id}, Queue: {queue})")
    return task.id


def get_task_result(task_id: str, timeout: float = 10.0):
    """
    è·å–ä»»åŠ¡ç»“æœ

    Args:
        task_id: ä»»åŠ¡ID
        timeout: ç­‰å¾…è¶…æ—¶(ç§’)

    Returns:
        dict: ä»»åŠ¡ç»“æœ
    """
    from celery.result import AsyncResult

    result = AsyncResult(task_id, app=celery_app)

    if result.ready():
        return {
            "status": result.status,
            "result": result.result,
            "ready": True
        }
    else:
        return {
            "status": result.status,
            "result": None,
            "ready": False
        }


# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

if __name__ == "__main__":
    print("Celery é…ç½®ç¤ºä¾‹")
    print("=" * 60)

    # æ£€æŸ¥é…ç½®
    status = get_celery_status()
    print(f"çŠ¶æ€: {status}")

    # ç¤ºä¾‹: è°ƒåº¦ä»»åŠ¡
    print("\nç¤ºä¾‹ä»»åŠ¡è°ƒåº¦:")
    print("  1. generate_embedding('node_123', 'å­¦ä¹ å†…å®¹æ ‡é¢˜\\nè¯¦ç»†æ‘˜è¦')")
    print("  2. batch_error_analysis(['error_1', 'error_2'], 'user_123')")
    print("  3. cleanup_old_data(30)")
    print("  4. notify_user('user_123', 'æ‚¨çš„åˆ†æå·²å®Œæˆ')")

    # æŸ¥çœ‹å‘¨æœŸä»»åŠ¡
    print("\nå‘¨æœŸä»»åŠ¡:")
    for name, config in celery_app.conf.beat_schedule.items():
        print(f"  - {name}: {config['task']} (æ¯ {config['schedule']}ç§’)")
