from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from uuid import UUID
from fastapi import HTTPException
from app.core.ingestion.ingestion_service import ingestion_service
from loguru import logger
from app.core.cache import cache_service
from app.models.galaxy import KnowledgeNode
from app.models.file_storage import StoredFile
from sqlalchemy import select, func
from app.config.phase5_config import phase5_config, get_quality_threshold_for_doc_type
import asyncio
import re
import unicodedata

@dataclass
class VectorChunk:
    content: str
    page_numbers: List[int]  # Changed to list
    section_title: Optional[str]
    metadata: Dict = field(default_factory=dict)
    ocr_confidence: Optional[float] = None  # 0.0-1.0, None if not from OCR

@dataclass
class QualityResult:
    passed: bool
    score: float
    issues: List[str]

class DocumentService:
    # ... existing methods ...

    def _detect_document_type(self, chunks: List[VectorChunk]) -> str:
        """
        æ£€æµ‹æ–‡æ¡£ç±»å‹
        Returns: "academic" | "invoice" | "general" | "code"
        """
        if not chunks:
            return "general"

        sample_text = " ".join([c.content[:200] for c in chunks[:3]])

        # å­¦æœ¯è®ºæ–‡ç‰¹å¾
        academic_keywords = ["abstract", "introduction", "conclusion", "references", "doi:", "arxiv"]
        academic_score = sum(1 for kw in academic_keywords if kw.lower() in sample_text.lower())

        # ä»£ç ç‰¹å¾
        code_patterns = [r'\bdef\s+\w+', r'\bclass\s+\w+', r'\bimport\s+\w+', r'function\s*\(']
        code_score = sum(1 for pattern in code_patterns if re.search(pattern, sample_text))

        # å‘ç¥¨ç‰¹å¾
        invoice_keywords = ["invoice", "bill", "amount", "tax", "total", "å‘ç¥¨", "é‡‘é¢"]
        invoice_score = sum(1 for kw in invoice_keywords if kw.lower() in sample_text.lower())

        if academic_score >= 2:
            return "academic"
        elif code_score >= 2:
            return "code"
        elif invoice_score >= 2:
            return "invoice"
        else:
            return "general"

    def _check_garbled_content(self, text: str, doc_type: str) -> tuple[float, List[str]]:
        """
        æ£€æŸ¥ä¹±ç å†…å®¹
        Returns: (garbled_ratio, issues)
        """
        issues = []

        if not text:
            return 1.0, ["Empty content"]

        # 1. ç»Ÿè®¡éæ‰“å°å­—ç¬¦
        non_printable_count = sum(
            1 for char in text
            if not char.isprintable() and char not in ['\n', '\t', '\r']
        )

        # 2. ç»Ÿè®¡æ›¿æ¢å­—ç¬¦ï¼ˆå¸¸è§ä¹±ç æ ‡å¿—ï¼‰
        replacement_chars = ['ï¿½', '\ufffd', 'â–¡', 'â–¯']
        replacement_count = sum(text.count(char) for char in replacement_chars)

        # 3. ç»Ÿè®¡è¿ç»­ä¹±ç å­—ç¬¦
        max_consecutive_garbled = 0
        current_consecutive = 0

        for char in text:
            if char in replacement_chars or (not char.isprintable() and char not in ['\n', '\t', '\r']):
                current_consecutive += 1
                max_consecutive_garbled = max(max_consecutive_garbled, current_consecutive)
            else:
                current_consecutive = 0

        if max_consecutive_garbled > phase5_config.DOC_QUALITY_MAX_CONSECUTIVE_GARBLED:
            issues.append(
                f"Found {max_consecutive_garbled} consecutive garbled characters"
            )

        # 4. è®¡ç®—ä¹±ç ç‡
        total_suspicious = non_printable_count + replacement_count
        garbled_ratio = total_suspicious / len(text) if len(text) > 0 else 0

        # 5. é’ˆå¯¹æ–‡æ¡£ç±»å‹è°ƒæ•´åˆ¤æ–­
        if doc_type == "academic" and phase5_config.DOC_QUALITY_MATH_SYMBOLS_ALLOWED:
            # å­¦æœ¯è®ºæ–‡å…è®¸æ•°å­¦ç¬¦å·ï¼Œé™ä½ä¹±ç ç‡æƒé‡
            garbled_ratio *= 0.7

        threshold = get_quality_threshold_for_doc_type(doc_type)

        if garbled_ratio > threshold:
            issues.append(
                f"High garbled ratio ({garbled_ratio:.2%}) exceeds threshold "
                f"({threshold:.2%}) for {doc_type} documents"
            )

        return garbled_ratio, issues

    def _check_language_consistency(self, text: str) -> tuple[float, List[str]]:
        """
        æ£€æŸ¥è¯­è¨€ä¸€è‡´æ€§ï¼ˆé’ˆå¯¹ä¸­æ–‡æ–‡æ¡£ï¼‰
        Returns: (consistency_score, issues)
        """
        issues = []

        if not text:
            return 0.0, ["Empty content"]

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        # ç»Ÿè®¡å­—æ¯
        latin_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
        # ç»Ÿè®¡æ•°å­—
        digit_chars = sum(1 for char in text if char.isdigit())

        total_meaningful = chinese_chars + latin_chars + digit_chars

        if total_meaningful == 0:
            return 0.0, ["No meaningful characters found"]

        chinese_ratio = chinese_chars / total_meaningful

        # å¦‚æœæ£€æµ‹åˆ°æ˜¯ä¸­æ–‡ä¸ºä¸»çš„æ–‡æ¡£ï¼Œæ£€æŸ¥ä¸­æ–‡æ¯”ä¾‹
        if chinese_ratio > phase5_config.DOC_QUALITY_CHINESE_MIN_RATIO:
            # è¿™æ˜¯ä¸­æ–‡æ–‡æ¡£
            if chinese_ratio < 0.3:
                issues.append(
                    f"Chinese document has low Chinese character ratio: {chinese_ratio:.2%}"
                )
                return chinese_ratio, issues

        return 1.0, []

    def _check_content_structure(self, chunks: List[VectorChunk]) -> tuple[float, List[str]]:
        """
        æ£€æŸ¥å†…å®¹ç»“æ„å®Œæ•´æ€§
        Returns: (structure_score, issues)
        """
        issues = []

        if not chunks:
            return 0.0, ["No chunks"]

        # 1. æ£€æŸ¥åˆ‡ç‰‡é•¿åº¦åˆ†å¸ƒ
        chunk_lengths = [len(c.content) for c in chunks]
        avg_length = sum(chunk_lengths) / len(chunk_lengths)

        # è¿‡çŸ­çš„åˆ‡ç‰‡ï¼ˆå¯èƒ½æ˜¯è¡¨æ ¼ç¢ç‰‡æˆ–è§£æé”™è¯¯ï¼‰
        very_short_chunks = sum(1 for length in chunk_lengths if length < 50)
        short_ratio = very_short_chunks / len(chunks)

        if short_ratio > 0.5:
            issues.append(
                f"Too many short chunks ({short_ratio:.1%}), "
                "may indicate poor OCR or table parsing"
            )

        # 2. æ£€æŸ¥é‡å¤å†…å®¹ï¼ˆé¡µçœ‰é¡µè„šï¼‰
        if len(chunks) > 2:
            # ç®€å•æ£€æŸ¥ï¼šå‰ä¸¤ä¸ªå’Œæœ€åä¸¤ä¸ªåˆ‡ç‰‡æ˜¯å¦é«˜åº¦ç›¸ä¼¼
            first_two = " ".join([c.content[:100] for c in chunks[:2]])
            last_two = " ".join([c.content[:100] for c in chunks[-2:]])

            # ç®€å•ç›¸ä¼¼åº¦ï¼šå…±åŒè¯æ±‡æ•°
            words1 = set(first_two.lower().split())
            words2 = set(last_two.lower().split())

            if len(words1) > 0 and len(words2) > 0:
                similarity = len(words1 & words2) / len(words1 | words2)
                if similarity > 0.8:
                    issues.append("Detected repeated headers/footers across pages")

        # 3. è®¡ç®—ç»“æ„åˆ†æ•°
        structure_score = 1.0 - short_ratio

        return max(structure_score, 0.0), issues

    def _check_ocr_confidence(self, chunks: List[VectorChunk]) -> tuple[float, List[str]]:
        """
        æ£€æŸ¥ OCR ç½®ä¿¡åº¦
        Returns: (avg_confidence, issues)
        """
        issues = []

        # æ”¶é›†æ‰€æœ‰ OCR ç½®ä¿¡åº¦
        ocr_confidences = [
            c.ocr_confidence for c in chunks
            if c.ocr_confidence is not None
        ]

        if not ocr_confidences:
            # æ²¡æœ‰ OCR å†…å®¹ï¼Œè¿”å›æ»¡åˆ†
            return 1.0, []

        avg_confidence = sum(ocr_confidences) / len(ocr_confidences)

        # æ£€æŸ¥æ˜¯å¦ä½äºé˜ˆå€¼
        threshold = phase5_config.DOC_QUALITY_OCR_CONFIDENCE_THRESHOLD

        if avg_confidence < threshold:
            issues.append(
                f"Low OCR confidence ({avg_confidence:.2%}), "
                f"threshold is {threshold:.2%}. "
                f"Document may contain poor scan quality or handwriting."
            )

        # æ£€æŸ¥ä¸ªåˆ«é¡µé¢ç½®ä¿¡åº¦è¿‡ä½
        low_conf_pages = sum(1 for c in ocr_confidences if c < 0.5)
        if low_conf_pages > 0:
            issues.append(
                f"{low_conf_pages} page(s) have very low OCR confidence (<50%)"
            )

        return avg_confidence, issues

    def check_quality(
        self,
        chunks: List[VectorChunk],
        doc_type: Optional[str] = None
    ) -> QualityResult:
        """
        æ”¹è¿›çš„æ–‡æ¡£è´¨é‡æ£€æµ‹

        åˆ†å±‚æ£€æµ‹ç­–ç•¥ï¼š
        1. åŸºç¡€æ£€æŸ¥ï¼šå†…å®¹é•¿åº¦
        2. å­—ç¬¦æ£€æŸ¥ï¼šä¹±ç ç‡
        3. è¯­è¨€æ£€æŸ¥ï¼šä¸­è‹±æ–‡ä¸€è‡´æ€§
        4. ç»“æ„æ£€æŸ¥ï¼šåˆ‡ç‰‡åˆ†å¸ƒã€é‡å¤å†…å®¹
        5. OCR ç½®ä¿¡åº¦æ£€æŸ¥

        Args:
            chunks: æ–‡æ¡£åˆ‡ç‰‡åˆ—è¡¨
            doc_type: æ–‡æ¡£ç±»å‹ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰

        Returns:
            QualityResult: è´¨é‡æ£€æµ‹ç»“æœ
        """
        issues = []

        # 0. åŸºç¡€æ£€æŸ¥
        if not chunks:
            self._report_metrics("unknown", False, 0.0, 0.0, None, ["no_content"])
            return QualityResult(
                passed=False,
                score=0.0,
                issues=["No text content extracted"]
            )

        total_text = " ".join([c.content for c in chunks])
        total_len = len(total_text)

        if total_len < phase5_config.DOC_QUALITY_MIN_LENGTH:
            self._report_metrics("unknown", False, 0.1, 0.0, None, ["too_short"])
            return QualityResult(
                passed=False,
                score=0.1,
                issues=[
                    f"Content too short: {total_len} chars "
                    f"(minimum: {phase5_config.DOC_QUALITY_MIN_LENGTH})"
                ]
            )

        # 1. æ–‡æ¡£ç±»å‹æ£€æµ‹
        if doc_type is None:
            doc_type = self._detect_document_type(chunks)

        logger.info(f"Detected document type: {doc_type}")

        # 2. ä¹±ç æ£€æµ‹
        garbled_ratio, garbled_issues = self._check_garbled_content(total_text, doc_type)
        issues.extend(garbled_issues)

        # 3. è¯­è¨€ä¸€è‡´æ€§æ£€æµ‹
        lang_score, lang_issues = self._check_language_consistency(total_text)
        issues.extend(lang_issues)

        # 4. ç»“æ„å®Œæ•´æ€§æ£€æµ‹
        structure_score, structure_issues = self._check_content_structure(chunks)
        issues.extend(structure_issues)

        # 5. OCR ç½®ä¿¡åº¦æ£€æµ‹
        ocr_confidence, ocr_issues = self._check_ocr_confidence(chunks)
        issues.extend(ocr_issues)

        # 6. ç»¼åˆè¯„åˆ†
        # æƒé‡ï¼šä¹±ç ç‡ 35%ï¼Œè¯­è¨€ä¸€è‡´æ€§ 25%ï¼Œç»“æ„ 25%ï¼ŒOCR ç½®ä¿¡åº¦ 15%
        garbled_score = 1.0 - garbled_ratio
        final_score = (
            0.35 * garbled_score +
            0.25 * lang_score +
            0.25 * structure_score +
            0.15 * ocr_confidence
        )

        # 7. åˆ¤å®šé€šè¿‡æ¡ä»¶
        threshold = get_quality_threshold_for_doc_type(doc_type)
        pass_threshold = 0.7

        # OCR ç½®ä¿¡åº¦è¿‡ä½ä¹Ÿåº”è¯¥å¤±è´¥
        ocr_threshold = phase5_config.DOC_QUALITY_OCR_CONFIDENCE_THRESHOLD
        ocr_passed = ocr_confidence >= ocr_threshold

        passed = (
            final_score >= pass_threshold and
            garbled_ratio <= threshold and
            ocr_passed
        )

        logger.info(
            f"Quality check: score={final_score:.3f}, "
            f"garbled={garbled_ratio:.3f}, "
            f"lang={lang_score:.3f}, "
            f"structure={structure_score:.3f}, "
            f"ocr_conf={ocr_confidence:.3f}, "
            f"passed={passed}"
        )

        # 8. ä¸ŠæŠ¥æŒ‡æ ‡
        issue_types = self._categorize_issues(issues)
        self._report_metrics(
            doc_type=doc_type,
            passed=passed,
            score=final_score,
            garbled_ratio=garbled_ratio,
            ocr_confidence=ocr_confidence if ocr_confidence < 1.0 else None,
            issue_types=issue_types
        )

        return QualityResult(
            passed=passed,
            score=final_score,
            issues=issues if not passed else []
        )

    def _categorize_issues(self, issues: List[str]) -> List[str]:
        """å°†é—®é¢˜åˆ—è¡¨åˆ†ç±»ä¸ºæŒ‡æ ‡æ ‡ç­¾"""
        categories = []
        issue_text = " ".join(issues).lower()

        if "garbled" in issue_text:
            categories.append("garbled")
        if "short" in issue_text:
            categories.append("too_short")
        if "chinese" in issue_text:
            categories.append("low_chinese_ratio")
        if "repeated" in issue_text or "headers" in issue_text:
            categories.append("repeated_headers")
        if "ocr" in issue_text or "confidence" in issue_text:
            categories.append("low_ocr_confidence")

        return categories

    def _report_metrics(
        self,
        doc_type: str,
        passed: bool,
        score: float,
        garbled_ratio: float,
        ocr_confidence: Optional[float],
        issue_types: List[str]
    ) -> None:
        """ä¸ŠæŠ¥æ–‡æ¡£è´¨é‡æŒ‡æ ‡åˆ° Prometheus"""
        try:
            from app.core.metrics import (
                DOC_QUALITY_CHECK_COUNT,
                DOC_QUALITY_SCORE,
                DOC_GARBLED_RATIO,
                DOC_OCR_CONFIDENCE,
                DOC_QUALITY_ISSUES
            )

            # 1. æ£€æµ‹è®¡æ•°
            result = "passed" if passed else "failed"
            DOC_QUALITY_CHECK_COUNT.labels(doc_type=doc_type, result=result).inc()

            # 2. è´¨é‡åˆ†æ•°åˆ†å¸ƒ
            DOC_QUALITY_SCORE.labels(doc_type=doc_type).observe(score)

            # 3. ä¹±ç ç‡åˆ†å¸ƒ
            DOC_GARBLED_RATIO.labels(doc_type=doc_type).observe(garbled_ratio)

            # 4. OCR ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆä»…å½“æœ‰ OCR å†…å®¹æ—¶ï¼‰
            if ocr_confidence is not None:
                DOC_OCR_CONFIDENCE.observe(ocr_confidence)

            # 5. é—®é¢˜ç±»å‹è®¡æ•°
            for issue_type in issue_types:
                DOC_QUALITY_ISSUES.labels(issue_type=issue_type).inc()

        except ImportError:
            # æŒ‡æ ‡æ¨¡å—æœªå®‰è£…ï¼Œé™é»˜è·³è¿‡
            pass
        except Exception as e:
            logger.warning(f"Failed to report quality metrics: {e}")

    async def draft_knowledge_nodes(self, db_session, file_id: UUID, user_id: UUID, chunks: List[VectorChunk]):
        """
        Create draft knowledge nodes from document chunks.
        Strategy: 
        - One root node for the Document.
        - Child nodes for identified sections (if any).
        """
        # 1. Get File Info
        file_record = await db_session.get(StoredFile, file_id)
        if not file_record:
            logger.error(f"File {file_id} not found for drafting")
            return

        # 2. Create Root Node
        root_node = KnowledgeNode(
            name=file_record.file_name,
            description=f"Imported from {file_record.file_name}",
            source_type="document_import",
            source_file_id=file_id,
            status="draft",
            importance_level=3,
            is_seed=False
        )
        db_session.add(root_node)
        await db_session.flush() # Get ID
        
        # 3. Create Section Nodes (Simple Heuristic)
        # Group chunks by section_title
        sections = {}
        for i, chunk in enumerate(chunks):
            title = chunk.section_title or "General"
            if title not in sections:
                sections[title] = []
            sections[title].append(i) # Store chunk index/ref
            
        for title, chunk_indices in sections.items():
            if title == "General" and len(sections) == 1:
                # If only General, link chunks to root
                root_node.chunk_refs = chunk_indices
                continue
                
            # Create child node
            child = KnowledgeNode(
                name=title[:50], # Truncate long titles
                parent_id=root_node.id,
                source_type="document_import",
                source_file_id=file_id,
                status="draft",
                chunk_refs=chunk_indices,
                is_seed=False
            )
            db_session.add(child)
            
        await db_session.commit()
        logger.info(f"Drafted knowledge nodes for file {file_id}")

    # ... existing clean_and_summarize ...

    """
    Service for intelligent document processing:
    - Text Extraction (via IngestionService)
    - Chunked Summarization (Map-Reduce via LLM)
    - Concept Extraction
    """

    async def update_progress(self, task_id: str, status: str, percent: int, result: Any = None):
        """Helper to update task status in Redis"""
        if not task_id: return
        
        data = {
            "status": status,
            "percent": percent,
            "message": status, # redundancy for UI
            "result": result
        }
        # Save for 1 hour
        await cache_service.set(f"task:{task_id}", data, ttl=3600)

    async def clean_and_summarize(self, file_path: str, task_id: str = None, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Main entry point for "Document Cleaning".
        Returns a structured summary designed for both UI display and Agent context.
        """
        options = options or {}
        enable_ocr = options.get("enable_ocr", True)
        # Note: IngestionService currently auto-detects OCR need. We could pass this flag down if we refactor IngestionService further.
        
        try:
            await self.update_progress(task_id, "Reading and parsing document...", 10)

            # 1. Physical Ingestion (OCR, Parsing)
            # This is a synchronous CPU-bound operation, might block event loop if not careful.
            chunks = await asyncio.to_thread(ingestion_service.process_file, file_path)
            
            if not chunks:
                await self.update_progress(task_id, "Failed: No text found", 100, {"error": "No extractable text found."})
                return {"status": "failed", "error": "No extractable text found."}

            await self.update_progress(task_id, "Analyzing structure...", 30)

            # 2. Reconstruct Text & Check Size
            formatted_chunks = []
            total_chars = 0
            current_section = []
            sections = []
            SECTION_LIMIT = 15000 
            
            for chunk in chunks:
                meta = chunk.metadata
                prefix = "## " if meta.get("is_header") else ""
                if meta.get("is_bold"): prefix += "**"
                suffix = "**" if meta.get("is_bold") and not meta.get("is_header") else ""
                
                text = f"{prefix}{chunk.text}{suffix}"
                current_section.append(text)
                total_chars += len(text)
                
                if sum(len(s) for s in current_section) > SECTION_LIMIT:
                    sections.append("\n\n".join(current_section))
                    current_section = []

            if current_section:
                sections.append("\n\n".join(current_section))

            # 3. Process based on size
            if total_chars < 20000:
                await self.update_progress(task_id, "Generating summary...", 60)
                
                # Small file: Return full text and a quick overall summary
                full_text = "\n\n".join(sections)
                summary = await self._generate_quick_summary(full_text)
                
                result = {
                    "status": "completed",
                    "mode": "full_text",
                    "summary": summary,
                    "full_text": full_text,
                    "char_count": total_chars
                }
                await self.update_progress(task_id, "Completed", 100, result)
                return result
            else:
                # Large file: Map-Reduce
                await self.update_progress(task_id, f"Processing {len(sections)} sections (Map-Reduce)...", 50)
                
                document_map = await self._run_map_reduce(sections, task_id)
                
                result = {
                    "status": "completed",
                    "mode": "map_reduce",
                    "summary": document_map, # This is the "compressed" version
                    "full_text_preview": sections[0][:1000] + "...", # Only preview
                    "char_count": total_chars,
                    "section_count": len(sections)
                }
                await self.update_progress(task_id, "Completed", 100, result)
                return result

        except Exception as e:
            logger.error(f"Document cleaning failed: {e}")
            await self.update_progress(task_id, f"Error: {str(e)}", 100, {"error": str(e)})
            return {"status": "error", "error": str(e)}

    async def _generate_quick_summary(self, text: str) -> str:
        """Single-shot summary for small files."""
        try:
            try:
                from app.agents.graph.llm_factory import LLMFactory
                from langchain_core.prompts import ChatPromptTemplate
            except ImportError as exc:
                raise HTTPException(
                    status_code=501,
                    detail="LLM summarization requires langchain dependencies (llm extras)."
                ) from exc

            llm = LLMFactory.get_llm("galaxy_guide", override_model="gpt-4o-mini")
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Summarize this document in markdown. Include Key Concepts and Exam Hints."),
                ("human", f"{text[:20000]}")
            ])
            res = await (prompt | llm).ainvoke({})
            return res.content
        except Exception as e:
            return f"Summary generation failed: {e}"

    async def _run_map_reduce(self, sections: List[str], task_id: str = None) -> str:
        """Execute parallel chunk summarization."""
        sem = asyncio.Semaphore(5)
        total = len(sections)
        completed = 0
        
        async def throttled_extract(i, sec):
            nonlocal completed
            async with sem:
                res = await self._extract_section_summary(i, sec)
                completed += 1
                if task_id:
                    # Update progress proportionally from 50% to 90%
                    progress = 50 + int((completed / total) * 40)
                    await self.update_progress(task_id, f"Analyzing section {completed}/{total}...", progress)
                return res
        
        summaries = await asyncio.gather(*[
            throttled_extract(i, sec) for i, sec in enumerate(sections)
        ])
        
        return "# ğŸ“‚ Document Structure (Compressed)\n\n" + "\n\n".join(summaries)

    async def _extract_section_summary(self, index: int, text: str) -> str:
        try:
            try:
                from app.agents.graph.llm_factory import LLMFactory
                from langchain_core.prompts import ChatPromptTemplate
            except ImportError as exc:
                raise HTTPException(
                    status_code=501,
                    detail="LLM summarization requires langchain dependencies (llm extras)."
                ) from exc

            llm = LLMFactory.get_llm("galaxy_guide", override_model="gpt-4o-mini")
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Analyze this section. Output compact Markdown."),
                ("human", f"""Analyze Part {index+1}.
                Format:
                ### Part {index+1}: [Title]
                - **Concepts**: [Tags]
                - **Summary**: [Content]
                - **Exam Hints**: [Hints]
                
                Text:
                {text[:15000]}
                """)
            ])
            response = await (prompt | llm).ainvoke({})
            return response.content
        except Exception as e:
            return f"### Part {index+1}: Error - {str(e)}"

    async def extract_vector_chunks(
        self,
        file_path: str,
        chunk_size: int = 1200,
        chunk_overlap: int = 200,
    ) -> List[VectorChunk]:
        """
        Extract document chunks suitable for vectorization.
        """
        chunks = await asyncio.to_thread(ingestion_service.process_file, file_path)
        if not chunks:
            return []

        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
        except ImportError as exc:
            raise HTTPException(
                status_code=501,
                detail="Vector chunking requires langchain-text-splitters (llm extras)."
            ) from exc

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        results: List[VectorChunk] = []
        for chunk in chunks:
            text = (chunk.text or "").strip()
            if not text:
                continue
            for piece in splitter.split_text(text):
                content = piece.strip()
                if len(content) < 20:
                    continue
                results.append(VectorChunk(
                    content=content,
                    page_numbers=[chunk.page_num] if chunk.page_num else [],
                    section_title=chunk.metadata.get("title") if chunk.metadata else None,
                    ocr_confidence=chunk.ocr_confidence,
                ))

        return results

document_service = DocumentService()
