import json
from typing import List, Dict, AsyncGenerator, Optional, Any, AsyncIterator
import asyncio
from loguru import logger
from dataclasses import dataclass
from opentelemetry import trace
from fastapi import HTTPException

from app.config import settings
from app.services.llm.base import LLMProvider
from app.services.llm.providers import OpenAICompatibleProvider

# ==========================================
# üé≠ ÊºîÁ§∫Ê®°ÂºèÈ¢ÑËÆæÂìçÂ∫î (Demo Mock Responses)
# ==========================================
# Áî®‰∫éÁ´ûËµõÊºîÁ§∫ÔºåÁ°Æ‰øùÂÖ≥ÈîÆÊµÅÁ®ã 100% ÊàêÂäü‰∏îÁßíÂõû
# Ë¶ÅÂêØÁî®: Âú® .env ‰∏≠ËÆæÁΩÆ DEMO_MODE=true
#
# üí° ‰ΩøÁî®ËØ¥Êòé:
# 1. Âú®ÊºîÁ§∫ËÑöÊú¨‰∏≠ËæìÂÖ•ÁöÑÊñáÂ≠óÂøÖÈ°ª‰∏é‰∏ãÈù¢ÁöÑ key ÂÆåÂÖ®‰∏ÄËá¥
# 2. ÂèØ‰ª•ÊåâÈúÄÊ∑ªÂä†Êõ¥Â§öÂÖ≥ÈîÆËØçÂíåÂìçÂ∫î
# ==========================================

DEMO_MOCK_RESPONSES: Dict[str, str] = {
    "Â∏ÆÊàëÂà∂ÂÆöÈ´òÊï∞Â§ç‰π†ËÆ°Âàí": """Â•ΩÁöÑÔºÅÂü∫‰∫é‰Ω†ÁöÑÂ≠¶‰π†ÊÉÖÂÜµÔºåÊàë‰∏∫‰Ω†Âà∂ÂÆö‰∫Ü‰∏Ä‰∏™È´òÊïàÁöÑÈ´òÊï∞Â§ç‰π†ËÆ°Âàí„ÄÇ

üìö **È´òÊï∞ÂÜ≤Âà∫Â§ç‰π†ËÆ°Âàí**

Ê†πÊçÆËâæÂÆæÊµ©ÊñØÈÅóÂøòÊõ≤Á∫øÂíå‰Ω†ÁöÑÁü•ËØÜÊòüÂõæÂàÜÊûêÔºåÊàëÂèëÁé∞‰Ω†Âú®‰ª•‰∏ãÂá†‰∏™Áü•ËØÜÁÇπÈúÄË¶ÅÈáçÁÇπÂ§ç‰π†Ôºö

1. **ÊûÅÈôê‰∏éËøûÁª≠** - ÊéåÊè°Â∫¶ËæÉ‰ΩéÔºåÂª∫ËÆÆ‰ºòÂÖàÂ§ç‰π†
2. **ÂØºÊï∞ÁöÑÂ∫îÁî®** - ÈúÄË¶ÅÂº∫ÂåñÔºåÁâπÂà´ÊòØÊúÄÂÄºÈóÆÈ¢ò
3. **ÁßØÂàÜËÆ°ÁÆó** - Âü∫Á°ÄËøò‰∏çÈîôÔºåÂÅöÈ¢òÂ∑©Âõ∫Âç≥ÂèØ

ÊàëÂ∑≤‰∏∫‰Ω†ÁîüÊàê‰ª•‰∏ã‰ªªÂä°Âç°ÁâáÔºö

```json
{
  "actions": [
    {
      "type": "create_task",
      "data": {
        "title": "ÊûÅÈôê‰∏éËøûÁª≠ÈáçÈöæÁÇπÂ§ç‰π†",
        "type": "learning",
        "estimated_minutes": 45,
        "priority": "high"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "ÂØºÊï∞Â∫îÁî®‰∏ìÈ¢òÁªÉ‰π†",
        "type": "training",
        "estimated_minutes": 30,
        "priority": "medium"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "ÁßØÂàÜËÆ°ÁÆóÂà∑È¢ò",
        "type": "training",
        "estimated_minutes": 25,
        "priority": "normal"
      }
    }
  ]
}
```

Âª∫ËÆÆÊåâÁÖß‰∏äËø∞È°∫Â∫èÂ≠¶‰π†ÔºåÂÖàÊîªÂÖãÂº±È°πÔºåÂÜçÂ∑©Âõ∫Âº∫È°π„ÄÇÂä†Ê≤πÔºÅüî•""",

    "Êàë‰ªäÂ§©Ë¶ÅÂ≠¶‰ªÄ‰πà": """Êó©‰∏äÂ•ΩÔºÅËÆ©ÊàëÁúãÁúã‰Ω†ÁöÑÂ≠¶‰π†Áä∂ÊÄÅ...

üìä **‰ªäÊó•Â≠¶‰π†Âª∫ËÆÆ**

Ê†πÊçÆ‰Ω†ÁöÑÁü•ËØÜÊòüÂõæÂíåÈÅóÂøòÊõ≤Á∫øÂàÜÊûêÔºö

üî¥ **ÈúÄË¶ÅÂ§ç‰π†** (ÊéåÊè°Â∫¶‰∏ãÈôç):
- Á∫øÊÄß‰ª£Êï∞ÔºöÁü©ÈòµËøêÁÆó (Ë∑ù‰∏äÊ¨°Â≠¶‰π†Â∑≤Ëøá 5 Â§©)
- È´òÊï∞ÔºöÁßØÂàÜÊäÄÂ∑ß (ÊéåÊè°Â∫¶ÈôçËá≥ 65%)

üü° **‰ªäÊó•Êé®ËçêÂ≠¶‰π†**:
- Ê¶ÇÁéáËÆ∫ÔºöÊù°‰ª∂Ê¶ÇÁéá (ÊåâËÆ°ÂàíÂ∫î‰ªäÊó•Â≠¶‰π†)

üí° ÊàëÂª∫ËÆÆ‰Ω†‰ªäÂ§©ÂÖàËä± 20 ÂàÜÈíüÂ§ç‰π†Á∫ø‰ª£Áü©ÈòµËøêÁÆóÔºåÁÑ∂ÂêéÂÜçÂ≠¶‰π†Êñ∞ÂÜÖÂÆπ„ÄÇ

ÈúÄË¶ÅÊàëÂ∏Æ‰Ω†ÂàõÂª∫‰ªäÊó•Â≠¶‰π†‰ªªÂä°ÂêóÔºü""",

    "ËøôÈÅìÈ¢òÊÄé‰πàÂÅö": """Â•ΩÁöÑÔºåËÆ©ÊàëÊù•Â∏Æ‰Ω†ÂàÜÊûêËøôÈÅìÈ¢òÔºÅ

üìù **Ëß£È¢òÊÄùË∑Ø**

È¶ñÂÖàÔºåÊàë‰ª¨ÈúÄË¶ÅËØÜÂà´È¢òÁõÆÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÂíåËÄÉÊü•ÁöÑÁü•ËØÜÁÇπ„ÄÇ

‰∏ÄËà¨Êù•ËØ¥ÔºåËß£È¢òÂèØ‰ª•ÂàÜ‰∏∫‰ª•‰∏ãÊ≠•È™§Ôºö
1. **ÂÆ°È¢ò** - ÊòéÁ°ÆÂ∑≤Áü•Êù°‰ª∂ÂíåÊâÄÊ±Ç
2. **Âª∫Ê®°** - Âª∫Á´ãÊï∞Â≠¶Ê®°ÂûãÊàñÊâæÂà∞ÈÄÇÁî®ÁöÑÂÖ¨Âºè
3. **ËÆ°ÁÆó** - ÊåâÊ≠•È™§ËßÑËåÉËÆ°ÁÆó
4. **È™åËØÅ** - Ê£ÄÊü•ÁªìÊûúÊòØÂê¶ÂêàÁêÜ

Â¶ÇÊûú‰Ω†ËÉΩÊääÂÖ∑‰ΩìÁöÑÈ¢òÁõÆÂèëÁªôÊàëÔºåÊàëÂèØ‰ª•Áªô‰Ω†Êõ¥ËØ¶ÁªÜÁöÑËß£Á≠îÂíåÂàÜÊûêÂì¶ÔºÅ

üí° Â∞èÊèêÁ§∫ÔºöÈÅáÂà∞‰∏ç‰ºöÁöÑÈ¢òÁõÆÔºåÂÖàÂ∞ùËØïËá™Â∑±ÊÄùËÄÉ 5 ÂàÜÈíüÔºåËøôÊ†∑Â≠¶‰π†ÊïàÊûúÊõ¥Â•ΩÔºÅ""",
}

@dataclass
class LLMResponse:
    content: str
    tool_calls: Optional[List[Dict]] = None
    finish_reason: str = "stop"

@dataclass
class StreamChunk:
    type: str  # "text" | "tool_call_chunk" | "tool_call_end" | "usage"
    content: Optional[str] = None
    tool_call_id: Optional[str] = None
    tool_name: Optional[str] = None
    arguments: Optional[str] = None # For tool_call_chunk
    full_arguments: Optional[Dict] = None # For tool_call_end
    # Token usage fields
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None

tracer = trace.get_tracer(__name__)

class LLMService:
    """
    LLM ÊúçÂä°
    ÊîØÊåÅÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºàFunction CallingÔºâ
    """
    
    def __init__(self):
        # Ê†πÊçÆÊèê‰æõÂïÜÈÄâÊã©ÈÖçÁΩÆ
        provider_type = settings.LLM_PROVIDER.lower()
        
        if provider_type == "deepseek":
            api_key = settings.DEEPSEEK_API_KEY
            base_url = settings.DEEPSEEK_BASE_URL
            self.chat_model = settings.DEEPSEEK_CHAT_MODEL or settings.LLM_MODEL_NAME
            self.reason_model = settings.DEEPSEEK_REASON_MODEL or settings.LLM_REASON_MODEL_NAME
        else:
            # ÈªòËÆ§‰ΩøÁî®ÈÄöÁî® LLM ÈÖçÁΩÆ (OpenAI, Qwen Á≠â)
            api_key = settings.LLM_API_KEY
            base_url = settings.LLM_API_BASE_URL
            self.chat_model = settings.LLM_MODEL_NAME
            self.reason_model = settings.LLM_REASON_MODEL_NAME or settings.LLM_MODEL_NAME
            
        self._provider_error: Optional[str] = None
        try:
            self.provider = OpenAICompatibleProvider(
                api_key=api_key,
                base_url=base_url
            )
        except Exception as e:
            self.provider = None
            self._provider_error = str(e)
            logger.warning(f"LLM provider unavailable; LLM features disabled: {e}")
        self.default_model = self.chat_model
        self.demo_mode = getattr(settings, 'DEMO_MODE', False)

    def _check_demo_match(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """
        Ê£ÄÊü•ÊòØÂê¶ÂåπÈÖçÊºîÁ§∫ÂÖ≥ÈîÆËØç

        Returns:
            ÂåπÈÖçÁöÑÈ¢ÑËÆæÂìçÂ∫îÔºåÂ¶ÇÊûú‰∏çÂåπÈÖçÂàôËøîÂõû None
        """
        if not self.demo_mode:
            return None

        # Ëé∑ÂèñÊúÄÂêé‰∏ÄÊù°Áî®Êà∑Ê∂àÊÅØ
        user_content = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_content = msg.get("content", "").strip()
                break

        if not user_content:
            return None

        # Á≤æÁ°ÆÂåπÈÖç
        if user_content in DEMO_MOCK_RESPONSES:
            logger.info(f"‚ö° [DEMO MODE] Exact match for: {user_content}")
            return DEMO_MOCK_RESPONSES[user_content]

        # Ê®°Á≥äÂåπÈÖç (ÂåÖÂê´ÂÖ≥ÈîÆËØç)
        for key, response in DEMO_MOCK_RESPONSES.items():
            if key in user_content or user_content in key:
                logger.info(f"‚ö° [DEMO MODE] Fuzzy match for: {user_content} -> {key}")
                return response

        return None

    async def chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        Send a chat request to the LLM.
        """
        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )
        model = model or self.chat_model
        with tracer.start_as_current_span("llm_chat") as span:
            span.set_attribute("llm.model", model)
            span.set_attribute("llm.temperature", temperature)
            
            # üé≠ Demo Mode Êã¶Êà™
            mock_response = self._check_demo_match(messages)
            if mock_response:
                span.set_attribute("llm.demo_mode", True)
                # Ê®°ÊãüÊÄùËÄÉÂª∂Ëøü
                await asyncio.sleep(1.0)
                return mock_response

            logger.debug(f"Sending chat request to model: {model}")
            response = await self.provider.chat(messages, model=model, temperature=temperature, **kwargs)
            return response

    async def reason(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.2,
        **kwargs
    ) -> str:
        """
        Send a deep reasoning request to the LLM.
        """
        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )
        model = model or self.reason_model
        with tracer.start_as_current_span("llm_reason") as span:
            span.set_attribute("llm.model", model)
            span.set_attribute("llm.temperature", temperature)
            response = await self.provider.chat(messages, model=model, temperature=temperature, **kwargs)
            return response

    async def reason_json(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.2,
        **kwargs
    ) -> Any:
        """
        Request JSON output from the LLM using reasoning model.
        """
        raw = await self.reason(messages, model=model, temperature=temperature, **kwargs)
        cleaned = raw.replace("```json", "").replace("```", "").strip()

        def _extract_json_block(text: str) -> Optional[str]:
            for start, end in (("{", "}"), ("[", "]")):
                if start in text and end in text:
                    return text[text.find(start):text.rfind(end) + 1]
            return None

        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            extracted = _extract_json_block(cleaned)
            if extracted:
                return json.loads(extracted)
            logger.warning("Failed to parse JSON from LLM reasoning response, returning empty result")
            return {}

    async def chat_json(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.3,
        **kwargs
    ) -> Any:
        """
        Request JSON output from the LLM and parse it safely.
        """
        raw = await self.chat(messages, model=model, temperature=temperature, **kwargs)
        cleaned = raw.replace("```json", "").replace("```", "").strip()

        def _extract_json_block(text: str) -> Optional[str]:
            for start, end in (("{", "}"), ("[", "]")):
                if start in text and end in text:
                    return text[text.find(start):text.rfind(end) + 1]
            return None

        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            extracted = _extract_json_block(cleaned)
            if extracted:
                return json.loads(extracted)
            logger.warning("Failed to parse JSON from LLM response, returning empty result")
            return {}

    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat response from the LLM.
        """
        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )
        model = model or self.chat_model
        with tracer.start_as_current_span("llm_stream_chat") as span:
            span.set_attribute("llm.model", model)
            
            # üé≠ Demo Mode Êã¶Êà™ - ÊµÅÂºèËøîÂõûÈ¢ÑËÆæÂìçÂ∫î
            mock_response = self._check_demo_match(messages)
            if mock_response:
                span.set_attribute("llm.demo_mode", True)
                # Ê®°ÊãüÊµÅÂºèËæìÂá∫ÔºåÊØèÊ¨°ËæìÂá∫Âá†‰∏™Â≠óÁ¨¶
                chunk_size = 10
                for i in range(0, len(mock_response), chunk_size):
                    chunk = mock_response[i:i + chunk_size]
                    yield chunk
                    # Ê®°ÊãüÊâìÂ≠óÊïàÊûúÁöÑÂª∂Ëøü
                    await asyncio.sleep(0.03)
                return

            logger.debug(f"Starting stream chat with model: {model}")
            async for chunk in self.provider.stream_chat(messages, model=model, temperature=temperature, **kwargs):
                yield chunk

    async def chat_with_tools(
        self,
        system_prompt: str,
        user_message: str,
        tools: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict]] = None
    ) -> LLMResponse:
        """
        Â∏¶Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑËÅäÂ§©
        """
        messages = [{"role": "system", "content": system_prompt}]
        
        if conversation_history:
            messages.extend(conversation_history)
        
        messages.append({"role": "user", "content": user_message})

        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )

        if hasattr(self.provider, 'client'):
            with tracer.start_as_current_span("llm_chat_with_tools") as span:
                span.set_attribute("llm.model", self.default_model)
                
                response = await self.provider.client.chat.completions.create(
                    model=self.default_model,
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    temperature=0.7,
                )
                
                choice = response.choices[0]
                message = choice.message
                
                if response.usage:
                    span.set_attribute("llm.usage.prompt_tokens", response.usage.prompt_tokens)
                    span.set_attribute("llm.usage.completion_tokens", response.usage.completion_tokens)
                    span.set_attribute("llm.usage.total_tokens", response.usage.total_tokens)

                tool_calls_dicts = []
                if message.tool_calls:
                    for tc in message.tool_calls:
                        tool_calls_dicts.append({
                            "id": tc.id,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            }
                        })

                return LLMResponse(
                    content=message.content or "",
                    tool_calls=tool_calls_dicts,
                    finish_reason=choice.finish_reason
                )
        else:
            raise NotImplementedError("Current LLM provider does not support tool calling directly.")
    
    async def continue_with_tool_results(
        self,
        conversation_history: List[Dict],
        tool_results: List[Dict]
    ) -> LLMResponse:
        """
        Â∞ÜÂ∑•ÂÖ∑ÊâßË°åÁªìÊûúÂèçÈ¶àÁªô LLMÔºåËé∑ÂèñÊúÄÁªàÂõûÂ§ç
        """
        messages = conversation_history[:]
        for result in tool_results:
            messages.append({
                "role": "tool",
                "content": json.dumps(result, ensure_ascii=False)
            })
        
        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )

        if hasattr(self.provider, 'client'):
            with tracer.start_as_current_span("llm_continue_after_tools") as span:
                span.set_attribute("llm.model", self.default_model)
                
                response = await self.provider.client.chat.completions.create(
                    model=self.default_model,
                    messages=messages,
                    temperature=0.7,
                )
                choice = response.choices[0]
                message = choice.message
                
                if response.usage:
                    span.set_attribute("llm.usage.prompt_tokens", response.usage.prompt_tokens)
                    span.set_attribute("llm.usage.completion_tokens", response.usage.completion_tokens)
                    span.set_attribute("llm.usage.total_tokens", response.usage.total_tokens)

                return LLMResponse(
                    content=message.content or "",
                    tool_calls=None,
                    finish_reason=choice.finish_reason
                )
        else:
            raise NotImplementedError("Current LLM provider does not support tool calling directly.")
    
    async def chat_stream_with_tools(
        self,
        system_prompt: str,
        user_message: str,
        tools: List[Dict[str, Any]]
    ) -> AsyncIterator[StreamChunk]:
        """
        ÊµÅÂºèËÅäÂ§©ÔºàÊîØÊåÅÂ∑•ÂÖ∑Ë∞ÉÁî®Ôºâ
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        if not self.provider:
            raise HTTPException(
                status_code=501,
                detail=f"LLM provider unavailable: {self._provider_error or 'missing dependency'}"
            )

        if hasattr(self.provider, 'client'):
            with tracer.start_as_current_span("llm_chat_stream_with_tools") as span:
                span.set_attribute("llm.model", self.default_model)
                
                stream = await self.provider.client.chat.completions.create(
                    model=self.default_model,
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    stream=True,
                    temperature=0.7,
                    stream_options={"include_usage": True}
                )

                collected_tool_call_chunks = {}
                usage_data = None

                async for chunk in stream:
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage_data = chunk.usage

                    if chunk.choices:
                        delta = chunk.choices[0].delta
                        if delta.content:
                            yield StreamChunk(type="text", content=delta.content)

                        if delta.tool_calls:
                            for tc_chunk in delta.tool_calls:
                                tool_call_id = tc_chunk.id
                                if tool_call_id not in collected_tool_call_chunks:
                                    collected_tool_call_chunks[tool_call_id] = {"name": "", "args_str": ""}
                                if tc_chunk.function.name:
                                    collected_tool_call_chunks[tool_call_id]["name"] = tc_chunk.function.name
                                    yield StreamChunk(type="tool_call_chunk", tool_call_id=tool_call_id, tool_name=tc_chunk.function.name)
                                if tc_chunk.function.arguments:
                                    collected_tool_call_chunks[tool_call_id]["args_str"] += tc_chunk.function.arguments
                                    yield StreamChunk(type="tool_call_chunk", tool_call_id=tool_call_id, arguments=tc_chunk.function.arguments)

                for tool_call_id, data in collected_tool_call_chunks.items():
                    if data["name"] and data["args_str"]:
                        try:
                            full_arguments = json.loads(data["args_str"])
                            yield StreamChunk(
                                type="tool_call_end",
                                tool_call_id=tool_call_id,
                                tool_name=data["name"],
                                full_arguments=full_arguments
                            )
                        except json.JSONDecodeError:
                            logger.error(f"Failed to decode tool arguments for {tool_call_id}: {data['args_str']}")

                if usage_data:
                    span.set_attribute("llm.usage.prompt_tokens", usage_data.prompt_tokens)
                    span.set_attribute("llm.usage.completion_tokens", usage_data.completion_tokens)
                    span.set_attribute("llm.usage.total_tokens", usage_data.total_tokens)
                    yield StreamChunk(
                        type="usage",
                        prompt_tokens=usage_data.prompt_tokens,
                        completion_tokens=usage_data.completion_tokens,
                        total_tokens=usage_data.total_tokens
                    )
        else:
            raise NotImplementedError("Current LLM provider does not support streamed tool calling directly.")

    async def generate_push_content(
        self,
        user_nickname: str,
        persona: str,
        trigger_type: str,
        context_data: Dict
    ) -> Dict[str, str]:
        """
        Generate "irresistible" push notification content based on persona.
        """
        persona_prompts = {
            "coach": "Role: Strict, discipline-focused Study Coach. Tone: Stern, urgent, authoritative.",
            "anime": "Role: Gentle, cute, energetic Anime Assistant. Tone: Sweet, encouraging."
        }
        selected_persona_prompt = persona_prompts.get(persona, persona_prompts["coach"])
        
        trigger_desc = ""
        if trigger_type == "memory":
            nodes = ", ".join(context_data.get("nodes", []))
            trigger_desc = f"User is forgetting: {nodes}."
        elif trigger_type == "sprint":
            trigger_desc = f"Deadline approaching for plan '{context_data.get('plan_name')}'."
        elif trigger_type == "inactivity":
            trigger_desc = "User hasn't studied for over 24 hours."
        
        system_prompt = f"You are Sparkle, an AI Learning Assistant. {selected_persona_prompt} Context: {trigger_desc}"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "Generate push notification now."}
        ]
        
        try:
            with tracer.start_as_current_span("llm_generate_push") as span:
                span.set_attribute("llm.persona", persona)
                span.set_attribute("llm.trigger", trigger_type)
                
                response_text = await self.chat(messages, temperature=0.8)
                cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
                content = json.loads(cleaned_text)
                return content
        except Exception as e:
            logger.error(f"Failed to generate push content: {e}")
            return {"title": "Â≠¶‰π†ÊèêÈÜí", "body": f"{user_nickname}ÔºåËØ•Â§ç‰π†‰∫Ü„ÄÇ"}

# Singleton instance
llm_service = LLMService()
