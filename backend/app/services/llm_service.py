import json
from typing import List, Dict, AsyncGenerator, Optional, Any, AsyncIterator
import asyncio
from loguru import logger
from dataclasses import dataclass

from app.config import settings
from app.services.llm.base import LLMProvider
from app.services.llm.providers import OpenAICompatibleProvider

# ==========================================
# ğŸ­ æ¼”ç¤ºæ¨¡å¼é¢„è®¾å“åº” (Demo Mock Responses)
# ==========================================
# ç”¨äºç«èµ›æ¼”ç¤ºï¼Œç¡®ä¿å…³é”®æµç¨‹ 100% æˆåŠŸä¸”ç§’å›
# è¦å¯ç”¨: åœ¨ .env ä¸­è®¾ç½® DEMO_MODE=true
#
# ğŸ’¡ ä½¿ç”¨è¯´æ˜:
# 1. åœ¨æ¼”ç¤ºè„šæœ¬ä¸­è¾“å…¥çš„æ–‡å­—å¿…é¡»ä¸ä¸‹é¢çš„ key å®Œå…¨ä¸€è‡´
# 2. å¯ä»¥æŒ‰éœ€æ·»åŠ æ›´å¤šå…³é”®è¯å’Œå“åº”
# ==========================================

DEMO_MOCK_RESPONSES: Dict[str, str] = {
    "å¸®æˆ‘åˆ¶å®šé«˜æ•°å¤ä¹ è®¡åˆ’": """å¥½çš„ï¼åŸºäºä½ çš„å­¦ä¹ æƒ…å†µï¼Œæˆ‘ä¸ºä½ åˆ¶å®šäº†ä¸€ä¸ªé«˜æ•ˆçš„é«˜æ•°å¤ä¹ è®¡åˆ’ã€‚

ğŸ“š **é«˜æ•°å†²åˆºå¤ä¹ è®¡åˆ’**

æ ¹æ®è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿å’Œä½ çš„çŸ¥è¯†æ˜Ÿå›¾åˆ†æï¼Œæˆ‘å‘ç°ä½ åœ¨ä»¥ä¸‹å‡ ä¸ªçŸ¥è¯†ç‚¹éœ€è¦é‡ç‚¹å¤ä¹ ï¼š

1. **æé™ä¸è¿ç»­** - æŒæ¡åº¦è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå¤ä¹ 
2. **å¯¼æ•°çš„åº”ç”¨** - éœ€è¦å¼ºåŒ–ï¼Œç‰¹åˆ«æ˜¯æœ€å€¼é—®é¢˜
3. **ç§¯åˆ†è®¡ç®—** - åŸºç¡€è¿˜ä¸é”™ï¼Œåšé¢˜å·©å›ºå³å¯

æˆ‘å·²ä¸ºä½ ç”Ÿæˆä»¥ä¸‹ä»»åŠ¡å¡ç‰‡ï¼š

```json
{
  "actions": [
    {
      "type": "create_task",
      "data": {
        "title": "æé™ä¸è¿ç»­é‡éš¾ç‚¹å¤ä¹ ",
        "type": "learning",
        "estimated_minutes": 45,
        "priority": "high"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "å¯¼æ•°åº”ç”¨ä¸“é¢˜ç»ƒä¹ ",
        "type": "training",
        "estimated_minutes": 30,
        "priority": "medium"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "ç§¯åˆ†è®¡ç®—åˆ·é¢˜",
        "type": "training",
        "estimated_minutes": 25,
        "priority": "normal"
      }
    }
  ]
}
```

å»ºè®®æŒ‰ç…§ä¸Šè¿°é¡ºåºå­¦ä¹ ï¼Œå…ˆæ”»å…‹å¼±é¡¹ï¼Œå†å·©å›ºå¼ºé¡¹ã€‚åŠ æ²¹ï¼ğŸ”¥""",

    "æˆ‘ä»Šå¤©è¦å­¦ä»€ä¹ˆ": """æ—©ä¸Šå¥½ï¼è®©æˆ‘çœ‹çœ‹ä½ çš„å­¦ä¹ çŠ¶æ€...

ğŸ“Š **ä»Šæ—¥å­¦ä¹ å»ºè®®**

æ ¹æ®ä½ çš„çŸ¥è¯†æ˜Ÿå›¾å’Œé—å¿˜æ›²çº¿åˆ†æï¼š

ğŸ”´ **éœ€è¦å¤ä¹ ** (æŒæ¡åº¦ä¸‹é™):
- çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µè¿ç®— (è·ä¸Šæ¬¡å­¦ä¹ å·²è¿‡ 5 å¤©)
- é«˜æ•°ï¼šç§¯åˆ†æŠ€å·§ (æŒæ¡åº¦é™è‡³ 65%)

ğŸŸ¡ **ä»Šæ—¥æ¨èå­¦ä¹ **:
- æ¦‚ç‡è®ºï¼šæ¡ä»¶æ¦‚ç‡ (æŒ‰è®¡åˆ’åº”ä»Šæ—¥å­¦ä¹ )

ğŸ’¡ æˆ‘å»ºè®®ä½ ä»Šå¤©å…ˆèŠ± 20 åˆ†é’Ÿå¤ä¹ çº¿ä»£çŸ©é˜µè¿ç®—ï¼Œç„¶åå†å­¦ä¹ æ–°å†…å®¹ã€‚

éœ€è¦æˆ‘å¸®ä½ åˆ›å»ºä»Šæ—¥å­¦ä¹ ä»»åŠ¡å—ï¼Ÿ""",

    "è¿™é“é¢˜æ€ä¹ˆåš": """å¥½çš„ï¼Œè®©æˆ‘æ¥å¸®ä½ åˆ†æè¿™é“é¢˜ï¼

ğŸ“ **è§£é¢˜æ€è·¯**

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è¯†åˆ«é¢˜ç›®çš„å…³é”®ä¿¡æ¯å’Œè€ƒæŸ¥çš„çŸ¥è¯†ç‚¹ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œè§£é¢˜å¯ä»¥åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š
1. **å®¡é¢˜** - æ˜ç¡®å·²çŸ¥æ¡ä»¶å’Œæ‰€æ±‚
2. **å»ºæ¨¡** - å»ºç«‹æ•°å­¦æ¨¡å‹æˆ–æ‰¾åˆ°é€‚ç”¨çš„å…¬å¼
3. **è®¡ç®—** - æŒ‰æ­¥éª¤è§„èŒƒè®¡ç®—
4. **éªŒè¯** - æ£€æŸ¥ç»“æœæ˜¯å¦åˆç†

å¦‚æœä½ èƒ½æŠŠå…·ä½“çš„é¢˜ç›®å‘ç»™æˆ‘ï¼Œæˆ‘å¯ä»¥ç»™ä½ æ›´è¯¦ç»†çš„è§£ç­”å’Œåˆ†æå“¦ï¼

ğŸ’¡ å°æç¤ºï¼šé‡åˆ°ä¸ä¼šçš„é¢˜ç›®ï¼Œå…ˆå°è¯•è‡ªå·±æ€è€ƒ 5 åˆ†é’Ÿï¼Œè¿™æ ·å­¦ä¹ æ•ˆæœæ›´å¥½ï¼""",
}

@dataclass
class LLMResponse:
    content: str
    tool_calls: Optional[List[Dict]] = None
    finish_reason: str = "stop"

@dataclass
class StreamChunk:
    type: str  # "text" | "tool_call_chunk" | "tool_call_end" | "usage"
    content: Optional[str] = None
    tool_call_id: Optional[str] = None
    tool_name: Optional[str] = None
    arguments: Optional[str] = None # For tool_call_chunk
    full_arguments: Optional[Dict] = None # For tool_call_end
    # Token usage fields
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None

class LLMService:
    """
    LLM æœåŠ¡
    æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰
    """
    
    def __init__(self):
        # æ ¹æ®æä¾›å•†é€‰æ‹©é…ç½®
        provider_type = settings.LLM_PROVIDER.lower()
        
        if provider_type == "deepseek":
            api_key = settings.DEEPSEEK_API_KEY
            base_url = settings.DEEPSEEK_BASE_URL
        else:
            # é»˜è®¤ä½¿ç”¨é€šç”¨ LLM é…ç½® (OpenAI, Qwen ç­‰)
            api_key = settings.LLM_API_KEY
            base_url = settings.LLM_API_BASE_URL
            
        self.provider: LLMProvider = OpenAICompatibleProvider(
            api_key=api_key,
            base_url=base_url
        )
        self.default_model = settings.LLM_MODEL_NAME
        self.demo_mode = getattr(settings, 'DEMO_MODE', False)

    def _check_demo_match(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """
        æ£€æŸ¥æ˜¯å¦åŒ¹é…æ¼”ç¤ºå…³é”®è¯

        Returns:
            åŒ¹é…çš„é¢„è®¾å“åº”ï¼Œå¦‚æœä¸åŒ¹é…åˆ™è¿”å› None
        """
        if not self.demo_mode:
            return None

        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_content = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_content = msg.get("content", "").strip()
                break

        if not user_content:
            return None

        # ç²¾ç¡®åŒ¹é…
        if user_content in DEMO_MOCK_RESPONSES:
            logger.info(f"âš¡ [DEMO MODE] Exact match for: {user_content}")
            return DEMO_MOCK_RESPONSES[user_content]

        # æ¨¡ç³ŠåŒ¹é… (åŒ…å«å…³é”®è¯)
        for key, response in DEMO_MOCK_RESPONSES.items():
            if key in user_content or user_content in key:
                logger.info(f"âš¡ [DEMO MODE] Fuzzy match for: {user_content} -> {key}")
                return response

        return None

    async def chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        Send a chat request to the LLM.
        """
        # ğŸ­ Demo Mode æ‹¦æˆª
        mock_response = self._check_demo_match(messages)
        if mock_response:
            # æ¨¡æ‹Ÿæ€è€ƒå»¶è¿Ÿ
            await asyncio.sleep(1.0)
            return mock_response

        model = model or self.default_model
        logger.debug(f"Sending chat request to model: {model}")
        return await self.provider.chat(messages, model=model, temperature=temperature, **kwargs)

    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat response from the LLM.
        """
        # ğŸ­ Demo Mode æ‹¦æˆª - æµå¼è¿”å›é¢„è®¾å“åº”
        mock_response = self._check_demo_match(messages)
        if mock_response:
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼Œæ¯æ¬¡è¾“å‡ºå‡ ä¸ªå­—ç¬¦
            chunk_size = 10
            for i in range(0, len(mock_response), chunk_size):
                chunk = mock_response[i:i + chunk_size]
                yield chunk
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœçš„å»¶è¿Ÿ
                await asyncio.sleep(0.03)
            return

        model = model or self.default_model
        logger.debug(f"Starting stream chat with model: {model}")
        async for chunk in self.provider.stream_chat(messages, model=model, temperature=temperature, **kwargs):
            yield chunk

    async def chat_with_tools(
        self,
        system_prompt: str,
        user_message: str,
        tools: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict]] = None
    ) -> LLMResponse:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„èŠå¤©
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            tools: OpenAI æ ¼å¼çš„å·¥å…·å®šä¹‰
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            LLMResponse: åŒ…å«æ–‡æœ¬å’Œå·¥å…·è°ƒç”¨çš„å“åº”
        """
        messages = [{"role": "system", "content": system_prompt}]
        
        if conversation_history:
            messages.extend(conversation_history)
        
        messages.append({"role": "user", "content": user_message})

        # Using self.provider.client (AsyncOpenAI) directly for tool calls
        if hasattr(self.provider, 'client'):
            response = await self.provider.client.chat.completions.create(
                model=self.default_model,
                messages=messages,
                tools=tools,
                tool_choice="auto",  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
                temperature=0.7, # Default temperature
            )
            
            choice = response.choices[0]
            message = choice.message
            
            tool_calls_dicts = []
            if message.tool_calls:
                for tc in message.tool_calls:
                    tool_calls_dicts.append({
                        "id": tc.id,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments, # Arguments are already string
                        }
                    })

            return LLMResponse(
                content=message.content or "",
                tool_calls=tool_calls_dicts,
                finish_reason=choice.finish_reason
            )
        else:
            raise NotImplementedError("Current LLM provider does not support tool calling directly.")
    
    async def continue_with_tool_results(
        self,
        conversation_history: List[Dict], # full history up to LLM's initial response
        tool_results: List[Dict] # tool_results from executor
    ) -> LLMResponse:
        """
        å°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™ LLMï¼Œè·å–æœ€ç»ˆå›å¤
        """
        messages = conversation_history[:] # Copy history
        
        # Append tool messages
        for result in tool_results:
            # Need to find the original tool_call_id from the conversation_history if possible
            # Or just append as a 'tool' role message
            messages.append({
                "role": "tool",
                # "tool_call_id": result.get("tool_call_id", ""), # if we track original tool_call_id
                "content": json.dumps(result, ensure_ascii=False)
            })
        
        # Now call LLM again without tools, get final message
        if hasattr(self.provider, 'client'):
            response = await self.provider.client.chat.completions.create(
                model=self.default_model,
                messages=messages,
                temperature=0.7,
            )
            choice = response.choices[0]
            message = choice.message
            return LLMResponse(
                content=message.content or "",
                tool_calls=None, # No more tool calls expected
                finish_reason=choice.finish_reason
            )
        else:
            raise NotImplementedError("Current LLM provider does not support tool calling directly.")
    
    async def chat_stream_with_tools(
        self,
        system_prompt: str,
        user_message: str,
        tools: List[Dict[str, Any]]
    ) -> AsyncIterator[StreamChunk]:
        """
        æµå¼èŠå¤©ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

        Yields:
            StreamChunk: æ–‡æœ¬å—ã€å·¥å…·è°ƒç”¨æˆ– Token ä½¿ç”¨é‡
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        if hasattr(self.provider, 'client'):
            stream = await self.provider.client.chat.completions.create(
                model=self.default_model,
                messages=messages,
                tools=tools,
                tool_choice="auto",
                stream=True,
                temperature=0.7,
                stream_options={"include_usage": True}  # è¯·æ±‚ usage ä¿¡æ¯
            )

            collected_tool_call_chunks = {} # {id: {name: "", args_str: ""}}
            usage_data = None

            async for chunk in stream:
                # Handle usage data (may come in final chunk)
                if hasattr(chunk, 'usage') and chunk.usage:
                    usage_data = chunk.usage

                # Handle choices
                if chunk.choices:
                    delta = chunk.choices[0].delta

                    # Text content
                    if delta.content:
                        yield StreamChunk(type="text", content=delta.content)

                    # Tool call chunks
                    if delta.tool_calls:
                        for tc_chunk in delta.tool_calls:
                            tool_call_id = tc_chunk.id

                            if tool_call_id not in collected_tool_call_chunks:
                                collected_tool_call_chunks[tool_call_id] = {
                                    "name": "",
                                    "args_str": ""
                                }

                            if tc_chunk.function.name:
                                collected_tool_call_chunks[tool_call_id]["name"] = tc_chunk.function.name
                                yield StreamChunk(type="tool_call_chunk", tool_call_id=tool_call_id, tool_name=tc_chunk.function.name)

                            if tc_chunk.function.arguments:
                                collected_tool_call_chunks[tool_call_id]["args_str"] += tc_chunk.function.arguments
                                yield StreamChunk(type="tool_call_chunk", tool_call_id=tool_call_id, arguments=tc_chunk.function.arguments)

            # After stream ends, yield full tool call if any
            for tool_call_id, data in collected_tool_call_chunks.items():
                if data["name"] and data["args_str"]:
                    try:
                        full_arguments = json.loads(data["args_str"])
                        yield StreamChunk(
                            type="tool_call_end",
                            tool_call_id=tool_call_id,
                            tool_name=data["name"],
                            full_arguments=full_arguments
                        )
                    except json.JSONDecodeError:
                        logger.error(f"Failed to decode tool arguments for {tool_call_id}: {data['args_str']}")

            # Finally, yield usage data
            if usage_data:
                yield StreamChunk(
                    type="usage",
                    prompt_tokens=usage_data.prompt_tokens,
                    completion_tokens=usage_data.completion_tokens,
                    total_tokens=usage_data.total_tokens
                )

        else:
            raise NotImplementedError("Current LLM provider does not support streamed tool calling directly.")

    async def generate_push_content(
        self,
        user_nickname: str,
        persona: str,
        trigger_type: str,
        context_data: Dict
    ) -> Dict[str, str]:
        """
        Generate "irresistible" push notification content based on persona.
        
        Args:
            user_nickname: Name of the user
            persona: "coach" (strict) or "anime" (gentle/cute) or others
            trigger_type: "memory", "sprint", "inactivity"
            context_data: Data from strategy (nodes, plan name, etc.)
            
        Returns:
            Dict with "title" and "body" keys.
        """
        
        # 1. Define Persona Prompts
        persona_prompts = {
            "coach": """
            Role: Strict, discipline-focused Study Coach.
            Tone: Stern, urgent, authoritative. 
            Style: Use rhetorical questions, emphasize consequences of laziness.
            Example: "è¿˜æ²¡å­¦å®Œï¼Ÿä½ çš„çº¿æ€§ä»£æ•°æ­£åœ¨å“­æ³£ï¼"
            """,
            "anime": """
            Role: Gentle, cute, energetic Anime Assistant (like a younger sister or supportive friend).
            Tone: Sweet, encouraging, uses emojis (âœ¨, ğŸ¥º, ğŸ”¥).
            Style: Address user as 'æ¬§å°¼é…±' or 'äº²çˆ±çš„', emphasize growing together.
            Example: "æ¬§å°¼é…±~ è®°å¿†ç¢ç‰‡è¦æ¶ˆå¤±äº†å“¦ï¼Œå¿«æ¥è¡¥æ•‘å§ï¼âœ¨"
            """
        }
        
        selected_persona_prompt = persona_prompts.get(persona, persona_prompts["coach"]) # Default to coach
        
        # 2. Define Context Description based on Trigger
        trigger_desc = ""
        if trigger_type == "memory":
            nodes = ", ".join(context_data.get("nodes", []))
            retention = int(context_data.get("retention_rate", 0) * 100)
            trigger_desc = f"User is forgetting these topics: {nodes}. Retention is down to {retention}%. Explain that reviewing now saves time later."
        elif trigger_type == "sprint":
            plan_name = context_data.get("plan_name", "Plan")
            hours = context_data.get("hours_remaining", 0)
            trigger_desc = f"Deadline approaching for plan '{plan_name}' in {hours} hours. Urge immediate action to avoid failure."
        elif trigger_type == "inactivity":
            trigger_desc = "User hasn't studied for over 24 hours. Gently guilt-trip (coach) or sweetly miss them (anime) to bring them back."
        
        # 3. Construct Full Prompt
        system_prompt = f"""
        You are Sparkle, an AI Learning Assistant.
        {selected_persona_prompt}
        
        Task: Write a push notification for user '{user_nickname}'.
        Context: {trigger_desc}
        
        Constraints:
        1. Language: Chinese (Simplified).
        2. Length: Body must be under 30 words. Title under 10 words.
        3. Format: Return ONLY a valid JSON object with keys "title" and "body". Do not wrap in markdown code blocks.
        4. Content: Must explain "WHY study NOW".
        """
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "Generate push notification now."}
        ]
        
        # 4. Call LLM
        try:
            response_text = await self.chat(messages, temperature=0.8) # Slightly higher temp for creativity
            
            # 5. Parse JSON
            # Clean up potential markdown formatting like ```json ... ```
            cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
            
            content = json.loads(cleaned_text)
            
            # Fallback validation
            if "title" not in content or "body" not in content:
                raise ValueError("Missing keys in JSON response")
                
            return content
            
        except Exception as e:
            logger.error(f"Failed to generate push content: {e}")
            # Fallback hardcoded messages
            if persona == "anime":
                return {
                    "title": "æƒ³ä½ äº†~ âœ¨",
                    "body": f"{user_nickname}ï¼Œå¥½ä¹…æ²¡æ¥å­¦ä¹ äº†ï¼Œè®°å¿†éƒ½è¦å‘éœ‰å•¦ï¼ğŸ¥º"
                }
            else:
                return {
                    "title": "å­¦ä¹ æé†’",
                    "body": f"{user_nickname}ï¼Œè¯¥å¤ä¹ äº†ã€‚æ‹–å»¶åªä¼šå¢åŠ æœªæ¥çš„è´Ÿæ‹…ã€‚"
                }

# Singleton instance
llm_service = LLMService()