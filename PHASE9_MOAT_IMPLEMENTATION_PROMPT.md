# Phase 9 æŠ¤åŸæ²³å·¥ç¨‹ - æ‰§è¡ŒAgentå®æ–½Prompt

> **ç›®æ ‡**ï¼šæ„å»ºä¸‰é£è½®é—­ç¯ä½“éªŒæŠ¤åŸæ²³ï¼ˆèµ„äº§â†’å›¾è°±ã€å¤ä¹ â†’ä¸ªæ€§åŒ–ã€å¤šç«¯ä¸€è‡´ï¼‰
> **æ‰§è¡Œæ–¹å¼**ï¼šæŒ‰Milestoneé¡ºåºæ‰§è¡Œï¼Œæ¯ä¸ªMilestoneç‹¬ç«‹å¯éªŒæ”¶å¯å›æ»š

---

## ğŸ§  æ¶æ„ç†è§£å‰ç½®ï¼ˆå¿…è¯»ï¼‰

æ‰§è¡Œå‰å¿…é¡»ç†è§£ç°æœ‰æ¶æ„ï¼š

### å·²æœ‰åŸºç¡€è®¾æ–½
```
äº‹ä»¶ç³»ç»Ÿ:
â”œâ”€â”€ event_outboxè¡¨ (aggregate_type, aggregate_id, event_type, sequence_number, payload)
â”œâ”€â”€ event_sequence_countersè¡¨ (ä¿è¯å•èšåˆsequenceå•è°ƒé€’å¢)
â””â”€â”€ Redis EventBus (å®æ—¶å‘å¸ƒè®¢é˜…)

å­¦ä¹ èµ„äº§:
â”œâ”€â”€ learning_assetsè¡¨ (status, headword, snapshot_json, provenance_json, SRSå­—æ®µ)
â”œâ”€â”€ LearningAssetService (backend/app/services/learning_asset_service.py)
â””â”€â”€ å·²æœ‰äº‹ä»¶: asset_created, asset_status_changed, review_recorded

çŸ¥è¯†å›¾è°±:
â”œâ”€â”€ knowledge_nodesè¡¨ (name, embedding, position_x/y, status)
â”œâ”€â”€ node_relationsè¡¨ (source_node_id, target_node_id, relation_type, strength, created_by)
â”‚   âš ï¸ æ³¨æ„: æ˜¯å…¨å±€è¾¹ï¼Œæ²¡æœ‰user_id
â”œâ”€â”€ user_node_statusè¡¨ (user_id, node_id, mastery_score, bkt_mastery_prob, revision, next_review_at)
â””â”€â”€ GalaxyService (backend/app/services/galaxy_service.py)

Flutteræœ¬åœ°å­˜å‚¨:
â”œâ”€â”€ ä½¿ç”¨ Isar (ä¸æ˜¯SQLite!)
â”œâ”€â”€ LocalKnowledgeNode, PendingUpdate, OutboxItem, LocalCRDTSnapshot
â””â”€â”€ SyncEngine (mobile/lib/core/offline/sync_engine.dart)
```

### å…³é”®æ–‡ä»¶è·¯å¾„
```
Python Models:
- /backend/app/models/learning_assets.py (LearningAsset, AssetSuggestionLog)
- /backend/app/models/galaxy.py (KnowledgeNode, NodeRelation, UserNodeStatus)
- /backend/app/models/base.py (BaseModel with soft delete)

Python Services:
- /backend/app/services/learning_asset_service.py (1042è¡Œï¼Œå·²æœ‰SRS/äº‹ä»¶å†™å…¥)
- /backend/app/services/galaxy_service.py
- /backend/app/services/galaxy_grpc_service.py

Alembic Migrations:
- /backend/alembic/versions/

Flutter Offline:
- /mobile/lib/core/offline/local_database.dart (Isar schema)
- /mobile/lib/core/offline/sync_engine.dart

Proto:
- /proto/galaxy_service.proto (å·²æœ‰UpdateNodeMastery, SyncCollaborativeGalaxy)
```

---

## M0ï¼šç°çŠ¶éªŒè¯ä¸åŸºçº¿å¯¹é½ï¼ˆ4å°æ—¶ï¼‰

### M0.1 éªŒè¯åŸºç¡€è®¾æ–½å¯ç”¨æ€§

```bash
# 1. éªŒè¯äº‹ä»¶outboxå†™å…¥
# åœ¨Pythonä¸­æµ‹è¯•:
from sqlalchemy import text
await db.execute(text("SELECT * FROM event_outbox LIMIT 5"))
await db.execute(text("SELECT * FROM event_sequence_counters LIMIT 5"))

# 2. éªŒè¯fingerprintç”Ÿæˆ
from app.core.fingerprint import generate_fingerprints
fp = generate_fingerprints("polymorphism")
assert fp.selection_fp is not None

# 3. éªŒè¯provenanceåŒ¹é…
from app.models.learning_assets import MatchStrength
assert MatchStrength.STRONG.value == "STRONG"
```

### M0.2 è¾“å‡ºåŸºçº¿æ–‡æ¡£

åˆ›å»ºæ–‡ä»¶ `/docs/PHASE9_BASELINE.md`:

```markdown
# Phase 9 Baseline

## ç°æœ‰è¡¨ç»“æ„

### event_outbox
- id, aggregate_type, aggregate_id, event_type, event_version, sequence_number, payload, metadata, created_at, processed_at

### event_sequence_counters
- aggregate_type, aggregate_id, next_sequence

### learning_assets
- [åˆ—å‡ºæ‰€æœ‰å­—æ®µ]

### knowledge_nodes
- [åˆ—å‡ºæ‰€æœ‰å­—æ®µï¼Œç‰¹åˆ«æ ‡æ³¨ position_x/y]

### node_relations
- âš ï¸ æ— user_idï¼Œæ˜¯å…¨å±€è¾¹

### user_node_status
- æœ‰revisionå­—æ®µç”¨äºå†²çªæ£€æµ‹
- æœ‰bkt_mastery_probç”¨äºBKT

## Phase 9 äº‹ä»¶ç±»å‹é›†åˆ

| äº‹ä»¶ç±»å‹ | aggregate_type | payloadæœ€å°å­—æ®µ |
|---------|----------------|----------------|
| asset_concept_link.upserted | asset_concept_link | asset_id, concept_id, link_type, confidence |
| asset_concept_link.deleted | asset_concept_link | asset_id, concept_id |
| user_node_relation.upserted | user_node_relation | src_concept_id, dst_concept_id, relation_type, weight |
| review.calibrated | learning_asset | asset_id, difficulty, predicted_recall, actual_recall, brier_error |
```

### M0.3 éªŒæ”¶æ ‡å‡†
- [ ] èƒ½æŸ¥è¯¢event_outboxå¹¶çœ‹åˆ°å†å²äº‹ä»¶
- [ ] fingerprintç”Ÿæˆé“¾è·¯å¯ç”¨
- [ ] è¾“å‡ºPHASE9_BASELINE.md

---

## M1ï¼šæ•°æ®æ¨¡å‹æ‰©å±•ï¼ˆ1å¤©ï¼‰

### M1.1 æ–°è¡¨ï¼šasset_concept_links

**æ–‡ä»¶**: `/backend/alembic/versions/p20_add_asset_concept_links.py`

```python
"""add asset_concept_links table

Revision ID: p20
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

def upgrade():
    op.create_table(
        'asset_concept_links',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('users.id', ondelete='CASCADE'), nullable=False),
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('learning_assets.id', ondelete='CASCADE'), nullable=False),
        sa.Column('concept_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('knowledge_nodes.id', ondelete='CASCADE'), nullable=False),
        sa.Column('link_type', sa.String(32), nullable=False),  # provenance | co_activation | manual
        sa.Column('confidence', sa.Float, nullable=False, server_default='1.0'),
        sa.Column('metadata', postgresql.JSONB, nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
        sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
    )

    # éƒ¨åˆ†å”¯ä¸€ç´¢å¼•ï¼ˆè½¯åˆ é™¤å‹å¥½ï¼‰
    op.create_index(
        'uix_asset_concept_link_unique',
        'asset_concept_links',
        ['user_id', 'asset_id', 'concept_id', 'link_type'],
        unique=True,
        postgresql_where=sa.text('deleted_at IS NULL')
    )

    op.create_index('idx_acl_user_asset', 'asset_concept_links', ['user_id', 'asset_id'], postgresql_where=sa.text('deleted_at IS NULL'))
    op.create_index('idx_acl_user_concept', 'asset_concept_links', ['user_id', 'concept_id'], postgresql_where=sa.text('deleted_at IS NULL'))

def downgrade():
    op.drop_table('asset_concept_links')
```

**ORM Model**: `/backend/app/models/asset_concept_link.py`

```python
"""Asset-Concept Link Model"""
import enum
from sqlalchemy import Column, String, Float, ForeignKey, DateTime
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship
from app.models.base import BaseModel, GUID


class LinkType(str, enum.Enum):
    PROVENANCE = "provenance"       # æ¥è‡ªæ–‡æ¡£æº¯æº
    CO_ACTIVATION = "co_activation"  # åŒä¼šè¯æ¿€æ´»
    MANUAL = "manual"               # ç”¨æˆ·æ‰‹åŠ¨å…³è”


class AssetConceptLink(BaseModel):
    """
    èµ„äº§-æ¦‚å¿µå…³è”è¡¨

    è¿æ¥LearningAssetï¼ˆç”¨æˆ·æ”¶é›†çš„è¯æ±‡/å¥å­ï¼‰ä¸KnowledgeNodeï¼ˆçŸ¥è¯†å›¾è°±æ¦‚å¿µï¼‰
    """
    __tablename__ = "asset_concept_links"

    user_id = Column(GUID(), ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
    asset_id = Column(GUID(), ForeignKey("learning_assets.id", ondelete="CASCADE"), nullable=False, index=True)
    concept_id = Column(GUID(), ForeignKey("knowledge_nodes.id", ondelete="CASCADE"), nullable=False, index=True)
    link_type = Column(String(32), nullable=False)
    confidence = Column(Float, nullable=False, default=1.0)
    metadata = Column(JSONB, nullable=True)

    # Relationships
    asset = relationship("LearningAsset")
    concept = relationship("KnowledgeNode")
```

### M1.2 æ‰©å±•node_relationsæ”¯æŒç”¨æˆ·ç§æœ‰è¾¹

**æ–¹æ¡ˆAï¼ˆæ¨èï¼‰**: ç»™ç°æœ‰`node_relations`è¡¨æ·»åŠ `user_id`å­—æ®µ

```python
# Migration: p21_add_user_id_to_node_relations.py

def upgrade():
    op.add_column('node_relations', sa.Column('user_id', postgresql.UUID(as_uuid=True), nullable=True))
    op.create_foreign_key('fk_node_relations_user', 'node_relations', 'users', ['user_id'], ['id'], ondelete='CASCADE')
    op.create_index('idx_nr_user_relation', 'node_relations', ['user_id', 'relation_type'], postgresql_where=sa.text('deleted_at IS NULL'))

def downgrade():
    op.drop_constraint('fk_node_relations_user', 'node_relations', type_='foreignkey')
    op.drop_column('node_relations', 'user_id')
```

**çº¦å®š**:
- `user_id IS NULL` â†’ å…¨å±€è¾¹ï¼ˆseed/systemç”Ÿæˆï¼‰
- `user_id IS NOT NULL` â†’ ç”¨æˆ·ç§æœ‰è¾¹ï¼ˆco_activation/co_reviewï¼‰

### M1.3 æ–°è¡¨ï¼šreview_calibration_logs

```python
# Migration: p22_add_review_calibration_logs.py

def upgrade():
    op.create_table(
        'review_calibration_logs',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('user_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('users.id', ondelete='CASCADE'), nullable=False),
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('learning_assets.id', ondelete='SET NULL'), nullable=True),
        sa.Column('concept_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('knowledge_nodes.id', ondelete='SET NULL'), nullable=True),
        sa.Column('reviewed_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('difficulty', sa.String(16), nullable=False),  # easy/good/hard
        sa.Column('predicted_recall', sa.Float, nullable=True),
        sa.Column('actual_recall', sa.Boolean, nullable=True),
        sa.Column('brier_error', sa.Float, nullable=True),
        sa.Column('review_count', sa.Integer, nullable=False),
        sa.Column('interval_days_before', sa.Integer, nullable=True),
        sa.Column('interval_days_after', sa.Integer, nullable=True),
        sa.Column('explanation_code', sa.String(50), nullable=True),  # learning_difficulty_adjusted, etc.
        sa.Column('metadata', postgresql.JSONB, nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    )

    op.create_index('idx_rcl_user_reviewed', 'review_calibration_logs', ['user_id', 'reviewed_at'])
    op.create_index('idx_rcl_asset', 'review_calibration_logs', ['asset_id'])

def downgrade():
    op.drop_table('review_calibration_logs')
```

### M1.4 éªŒæ”¶æ ‡å‡†
- [ ] `alembic upgrade head` æˆåŠŸ
- [ ] `alembic downgrade -1` å¯å›æ»šï¼ˆéªŒè¯åå†upgradeå›æ¥ï¼‰
- [ ] éƒ¨åˆ†å”¯ä¸€ç´¢å¼•æµ‹è¯•ï¼šåŒä¸€(user, asset, concept, link_type)ä¸èƒ½é‡å¤æ’å…¥ï¼ˆé™¤éè½¯åˆ ï¼‰
- [ ] æ’å…¥æµ‹è¯•æ•°æ®ç¡®è®¤å…³ç³»æ­£å¸¸

---

## M2ï¼šå›¾è°±é£è½®ï¼ˆèµ„äº§â†’æ¦‚å¿µé“¾æ¥ç”Ÿæˆï¼‰ï¼ˆ1.5å¤©ï¼‰

### M2.1 AssetConceptLinkService

**æ–‡ä»¶**: `/backend/app/services/asset_concept_link_service.py`

```python
"""
Asset-Concept Link Service

æ ¸å¿ƒèŒè´£:
1. èµ„äº§åˆ›å»ºæ—¶è‡ªåŠ¨ç”Ÿæˆ provenance é“¾æ¥
2. æ ¹æ®headwordåŒ¹é…æˆ–åˆ›å»ºconceptèŠ‚ç‚¹
3. å†™å…¥äº‹ä»¶åˆ°outbox
"""
from datetime import datetime, timezone
from typing import Optional, List
from uuid import UUID
import json

from sqlalchemy import select, and_, text
from sqlalchemy.ext.asyncio import AsyncSession
from loguru import logger

from app.models.asset_concept_link import AssetConceptLink, LinkType
from app.models.learning_assets import LearningAsset, MatchStrength
from app.models.galaxy import KnowledgeNode


# === æŠ¤æ å¸¸é‡ ===
MAX_METADATA_BYTES = 2048


class AssetConceptLinkService:
    """èµ„äº§-æ¦‚å¿µé“¾æ¥æœåŠ¡"""

    async def generate_links_for_asset(
        self,
        db: AsyncSession,
        asset: LearningAsset,
    ) -> List[AssetConceptLink]:
        """
        ä¸ºæ–°èµ„äº§ç”Ÿæˆæ¦‚å¿µé“¾æ¥

        è§„åˆ™:
        1. æ ¹æ®provenance.match_strengthç¡®å®šconfidence
        2. å°è¯•åŒ¹é…å·²æœ‰conceptï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        3. å†™å…¥äº‹ä»¶åˆ°outbox
        """
        links = []

        # 1. è·å–æˆ–åˆ›å»ºconcept
        concept = await self._get_or_create_concept(
            db=db,
            headword=asset.headword,
            user_id=asset.user_id,
            language_code=asset.language_code,
        )

        # 2. è®¡ç®—confidence
        provenance = asset.provenance_json or {}
        match_strength = provenance.get("match_strength", "ORPHAN")
        confidence = self._strength_to_confidence(match_strength)

        # 3. æ„å»ºmetadataï¼ˆé™é•¿ï¼‰
        metadata = self._build_limited_metadata(provenance)

        # 4. Upserté“¾æ¥
        link = await self._upsert_link(
            db=db,
            user_id=asset.user_id,
            asset_id=asset.id,
            concept_id=concept.id,
            link_type=LinkType.PROVENANCE,
            confidence=confidence,
            metadata=metadata,
        )
        links.append(link)

        logger.info(f"Generated {len(links)} links for asset {asset.id} â†’ concept {concept.id}")
        return links

    async def _get_or_create_concept(
        self,
        db: AsyncSession,
        headword: str,
        user_id: UUID,
        language_code: str,
    ) -> KnowledgeNode:
        """è·å–æˆ–åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹"""
        # æŒ‰headwordç²¾ç¡®åŒ¹é…
        query = select(KnowledgeNode).where(
            and_(
                KnowledgeNode.name == headword,
                KnowledgeNode.deleted_at.is_(None),
            )
        ).limit(1)

        result = await db.execute(query)
        existing = result.scalar_one_or_none()

        if existing:
            return existing

        # åˆ›å»ºæœ€å°èŠ‚ç‚¹
        node = KnowledgeNode(
            name=headword,
            source_type="user_created",
            status="published",
            # positionå°†ç”±LayoutServiceè®¡ç®—
        )
        db.add(node)
        await db.flush()
        await db.refresh(node)

        # å†™å…¥äº‹ä»¶
        await self._write_event(
            db=db,
            aggregate_type="knowledge_node",
            aggregate_id=node.id,
            event_type="node_created",
            payload={"name": headword, "source": "asset_link"},
        )

        return node

    def _strength_to_confidence(self, strength: str) -> float:
        """åŒ¹é…å¼ºåº¦è½¬ç½®ä¿¡åº¦"""
        mapping = {
            "STRONG": 0.9,
            "WEAK": 0.6,
            "ORPHAN": 0.4,
        }
        return mapping.get(strength, 0.4)

    def _build_limited_metadata(self, provenance: dict) -> Optional[dict]:
        """æ„å»ºé™é•¿metadata"""
        if not provenance:
            return None

        # åªä¿ç•™å…³é”®å­—æ®µï¼ˆä¸å«å…¨æ–‡ï¼‰
        allowed_keys = ["chunk_id", "doc_id", "page_no", "score", "match_strength"]
        filtered = {k: v for k, v in provenance.items() if k in allowed_keys}

        # é™é•¿æ£€æŸ¥
        serialized = json.dumps(filtered)
        if len(serialized) > MAX_METADATA_BYTES:
            filtered["truncated"] = True
            filtered["original_size"] = len(serialized)
            # ç§»é™¤æœ€å¤§çš„å­—æ®µç›´åˆ°æ»¡è¶³é™åˆ¶
            while len(json.dumps(filtered)) > MAX_METADATA_BYTES and len(filtered) > 2:
                largest_key = max(filtered.keys(), key=lambda k: len(str(filtered.get(k, ""))))
                if largest_key not in ["truncated", "original_size"]:
                    del filtered[largest_key]

        return filtered

    async def _upsert_link(
        self,
        db: AsyncSession,
        user_id: UUID,
        asset_id: UUID,
        concept_id: UUID,
        link_type: LinkType,
        confidence: float,
        metadata: Optional[dict],
    ) -> AssetConceptLink:
        """Upserté“¾æ¥ï¼ˆéƒ¨åˆ†å”¯ä¸€ç´¢å¼•å‹å¥½ï¼‰"""
        # æŸ¥æ‰¾å·²æœ‰é“¾æ¥
        query = select(AssetConceptLink).where(
            and_(
                AssetConceptLink.user_id == user_id,
                AssetConceptLink.asset_id == asset_id,
                AssetConceptLink.concept_id == concept_id,
                AssetConceptLink.link_type == link_type.value,
                AssetConceptLink.deleted_at.is_(None),
            )
        )
        result = await db.execute(query)
        existing = result.scalar_one_or_none()

        if existing:
            existing.confidence = confidence
            existing.metadata = metadata
            existing.updated_at = datetime.now(timezone.utc)
            link = existing
        else:
            link = AssetConceptLink(
                user_id=user_id,
                asset_id=asset_id,
                concept_id=concept_id,
                link_type=link_type.value,
                confidence=confidence,
                metadata=metadata,
            )
            db.add(link)

        await db.flush()
        if not existing:
            await db.refresh(link)

        # å†™å…¥äº‹ä»¶
        await self._write_event(
            db=db,
            aggregate_type="asset_concept_link",
            aggregate_id=link.id,
            event_type="link_upserted",
            payload={
                "asset_id": str(asset_id),
                "concept_id": str(concept_id),
                "link_type": link_type.value,
                "confidence": confidence,
            },
        )

        return link

    async def _write_event(
        self,
        db: AsyncSession,
        aggregate_type: str,
        aggregate_id: UUID,
        event_type: str,
        payload: dict,
    ) -> None:
        """å†™å…¥äº‹ä»¶åˆ°outboxï¼ˆå¤ç”¨ç°æœ‰æœºåˆ¶ï¼‰"""
        seq_result = await db.execute(
            text("""
                INSERT INTO event_sequence_counters (aggregate_type, aggregate_id, next_sequence)
                VALUES (:aggregate_type, :aggregate_id, 1)
                ON CONFLICT (aggregate_type, aggregate_id)
                DO UPDATE SET next_sequence = event_sequence_counters.next_sequence + 1
                RETURNING next_sequence
            """),
            {"aggregate_type": aggregate_type, "aggregate_id": aggregate_id}
        )
        sequence_number = seq_result.scalar()

        await db.execute(
            text("""
                INSERT INTO event_outbox
                (aggregate_type, aggregate_id, event_type, event_version, sequence_number, payload, metadata)
                VALUES (:aggregate_type, :aggregate_id, :event_type, 1, :sequence_number, :payload, :metadata)
            """),
            {
                "aggregate_type": aggregate_type,
                "aggregate_id": aggregate_id,
                "event_type": event_type,
                "sequence_number": sequence_number,
                "payload": json.dumps(payload),
                "metadata": json.dumps({"service": "asset_concept_link_service"}),
            }
        )


# Singleton
asset_concept_link_service = AssetConceptLinkService()
```

### M2.2 é›†æˆåˆ°LearningAssetService

åœ¨ `/backend/app/services/learning_asset_service.py` çš„ `create_asset_from_selection` æ–¹æ³•æœ«å°¾æ·»åŠ :

```python
# åœ¨ return asset ä¹‹å‰æ·»åŠ :

# 7. Generate concept links (async, non-blocking)
try:
    from app.services.asset_concept_link_service import asset_concept_link_service
    await asset_concept_link_service.generate_links_for_asset(db, asset)
except Exception as e:
    # é“¾æ¥ç”Ÿæˆå¤±è´¥ä¸åº”é˜»æ–­èµ„äº§åˆ›å»º
    logger.warning(f"Failed to generate concept links for asset {asset.id}: {e}")

return asset
```

### M2.3 ç”¨æˆ·ç§æœ‰è¾¹ç”Ÿæˆï¼ˆco_activationï¼‰

åˆ›å»ºCeleryä»»åŠ¡ `/backend/app/tasks/co_activation_builder.py`:

```python
"""
Co-Activation Edge Builder

æ¯æ—¥ä»»åŠ¡ï¼šåˆ†æç”¨æˆ·è¡Œä¸ºï¼Œç”Ÿæˆæ¦‚å¿µé—´co_activationè¾¹
"""
from datetime import datetime, timedelta, timezone
from sqlalchemy import select, and_, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.asset_concept_link import AssetConceptLink
from app.models.galaxy import NodeRelation


async def build_co_activation_edges(db: AsyncSession, user_id: UUID, window_days: int = 7):
    """
    æ„å»ºç”¨æˆ·çš„co_activationè¾¹

    è§„åˆ™ï¼šåŒä¸€ç”¨æˆ·åœ¨window_dayså†…æ¿€æ´»çš„èµ„äº§ï¼Œå…¶å…³è”æ¦‚å¿µå½¢æˆè¾¹
    """
    cutoff = datetime.now(timezone.utc) - timedelta(days=window_days)

    # æŸ¥è¯¢ç”¨æˆ·è¿‘æœŸçš„èµ„äº§-æ¦‚å¿µé“¾æ¥
    query = select(
        AssetConceptLink.concept_id,
        func.array_agg(AssetConceptLink.asset_id).label("asset_ids"),
        func.count(AssetConceptLink.id).label("count"),
    ).where(
        and_(
            AssetConceptLink.user_id == user_id,
            AssetConceptLink.created_at >= cutoff,
            AssetConceptLink.deleted_at.is_(None),
        )
    ).group_by(AssetConceptLink.concept_id)

    result = await db.execute(query)
    concept_data = result.fetchall()

    # æ„å»ºå…±ç°çŸ©é˜µå¹¶ç”Ÿæˆè¾¹
    concepts = [row.concept_id for row in concept_data]
    for i, c1 in enumerate(concepts):
        for c2 in concepts[i+1:]:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¾¹
            existing = await db.execute(
                select(NodeRelation).where(
                    and_(
                        NodeRelation.source_node_id == c1,
                        NodeRelation.target_node_id == c2,
                        NodeRelation.user_id == user_id,
                        NodeRelation.relation_type == "co_activation",
                        NodeRelation.deleted_at.is_(None),
                    )
                )
            )
            if existing.scalar_one_or_none():
                # æ›´æ–°æƒé‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
                pass  # TODO: implement weight update
            else:
                # åˆ›å»ºæ–°è¾¹
                edge = NodeRelation(
                    source_node_id=c1,
                    target_node_id=c2,
                    user_id=user_id,
                    relation_type="co_activation",
                    strength=0.5,
                    created_by="system",
                )
                db.add(edge)

    await db.commit()
```

### M2.4 éªŒæ”¶æ ‡å‡†
- [ ] åˆ›å»ºèµ„äº§å1ç§’å†…å¯æŸ¥åˆ°`asset_concept_links`è®°å½•
- [ ] `event_outbox`ä¸­æœ‰`link_upserted`äº‹ä»¶
- [ ] Galaxyå¯æŸ¥è¯¢åˆ°æ–°æ¦‚å¿µèŠ‚ç‚¹ï¼ˆè‹¥headwordä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
- [ ] å¯¹åŒä¸€èµ„äº§é‡å¤è§¦å‘ä¸äº§ç”Ÿé‡å¤é“¾æ¥

---

## M3ï¼šåŒæ­¥é£è½®ï¼ˆäº‹ä»¶æµAPIï¼‰ï¼ˆ2å¤©ï¼‰

### M3.1 Protoå®šä¹‰æ‰©å±•

**æ–‡ä»¶**: `/proto/sync_service.proto`

```protobuf
syntax = "proto3";

package sync.v1;

option go_package = "github.com/sparkle/gateway/gen/sync/v1;syncv1";

import "google/protobuf/timestamp.proto";

service SyncService {
  // Bootstrap: è·å–åˆå§‹çŠ¶æ€å¿«ç…§
  rpc Bootstrap(BootstrapRequest) returns (BootstrapResponse);

  // GetEvents: è·å–å¢é‡äº‹ä»¶
  rpc GetEvents(GetEventsRequest) returns (GetEventsResponse);
}

message BootstrapRequest {
  string user_id = 1;
  string schema_version = 2;  // å®¢æˆ·ç«¯schemaç‰ˆæœ¬
}

message BootstrapResponse {
  string cursor = 1;  // ç”¨äºåç»­GetEvents
  SnapshotData snapshot = 2;
}

message SnapshotData {
  repeated LearningAssetSnapshot assets = 1;
  repeated AssetConceptLinkSnapshot links = 2;
  repeated ConceptSnapshot concepts = 3;
  repeated UserNodeStatusSnapshot statuses = 4;
}

message LearningAssetSnapshot {
  string id = 1;
  string status = 2;
  string headword = 3;
  string translation = 4;
  google.protobuf.Timestamp review_due_at = 5;
  int32 review_count = 6;
  google.protobuf.Timestamp updated_at = 7;
}

message AssetConceptLinkSnapshot {
  string id = 1;
  string asset_id = 2;
  string concept_id = 3;
  string link_type = 4;
  double confidence = 5;
}

message ConceptSnapshot {
  string id = 1;
  string name = 2;
  double position_x = 3;
  double position_y = 4;
  google.protobuf.Timestamp updated_at = 5;
}

message UserNodeStatusSnapshot {
  string node_id = 1;
  double mastery_score = 2;
  int64 revision = 3;
  google.protobuf.Timestamp next_review_at = 4;
}

message GetEventsRequest {
  string user_id = 1;
  string cursor = 2;  // ä¸Šæ¬¡è¿”å›çš„cursor
  int32 limit = 3;    // æœ€å¤§è¿”å›æ•°é‡ï¼Œé»˜è®¤100
}

message GetEventsResponse {
  repeated SyncEvent events = 1;
  string next_cursor = 2;
  bool has_more = 3;
}

message SyncEvent {
  string id = 1;
  string type = 2;           // e.g., "learning_asset.created"
  string aggregate_id = 3;
  int64 sequence = 4;
  google.protobuf.Timestamp occurred_at = 5;
  bytes payload = 6;         // JSON payload (é™é•¿)
}
```

### M3.2 Go Gatewayå®ç°

**æ–‡ä»¶**: `/backend/gateway/internal/handler/sync_handler.go`

```go
package handler

import (
    "encoding/json"
    "net/http"
    "strconv"
    "time"

    "github.com/gin-gonic/gin"
    "github.com/sparkle/gateway/internal/service"
)

// Payloadå¤§å°é™åˆ¶
const MaxEventPayloadBytes = 2048

type SyncHandler struct {
    syncService *service.SyncService
}

func NewSyncHandler(syncService *service.SyncService) *SyncHandler {
    return &SyncHandler{syncService: syncService}
}

// Bootstrap returns initial state snapshot
func (h *SyncHandler) Bootstrap(c *gin.Context) {
    userID := c.GetString("user_id") // from auth middleware

    snapshot, cursor, err := h.syncService.GetBootstrapData(c.Request.Context(), userID)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, gin.H{
        "cursor":   cursor,
        "snapshot": snapshot,
    })
}

// GetEvents returns incremental events after cursor
func (h *SyncHandler) GetEvents(c *gin.Context) {
    userID := c.GetString("user_id")
    cursor := c.Query("cursor")
    limitStr := c.DefaultQuery("limit", "100")
    limit, _ := strconv.Atoi(limitStr)
    if limit > 500 {
        limit = 500
    }

    events, nextCursor, hasMore, err := h.syncService.GetEvents(
        c.Request.Context(), userID, cursor, limit,
    )
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    // Enforce payload size limits
    for i := range events {
        events[i].Payload = truncatePayload(events[i].Payload, MaxEventPayloadBytes)
    }

    c.JSON(http.StatusOK, gin.H{
        "events":      events,
        "next_cursor": nextCursor,
        "has_more":    hasMore,
    })
}

func truncatePayload(payload json.RawMessage, maxBytes int) json.RawMessage {
    if len(payload) <= maxBytes {
        return payload
    }

    // Parse, truncate, re-serialize
    var data map[string]interface{}
    if err := json.Unmarshal(payload, &data); err != nil {
        return json.RawMessage(`{"truncated":true}`)
    }

    data["truncated"] = true
    data["original_size"] = len(payload)

    // Remove large fields until under limit
    for len(payload) > maxBytes && len(data) > 2 {
        var largestKey string
        var largestSize int
        for k, v := range data {
            if k == "truncated" || k == "original_size" {
                continue
            }
            size := len(fmt.Sprintf("%v", v))
            if size > largestSize {
                largestSize = size
                largestKey = k
            }
        }
        if largestKey != "" {
            delete(data, largestKey)
        }
        payload, _ = json.Marshal(data)
    }

    return payload
}
```

### M3.3 SyncServiceå®ç°

**æ–‡ä»¶**: `/backend/gateway/internal/service/sync_service.go`

```go
package service

import (
    "context"
    "encoding/base64"
    "encoding/json"
    "strconv"
    "time"

    "github.com/jackc/pgx/v5/pgxpool"
)

type SyncEvent struct {
    ID          string          `json:"id"`
    Type        string          `json:"type"`
    AggregateID string          `json:"aggregate_id"`
    Sequence    int64           `json:"sequence"`
    OccurredAt  time.Time       `json:"occurred_at"`
    Payload     json.RawMessage `json:"payload"`
}

type SyncService struct {
    db *pgxpool.Pool
}

func NewSyncService(db *pgxpool.Pool) *SyncService {
    return &SyncService{db: db}
}

func (s *SyncService) GetBootstrapData(ctx context.Context, userID string) (map[string]interface{}, string, error) {
    snapshot := make(map[string]interface{})

    // 1. Fetch learning assets
    assets, err := s.fetchUserAssets(ctx, userID)
    if err != nil {
        return nil, "", err
    }
    snapshot["assets"] = assets

    // 2. Fetch asset-concept links
    links, err := s.fetchUserLinks(ctx, userID)
    if err != nil {
        return nil, "", err
    }
    snapshot["links"] = links

    // 3. Fetch related concepts
    concepts, err := s.fetchRelatedConcepts(ctx, userID)
    if err != nil {
        return nil, "", err
    }
    snapshot["concepts"] = concepts

    // 4. Fetch user node statuses
    statuses, err := s.fetchUserNodeStatuses(ctx, userID)
    if err != nil {
        return nil, "", err
    }
    snapshot["statuses"] = statuses

    // 5. Generate cursor (based on max event ID or timestamp)
    cursor, err := s.getCurrentCursor(ctx, userID)
    if err != nil {
        return nil, "", err
    }

    return snapshot, cursor, nil
}

func (s *SyncService) GetEvents(ctx context.Context, userID, cursor string, limit int) ([]SyncEvent, string, bool, error) {
    // Decode cursor (format: "event_id:timestamp")
    afterID := int64(0)
    if cursor != "" {
        decoded, _ := base64.StdEncoding.DecodeString(cursor)
        afterID, _ = strconv.ParseInt(string(decoded), 10, 64)
    }

    // Query events
    query := `
        SELECT id, aggregate_type || '.' || event_type as type,
               aggregate_id, sequence_number, created_at, payload
        FROM event_outbox
        WHERE id > $1
          AND (
            aggregate_type = 'learning_asset'
            OR aggregate_type = 'asset_concept_link'
            OR aggregate_type = 'knowledge_node'
          )
        ORDER BY id ASC
        LIMIT $2
    `

    rows, err := s.db.Query(ctx, query, afterID, limit+1)
    if err != nil {
        return nil, "", false, err
    }
    defer rows.Close()

    var events []SyncEvent
    for rows.Next() {
        var e SyncEvent
        if err := rows.Scan(&e.ID, &e.Type, &e.AggregateID, &e.Sequence, &e.OccurredAt, &e.Payload); err != nil {
            return nil, "", false, err
        }
        events = append(events, e)
    }

    hasMore := len(events) > limit
    if hasMore {
        events = events[:limit]
    }

    nextCursor := ""
    if len(events) > 0 {
        lastID := events[len(events)-1].ID
        nextCursor = base64.StdEncoding.EncodeToString([]byte(lastID))
    }

    return events, nextCursor, hasMore, nil
}

// ... fetchUserAssets, fetchUserLinksç­‰è¾…åŠ©æ–¹æ³•çœç•¥
```

### M3.4 è·¯ç”±æ³¨å†Œ

```go
// backend/gateway/internal/router/router.go

func SetupRoutes(r *gin.Engine, ...) {
    // ... ç°æœ‰è·¯ç”±

    sync := r.Group("/api/v1/sync")
    sync.Use(authMiddleware.RequireAuth())
    {
        syncHandler := handler.NewSyncHandler(syncService)
        sync.GET("/bootstrap", syncHandler.Bootstrap)
        sync.GET("/events", syncHandler.GetEvents)
    }
}
```

### M3.5 éªŒæ”¶æ ‡å‡†
- [ ] `GET /api/v1/sync/bootstrap` è¿”å›ç”¨æˆ·çŠ¶æ€å¿«ç…§ + cursor
- [ ] `GET /api/v1/sync/events?cursor=xxx` è¿”å›å¢é‡äº‹ä»¶
- [ ] äº‹ä»¶payloadç¬¦åˆé™é•¿è¦æ±‚ï¼ˆ<2KBï¼‰
- [ ] åŒä¸€aggregateçš„äº‹ä»¶sequenceå•è°ƒé€’å¢
- [ ] cursorå¯é‡æ”¾ï¼ˆé‡å¤è¯·æ±‚è¿”å›ç¨³å®šç»“æœï¼‰

---

## M4ï¼šFlutteråŒæ­¥å¼•æ“ï¼ˆIsar + å¹‚ç­‰åº”ç”¨ï¼‰ï¼ˆ2å¤©ï¼‰

### M4.1 æ‰©å±•Isar Schema

**æ–‡ä»¶**: `/mobile/lib/core/offline/local_database.dart`

```dart
import 'package:isar/isar.dart';
import 'package:path_provider/path_provider.dart';

part 'local_database.g.dart';

enum SyncStatus {
  pending,
  synced,
  conflict,
  failed,
  waitingAck,
}

// === ç°æœ‰Collectionsä¿æŒä¸å˜ ===

@collection
class LocalKnowledgeNode {
  // ... ä¿æŒç°æœ‰ä»£ç 
}

@collection
class PendingUpdate {
  // ... ä¿æŒç°æœ‰ä»£ç 
}

@collection
class LocalCRDTSnapshot {
  // ... ä¿æŒç°æœ‰ä»£ç 
}

@collection
class OutboxItem {
  // ... ä¿æŒç°æœ‰ä»£ç 
}

// === Phase 9 æ–°å¢Collections ===

@collection
class LocalLearningAsset {
  Id id = Isar.autoIncrement;

  @Index(unique: true)
  late String serverId;

  late String status;  // INBOX, ACTIVE, ARCHIVED
  late String headword;
  String? translation;
  String? definition;

  DateTime? reviewDueAt;
  int reviewCount = 0;

  late DateTime updatedAt;

  @enumerated
  SyncStatus syncStatus = SyncStatus.synced;
}

@collection
class LocalAssetConceptLink {
  Id id = Isar.autoIncrement;

  @Index(unique: true)
  late String serverId;

  late String assetId;
  late String conceptId;
  late String linkType;
  double confidence = 1.0;

  late DateTime updatedAt;
}

@collection
class ProcessedEvent {
  Id id = Isar.autoIncrement;

  @Index(unique: true)
  late String eventId;  // æœåŠ¡ç«¯äº‹ä»¶ID

  late String aggregateId;
  late int sequence;
  late DateTime occurredAt;
  late DateTime processedAt;
}

@collection
class SyncState {
  Id id = Isar.autoIncrement;

  @Index(unique: true)
  String key = 'main';  // å•ä¾‹key

  String? cursor;
  DateTime? lastSyncAt;
  String schemaVersion = '1';
}

@collection
class SyncConflict {
  Id id = Isar.autoIncrement;

  late String entityType;  // asset, link, concept
  late String entityId;
  late String conflictType;  // version_mismatch, deleted_on_server
  late String localData;  // JSON
  late String serverData;  // JSON
  late DateTime detectedAt;
  bool resolved = false;
}

class LocalDatabase {
  factory LocalDatabase() => _instance;
  LocalDatabase._internal();
  static final LocalDatabase _instance = LocalDatabase._internal();

  late Isar isar;

  Future<void> init() async {
    final dir = await getApplicationDocumentsDirectory();
    isar = await Isar.open(
      [
        LocalKnowledgeNodeSchema,
        PendingUpdateSchema,
        LocalCRDTSnapshotSchema,
        OutboxItemSchema,
        // Phase 9 æ–°å¢
        LocalLearningAssetSchema,
        LocalAssetConceptLinkSchema,
        ProcessedEventSchema,
        SyncStateSchema,
        SyncConflictSchema,
      ],
      directory: dir.path,
    );
  }
}
```

### M4.2 SyncEngineé‡æ„

**æ–‡ä»¶**: `/mobile/lib/core/offline/sync_engine.dart`

```dart
import 'dart:async';
import 'dart:convert';

import 'package:connectivity_plus/connectivity_plus.dart';
import 'package:dio/dio.dart';
import 'package:isar/isar.dart';
import 'package:logger/logger.dart';
import 'package:sparkle/core/offline/local_database.dart';

class SyncEngine {
  SyncEngine(this._localDb, this._dio);

  final LocalDatabase _localDb;
  final Dio _dio;
  final Logger _logger = Logger();

  StreamSubscription<void>? _outboxSubscription;
  bool _isProcessing = false;
  bool _isBootstrapped = false;

  // === åˆå§‹åŒ– ===

  Future<void> initialize() async {
    // æ£€æŸ¥æ˜¯å¦éœ€è¦bootstrap
    final state = await _getSyncState();
    _isBootstrapped = state?.cursor != null;

    if (!_isBootstrapped) {
      await _performBootstrap();
    }

    // å¯åŠ¨å¢é‡åŒæ­¥
    _startIncrementalSync();
    _startOutboxProcessor();
  }

  Future<SyncState?> _getSyncState() async {
    return await _localDb.isar.syncStates
        .filter()
        .keyEqualTo('main')
        .findFirst();
  }

  // === Bootstrap ===

  Future<void> _performBootstrap() async {
    _logger.i('Starting bootstrap sync...');

    try {
      final response = await _dio.get('/api/v1/sync/bootstrap');
      final data = response.data;

      await _localDb.isar.writeTxn(() async {
        // 1. å†™å…¥assets
        for (final asset in data['snapshot']['assets'] ?? []) {
          final local = LocalLearningAsset()
            ..serverId = asset['id']
            ..status = asset['status']
            ..headword = asset['headword']
            ..translation = asset['translation']
            ..reviewDueAt = asset['review_due_at'] != null
                ? DateTime.parse(asset['review_due_at'])
                : null
            ..reviewCount = asset['review_count'] ?? 0
            ..updatedAt = DateTime.parse(asset['updated_at']);
          await _localDb.isar.localLearningAssets.put(local);
        }

        // 2. å†™å…¥links
        for (final link in data['snapshot']['links'] ?? []) {
          final local = LocalAssetConceptLink()
            ..serverId = link['id']
            ..assetId = link['asset_id']
            ..conceptId = link['concept_id']
            ..linkType = link['link_type']
            ..confidence = link['confidence'] ?? 1.0
            ..updatedAt = DateTime.now();
          await _localDb.isar.localAssetConceptLinks.put(local);
        }

        // 3. å†™å…¥concepts
        for (final concept in data['snapshot']['concepts'] ?? []) {
          final local = LocalKnowledgeNode()
            ..serverId = concept['id']
            ..name = concept['name']
            ..mastery = 0  // ä»statusesè·å–
            ..lastUpdated = DateTime.parse(concept['updated_at'])
            ..syncStatus = SyncStatus.synced;
          await _localDb.isar.localKnowledgeNodes.put(local);
        }

        // 4. æ›´æ–°sync state
        final state = SyncState()
          ..key = 'main'
          ..cursor = data['cursor']
          ..lastSyncAt = DateTime.now()
          ..schemaVersion = '1';
        await _localDb.isar.syncStates.put(state);
      });

      _isBootstrapped = true;
      _logger.i('Bootstrap complete, cursor: ${data['cursor']}');

    } catch (e) {
      _logger.e('Bootstrap failed: $e');
      rethrow;
    }
  }

  // === å¢é‡åŒæ­¥ ===

  void _startIncrementalSync() {
    // æ¯30ç§’æˆ–ç½‘ç»œæ¢å¤æ—¶è§¦å‘
    Timer.periodic(const Duration(seconds: 30), (_) => _pullEvents());

    Connectivity().onConnectivityChanged.listen((result) {
      if (!result.contains(ConnectivityResult.none) && _isBootstrapped) {
        _pullEvents();
      }
    });
  }

  Future<void> _pullEvents() async {
    if (_isProcessing || !_isBootstrapped) return;
    _isProcessing = true;

    try {
      final state = await _getSyncState();
      if (state == null) return;

      var cursor = state.cursor;
      var hasMore = true;

      while (hasMore) {
        final response = await _dio.get(
          '/api/v1/sync/events',
          queryParameters: {'cursor': cursor, 'limit': 100},
        );

        final events = response.data['events'] as List;

        for (final event in events) {
          await _applyEventIdempotent(event);
        }

        cursor = response.data['next_cursor'];
        hasMore = response.data['has_more'] ?? false;

        // æ›´æ–°cursor
        await _localDb.isar.writeTxn(() async {
          state.cursor = cursor;
          state.lastSyncAt = DateTime.now();
          await _localDb.isar.syncStates.put(state);
        });
      }

    } catch (e) {
      _logger.e('Pull events failed: $e');
    } finally {
      _isProcessing = false;
    }
  }

  // === å¹‚ç­‰äº‹ä»¶åº”ç”¨ ===

  Future<void> _applyEventIdempotent(Map<String, dynamic> event) async {
    final eventId = event['id'] as String;

    // 1. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    final existing = await _localDb.isar.processedEvents
        .filter()
        .eventIdEqualTo(eventId)
        .findFirst();

    if (existing != null) {
      _logger.d('Skipping already processed event: $eventId');
      return;
    }

    // 2. åº”ç”¨äº‹ä»¶
    final type = event['type'] as String;
    final payload = event['payload'];

    await _localDb.isar.writeTxn(() async {
      switch (type) {
        case 'learning_asset.created':
        case 'learning_asset.updated':
          await _applyAssetEvent(payload);
          break;
        case 'learning_asset.status_changed':
          await _applyAssetStatusChange(payload);
          break;
        case 'asset_concept_link.upserted':
          await _applyLinkUpsert(payload);
          break;
        case 'asset_concept_link.deleted':
          await _applyLinkDelete(payload);
          break;
        case 'knowledge_node.created':
        case 'knowledge_node.updated':
          await _applyConceptEvent(payload);
          break;
        default:
          _logger.w('Unknown event type: $type');
      }

      // 3. è®°å½•å·²å¤„ç†
      final processed = ProcessedEvent()
        ..eventId = eventId
        ..aggregateId = event['aggregate_id']
        ..sequence = event['sequence']
        ..occurredAt = DateTime.parse(event['occurred_at'])
        ..processedAt = DateTime.now();
      await _localDb.isar.processedEvents.put(processed);
    });
  }

  Future<void> _applyAssetEvent(Map<String, dynamic> payload) async {
    final serverId = payload['asset_id'] ?? payload['id'];
    final existing = await _localDb.isar.localLearningAssets
        .filter()
        .serverIdEqualTo(serverId)
        .findFirst();

    if (existing != null) {
      // LWW: æ›´æ–°ç°æœ‰
      existing.status = payload['status'] ?? existing.status;
      existing.headword = payload['headword'] ?? existing.headword;
      existing.updatedAt = DateTime.now();
      await _localDb.isar.localLearningAssets.put(existing);
    } else {
      // åˆ›å»ºæ–°çš„
      final local = LocalLearningAsset()
        ..serverId = serverId
        ..status = payload['status'] ?? 'INBOX'
        ..headword = payload['headword'] ?? ''
        ..updatedAt = DateTime.now();
      await _localDb.isar.localLearningAssets.put(local);
    }
  }

  Future<void> _applyAssetStatusChange(Map<String, dynamic> payload) async {
    // ç±»ä¼¼é€»è¾‘...
  }

  Future<void> _applyLinkUpsert(Map<String, dynamic> payload) async {
    // ç±»ä¼¼é€»è¾‘...
  }

  Future<void> _applyLinkDelete(Map<String, dynamic> payload) async {
    final assetId = payload['asset_id'];
    final conceptId = payload['concept_id'];

    final link = await _localDb.isar.localAssetConceptLinks
        .filter()
        .assetIdEqualTo(assetId)
        .and()
        .conceptIdEqualTo(conceptId)
        .findFirst();

    if (link != null) {
      await _localDb.isar.localAssetConceptLinks.delete(link.id);
    }
  }

  Future<void> _applyConceptEvent(Map<String, dynamic> payload) async {
    // ç±»ä¼¼é€»è¾‘...
  }

  // === Outboxå¤„ç†ï¼ˆä¿æŒç°æœ‰é€»è¾‘ï¼‰===

  void _startOutboxProcessor() {
    // ... ä¿æŒç°æœ‰outboxå¤„ç†é€»è¾‘
  }

  void stop() {
    _outboxSubscription?.cancel();
  }
}
```

### M4.3 éªŒæ”¶æ ‡å‡†
- [ ] æ–°è®¾å¤‡é¦–æ¬¡å¯åŠ¨æ‰§è¡Œbootstrapï¼Œæœ¬åœ°æœ‰å®Œæ•´æ•°æ®
- [ ] å¢é‡äº‹ä»¶æ‹‰å–åprocessed_eventsè®°å½•
- [ ] é‡å¤äº‹ä»¶ä¸é‡å¤å¤„ç†ï¼ˆå¹‚ç­‰æ€§ï¼‰
- [ ] ç¦»çº¿åˆ›å»ºèµ„äº§â†’æ¢å¤ç½‘ç»œâ†’åŒæ­¥åˆ°æœåŠ¡ç«¯â†’äº‹ä»¶å›æµåˆ°æœ¬åœ°

---

## M5ï¼šä¸ªæ€§åŒ–è°ƒåº¦ + Galaxyå¢é‡å¸ƒå±€ï¼ˆ1.5å¤©ï¼‰

### M5.1 å¤ä¹ æ ¡å‡†é—­ç¯

æ‰©å±• `/backend/app/services/learning_asset_service.py`:

```python
# åœ¨LearningAssetServiceç±»ä¸­æ·»åŠ 

# === åŠ¨æ€é—´éš”è°ƒæ•´ ===

async def record_review_with_calibration(
    self,
    db: AsyncSession,
    user_id: UUID,
    asset_id: UUID,
    difficulty: str,
) -> Tuple[LearningAsset, Dict[str, Any]]:
    """
    è®°å½•å¤ä¹ ç»“æœå¹¶è¿›è¡Œä¸ªæ€§åŒ–æ ¡å‡†

    Returns:
        (æ›´æ–°åçš„asset, æ ¡å‡†ä¿¡æ¯å­—å…¸)
    """
    asset = await self.get_asset_by_id(db, asset_id, user_id)
    if not asset:
        raise ValueError(f"Asset not found: {asset_id}")

    # 1. è®°å½•æ ¡å‡†æ—¥å¿—
    calibration = await self._record_calibration(db, asset, difficulty)

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´é—´éš”
    adjustment = await self._check_interval_adjustment(db, user_id, asset_id)

    # 3. åº”ç”¨è°ƒæ•´åçš„é—´éš”
    if adjustment["should_adjust"]:
        adjusted_intervals = self._get_adjusted_intervals(adjustment)
        base_interval = adjusted_intervals[min(asset.review_count, len(adjusted_intervals)-1)]
    else:
        base_interval = self.REVIEW_INTERVALS[difficulty][min(asset.review_count, 5)]

    # 4. æ›´æ–°èµ„äº§
    now = datetime.now(timezone.utc)
    asset.review_count += 1
    asset.last_seen_at = now
    asset.review_due_at = now + timedelta(days=base_interval)

    # 5. æ›´æ–°æˆåŠŸç‡
    success_score = {'easy': 1.0, 'good': 0.7, 'hard': 0.3}[difficulty]
    alpha = 0.3
    if asset.review_count == 1:
        asset.review_success_rate = success_score
    else:
        asset.review_success_rate = alpha * success_score + (1-alpha) * asset.review_success_rate

    await db.flush()

    # 6. å†™å…¥äº‹ä»¶
    await self._write_event_outbox(
        db=db,
        aggregate_type="learning_asset",
        aggregate_id=asset_id,
        event_type="review_calibrated",
        payload={
            "difficulty": difficulty,
            "interval_days": base_interval,
            "review_count": asset.review_count,
            "explanation_code": adjustment.get("explanation_code", "standard"),
            "adjustment_factor": adjustment.get("factor", 1.0),
        },
    )

    return asset, {
        "interval_days": base_interval,
        "explanation_code": adjustment.get("explanation_code", "standard"),
        "next_review_at": asset.review_due_at.isoformat(),
    }

async def _record_calibration(
    self,
    db: AsyncSession,
    asset: LearningAsset,
    difficulty: str,
) -> "ReviewCalibrationLog":
    """è®°å½•æ ¡å‡†æ—¥å¿—"""
    from app.models.review_calibration import ReviewCalibrationLog

    log = ReviewCalibrationLog(
        user_id=asset.user_id,
        asset_id=asset.id,
        reviewed_at=datetime.now(timezone.utc),
        difficulty=difficulty,
        review_count=asset.review_count + 1,
    )
    db.add(log)
    await db.flush()
    return log

async def _check_interval_adjustment(
    self,
    db: AsyncSession,
    user_id: UUID,
    asset_id: UUID,
) -> Dict[str, Any]:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦é—´éš”è°ƒæ•´"""
    # æŸ¥è¯¢æœ€è¿‘Næ¬¡å¤ä¹ 
    from app.models.review_calibration import ReviewCalibrationLog

    query = select(ReviewCalibrationLog).where(
        and_(
            ReviewCalibrationLog.user_id == user_id,
            ReviewCalibrationLog.asset_id == asset_id,
        )
    ).order_by(ReviewCalibrationLog.reviewed_at.desc()).limit(5)

    result = await db.execute(query)
    recent_logs = list(result.scalars().all())

    # è§„åˆ™ï¼šè¿ç»­3æ¬¡hard â†’ ç¼©çŸ­é—´éš”
    if len(recent_logs) >= 3:
        last_three = [log.difficulty for log in recent_logs[:3]]
        if last_three == ["hard", "hard", "hard"]:
            return {
                "should_adjust": True,
                "factor": 0.5,  # é—´éš”å‡åŠ
                "explanation_code": "learning_difficulty_adjusted",
            }

    # è§„åˆ™ï¼šè¿ç»­3æ¬¡easy â†’ å»¶é•¿é—´éš”
    if len(recent_logs) >= 3:
        last_three = [log.difficulty for log in recent_logs[:3]]
        if last_three == ["easy", "easy", "easy"]:
            return {
                "should_adjust": True,
                "factor": 1.5,  # é—´éš”å»¶é•¿50%
                "explanation_code": "mastery_accelerated",
            }

    return {"should_adjust": False}

def _get_adjusted_intervals(self, adjustment: Dict) -> List[int]:
    """è·å–è°ƒæ•´åçš„é—´éš”è¡¨"""
    factor = adjustment.get("factor", 1.0)
    base = self.REVIEW_INTERVALS["good"]
    return [max(1, int(d * factor)) for d in base]
```

### M5.2 Galaxyå¢é‡å¸ƒå±€

**æ–‡ä»¶**: `/backend/app/services/galaxy_layout_service.py`

```python
"""
Galaxy Layout Service

èŒè´£ï¼š
1. ä¸ºæ–°æ¦‚å¿µè®¡ç®—å¢é‡ä½ç½®ï¼ˆä¸è§¦å‘å…¨å±€é‡æ’ï¼‰
2. æ”¯æŒä½ç½®é”å®š
3. ä½ç½®æ›´æ–°å†·å´ï¼ˆ24å°æ—¶å†…ä¸é‡æ’ï¼‰
"""
import random
from datetime import datetime, timedelta, timezone
from typing import Optional, Tuple, List
from uuid import UUID

import numpy as np
from sqlalchemy import select, and_
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.galaxy import KnowledgeNode
from app.services.embedding_service import embedding_service


class GalaxyLayoutService:
    """Galaxyå¸ƒå±€æœåŠ¡"""

    # ä½ç½®æ›´æ–°å†·å´æ—¶é—´
    POSITION_COOLDOWN_HOURS = 24

    # é»˜è®¤å¸ƒå±€èŒƒå›´
    LAYOUT_RANGE = 1000.0  # -500 to 500

    async def compute_position_for_concept(
        self,
        db: AsyncSession,
        concept: KnowledgeNode,
        force: bool = False,
    ) -> Tuple[float, float]:
        """
        ä¸ºæ¦‚å¿µè®¡ç®—ä½ç½®

        ç­–ç•¥ï¼š
        1. å¦‚æœæœ‰embeddingï¼Œæ‰¾kNNé‚»å±…å–è´¨å¿ƒ
        2. å¦åˆ™éšæœºä½ç½®
        3. æ·»åŠ å¾®æ‰°åŠ¨é¿å…é‡å 
        """
        # æ£€æŸ¥å†·å´
        if not force and self._is_position_locked(concept):
            return concept.position_x, concept.position_y

        if concept.embedding is not None:
            x, y = await self._compute_knn_position(db, concept)
        else:
            x, y = self._random_position()

        # å¾®æ‰°åŠ¨
        x += random.uniform(-5, 5)
        y += random.uniform(-5, 5)

        # æ›´æ–°ä½ç½®
        concept.position_x = x
        concept.position_y = y
        concept.updated_at = datetime.now(timezone.utc)

        await db.flush()

        return x, y

    async def _compute_knn_position(
        self,
        db: AsyncSession,
        concept: KnowledgeNode,
        k: int = 3,
    ) -> Tuple[float, float]:
        """åŸºäºkNNè®¡ç®—è´¨å¿ƒä½ç½®"""
        # æŸ¥æ‰¾æœ‰embeddingå’Œä½ç½®çš„é‚»å±…èŠ‚ç‚¹
        query = select(KnowledgeNode).where(
            and_(
                KnowledgeNode.id != concept.id,
                KnowledgeNode.embedding.isnot(None),
                KnowledgeNode.position_x.isnot(None),
                KnowledgeNode.deleted_at.is_(None),
            )
        ).order_by(
            KnowledgeNode.embedding.cosine_distance(concept.embedding)
        ).limit(k)

        result = await db.execute(query)
        neighbors = list(result.scalars().all())

        if not neighbors:
            return self._random_position()

        # è®¡ç®—è´¨å¿ƒ
        xs = [n.position_x for n in neighbors if n.position_x is not None]
        ys = [n.position_y for n in neighbors if n.position_y is not None]

        if not xs or not ys:
            return self._random_position()

        return float(np.mean(xs)), float(np.mean(ys))

    def _random_position(self) -> Tuple[float, float]:
        """ç”Ÿæˆéšæœºä½ç½®"""
        half = self.LAYOUT_RANGE / 2
        return (
            random.uniform(-half, half),
            random.uniform(-half, half),
        )

    def _is_position_locked(self, concept: KnowledgeNode) -> bool:
        """æ£€æŸ¥ä½ç½®æ˜¯å¦è¢«é”å®šï¼ˆå†·å´ä¸­ï¼‰"""
        if concept.position_x is None or concept.position_y is None:
            return False

        if concept.updated_at is None:
            return False

        cooldown_until = concept.updated_at + timedelta(hours=self.POSITION_COOLDOWN_HOURS)
        return datetime.now(timezone.utc) < cooldown_until


# Singleton
galaxy_layout_service = GalaxyLayoutService()
```

### M5.3 é›†æˆåˆ°æ¦‚å¿µåˆ›å»ºæµç¨‹

åœ¨`AssetConceptLinkService._get_or_create_concept`ä¸­ï¼š

```python
# åˆ›å»ºæ–°èŠ‚ç‚¹åï¼Œè®¡ç®—ä½ç½®
from app.services.galaxy_layout_service import galaxy_layout_service

# ... åˆ›å»ºnodeå
await galaxy_layout_service.compute_position_for_concept(db, node)
```

### M5.4 éªŒæ”¶æ ‡å‡†
- [ ] è¿ç»­3æ¬¡Hardåï¼Œä¸‹æ¬¡é—´éš”æ˜¾è‘—ç¼©çŸ­ï¼ˆ7dâ†’3då·¦å³ï¼‰
- [ ] å¤ä¹ å“åº”åŒ…å«`explanation_code`å­—æ®µ
- [ ] æ–°æ¦‚å¿µæ’å…¥åæœ‰position_x/yï¼ˆénullï¼‰
- [ ] åŒä¸€æ¦‚å¿µ24å°æ—¶å†…é‡å¤è¯·æ±‚ä¸æ”¹å˜ä½ç½®

---

## M6ï¼šé›†æˆéªŒæ”¶ï¼ˆ0.5å¤©ï¼‰

### M6.1 ä¸‰é—­ç¯æ¼”ç¤ºè„šæœ¬

**åœºæ™¯Aï¼šèµ„äº§â†’å›¾è°±**
```bash
# 1. åˆ›å»ºèµ„äº§
curl -X POST /api/v1/assets \
  -d '{"selected_text": "polymorphism", "translation": "å¤šæ€"}'

# 2. éªŒè¯é“¾æ¥ç”Ÿæˆ
curl /api/v1/assets/{asset_id}/links
# åº”è¿”å› asset_concept_links è®°å½•

# 3. éªŒè¯GalaxyèŠ‚ç‚¹
curl /api/v1/galaxy/nodes?name=polymorphism
# åº”è¿”å›èŠ‚ç‚¹ï¼Œæœ‰position_x/y
```

**åœºæ™¯Bï¼šå¤ä¹ â†’ä¸ªæ€§åŒ–**
```bash
# 1. è¿ç»­3æ¬¡æ ‡è®°Hard
for i in 1 2 3; do
  curl -X POST /api/v1/assets/{asset_id}/review -d '{"difficulty": "hard"}'
done

# 2. æ£€æŸ¥é—´éš”
curl /api/v1/assets/{asset_id}
# review_due_at åº”è¯¥æ¯”æ ‡å‡†é—´éš”çŸ­
# å“åº”åŒ…å« explanation_code: "learning_difficulty_adjusted"
```

**åœºæ™¯Cï¼šå¤šç«¯ä¸€è‡´**
```bash
# 1. æ‰‹æœºç¦»çº¿åˆ›å»ºèµ„äº§
# 2. æ¢å¤ç½‘ç»œ
# 3. å¦ä¸€è®¾å¤‡è°ƒç”¨ /sync/events
# 4. éªŒè¯èµ„äº§åŒæ­¥
```

### M6.2 è‡ªåŠ¨åŒ–æµ‹è¯•æ£€æŸ¥æ¸…å•

```python
# tests/test_phase9_integration.py

class TestPhase9Integration:
    async def test_asset_creates_concept_link(self):
        """èµ„äº§åˆ›å»ºåè‡ªåŠ¨ç”Ÿæˆé“¾æ¥"""
        asset = await create_asset("polymorphism")
        links = await get_links_for_asset(asset.id)
        assert len(links) >= 1
        assert links[0].link_type == "provenance"

    async def test_consecutive_hard_reduces_interval(self):
        """è¿ç»­Hardç¼©çŸ­é—´éš”"""
        asset = await create_and_activate_asset("test")

        for _ in range(3):
            await review_asset(asset.id, "hard")

        result = await review_asset(asset.id, "hard")
        assert result["interval_days"] < 7  # æ ‡å‡†goodæ˜¯7å¤©
        assert result["explanation_code"] == "learning_difficulty_adjusted"

    async def test_event_idempotency(self):
        """äº‹ä»¶é‡å¤ä¸é‡å¤å¤„ç†"""
        event_id = "test-event-123"
        event = {"id": event_id, "type": "learning_asset.created", ...}

        await sync_engine.apply_event(event)
        await sync_engine.apply_event(event)  # é‡å¤

        processed = await get_processed_events(event_id)
        assert len(processed) == 1

    async def test_bootstrap_then_incremental(self):
        """Bootstrapåå¢é‡åŒæ­¥"""
        # Bootstrap
        response = await client.get("/sync/bootstrap")
        cursor = response["cursor"]

        # åˆ›å»ºæ–°èµ„äº§
        await create_asset("new_word")

        # å¢é‡åŒæ­¥
        events = await client.get(f"/sync/events?cursor={cursor}")
        assert any(e["type"] == "learning_asset.created" for e in events["events"])
```

### M6.3 æŠ¤æ éªŒè¯

```python
async def test_payload_size_limit():
    """äº‹ä»¶payloadé™é•¿"""
    events = await get_sync_events(cursor, limit=100)
    for event in events:
        assert len(json.dumps(event["payload"])) <= 2048

async def test_galaxy_position_stability():
    """ä½ç½®ç¨³å®šæ€§"""
    node = await create_concept("test")
    pos1 = (node.position_x, node.position_y)

    # åˆ·æ–°
    await refresh_concept_position(node.id)
    node = await get_concept(node.id)
    pos2 = (node.position_x, node.position_y)

    # 24å°æ—¶å†…ä¸åº”å˜åŒ–
    assert pos1 == pos2
```

---

## äº¤ä»˜ç‰©æ¸…å•

1. **Alembic Migrations**
   - `p20_add_asset_concept_links.py`
   - `p21_add_user_id_to_node_relations.py`
   - `p22_add_review_calibration_logs.py`

2. **Python Services**
   - `/backend/app/models/asset_concept_link.py`
   - `/backend/app/models/review_calibration.py`
   - `/backend/app/services/asset_concept_link_service.py`
   - `/backend/app/services/galaxy_layout_service.py`
   - `/backend/app/tasks/co_activation_builder.py`

3. **Go Gateway**
   - `/backend/gateway/internal/handler/sync_handler.go`
   - `/backend/gateway/internal/service/sync_service.go`

4. **Proto**
   - `/proto/sync_service.proto`

5. **Flutter**
   - æ‰©å±• `/mobile/lib/core/offline/local_database.dart`
   - é‡æ„ `/mobile/lib/core/offline/sync_engine.dart`

6. **Tests**
   - `/backend/tests/test_phase9_integration.py`

7. **æ–‡æ¡£**
   - `/docs/PHASE9_BASELINE.md`

---

## ç¡¬æ€§çº¦æŸæé†’

æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¡å¿…éµå®ˆï¼š

- âœ… ä¸»é”®ä¸€å¾‹UUID
- âœ… æ‰€æœ‰è¡¨ç»§æ‰¿BaseModelè½¯åˆ è¯­ä¹‰
- âœ… å¤ç”¨ç°æœ‰event_outbox/event_sequence_counters
- âœ… JSONBé™é•¿ï¼ˆMAX_METADATA_BYTES=2048ï¼‰
- âœ… å…¨æ ˆæ—¶é—´ç»Ÿä¸€datetime.now(timezone.utc)
- âœ… ä¸æäº¤gen/ç›®å½•
- âŒ ä¸å¼•å…¥æ–°äº‹ä»¶ç³»ç»Ÿ
- âŒ ä¸ç›´æ¥è½åº“å…¨æ–‡
