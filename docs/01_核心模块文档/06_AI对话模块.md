# LLM服务模块 (LLM Service) - 功能描述文档

## 模块概述

LLM服务模块是Sparkle应用的AI核心，负责处理与大语言模型的交互，包括对话、工具调用、内容生成等功能。该模块不仅支持常规的AI对话功能，还集成了演示模式、智能推送内容生成等特色功能，为整个应用提供智能化支持。

## 后端实现 (LLMService)

### 核心功能

#### 1. 对话功能
- **chat**: 发送聊天请求到LLM并获取完整响应
  - 支持系统提示词、用户消息、对话历史
  - 包含演示模式支持
  - 可配置温度参数

- **stream_chat**: 流式聊天响应
  - 实时流式返回AI响应
  - 模拟打字效果
  - 支持演示模式的流式输出

#### 2. 工具调用功能
- **chat_with_tools**: 支持工具调用的聊天
  - 接收系统提示词、用户消息、工具定义
  - 返回文本内容和工具调用信息
  - 支持自动工具选择

- **chat_stream_with_tools**: 流式工具调用
  - 流式返回文本块和工具调用
  - 支持工具调用块和完整工具调用
  - 实时处理工具调用过程

#### 3. 演示模式
- **DEMO_MOCK_RESPONSES**: 预设响应系统
  - 为竞赛演示提供100%成功率
  - 包含精确匹配和模糊匹配
  - 支持多种常见场景的预设回复

- **_check_demo_match**: 演示模式匹配检查
  - 检测用户输入是否匹配预设关键词
  - 返回预设响应而非调用真实LLM

#### 4. 内容生成功能
- **generate_push_content**: 生成个性化推送内容
  - 基于用户偏好（教练型、动漫型等）
  - 根据触发类型（记忆、冲刺、不活跃）生成内容
  - 返回JSON格式的标题和正文

### 技术架构

#### 1. LLM提供商抽象
- **LLMProvider**: 抽象基类
- **OpenAICompatibleProvider**: OpenAI兼容实现
- 支持多种LLM提供商的切换

#### 2. 响应数据结构
- **LLMResponse**: 包含内容、工具调用、完成原因
- **StreamChunk**: 流式响应块，支持文本、工具调用等类型

### 演示模式设计

#### 预设响应场景
- "帮我制定高数复习计划": 生成复习计划和任务卡片
- "我今天要学什么": 提供当日学习建议
- "这道题怎么做": 提供解题思路

#### 演示模式机制
- 通过环境变量DEMO_MODE控制
- 精确匹配优先，模糊匹配作为补充
- 模拟真实响应延迟以增加真实感

## 前端实现

### 主要组件
- **ChatScreen**: 聊天界面
  - 提供实时对话界面
  - 展示工具调用结果
  - 支持流式响应显示

### 功能特性
- 实时对话显示
- 工具调用结果展示
- AI响应流式显示

## API 接口

### 主要端点
- `POST /api/v1/chat`: 标准聊天接口
- `POST /api/v1/chat/stream`: 流式聊天接口
- `POST /api/v1/chat/confirm`: 工具调用确认接口

## 数据模型

### 核心数据结构
1. **LLMResponse**
   - content: 响应内容
   - tool_calls: 工具调用列表
   - finish_reason: 完成原因

2. **StreamChunk**
   - type: 块类型（文本、工具调用块、工具调用结束）
   - content: 内容
   - tool_call_id: 工具调用ID
   - tool_name: 工具名称
   - arguments: 参数
   - full_arguments: 完整参数

## 关键业务流程

### AI对话流程
1. 用户发送消息
2. 检查演示模式匹配
3. 构建对话上下文
4. 调用LLM服务
5. 处理工具调用（如需要）
6. 返回响应给前端

### 工具调用流程
1. LLM识别需要调用工具
2. 返回工具调用信息
3. 前端执行相应操作
4. 将结果反馈给LLM
5. LLM生成最终回复

### 推送内容生成流程
1. 推送服务请求内容生成
2. 根据用户偏好和触发类型构建提示词
3. 调用LLM生成推送内容
4. 解析JSON响应
5. 返回标题和正文

## 技术亮点

### 1. 演示模式设计
- 确保竞赛演示的稳定性
- 预设响应保证关键流程成功率
- 模拟真实延迟增加体验感

### 2. 工具调用支持
- 完整的工具调用实现
- 流式工具调用支持
- 工具执行结果反馈机制

### 3. 多提供商支持
- 抽象化LLM提供商接口
- OpenAI兼容API支持
- 易于扩展新的LLM提供商

### 4. 智能推送生成
- 基于用户偏好的个性化内容
- 多种触发类型的适配
- JSON格式的标准化输出

## 模块关系

### 与任务模块的集成
- 任务创建和管理的AI辅助
- 任务执行过程中的智能建议

### 与知识星图模块的集成
- 知识点拓展的AI生成
- 学习内容的智能推荐

### 与推送模块的集成
- 推送内容的AI生成
- 个性化推送文案

### 与计划模块的集成
- 计划制定的AI辅助
- 计划内容的智能生成

## 配置参数

### 环境变量
- `LLM_API_KEY`: LLM API密钥
- `LLM_API_BASE_URL`: LLM API基础URL
- `LLM_MODEL_NAME`: 使用的模型名称
- `LLM_PROVIDER`: LLM提供商
- `DEMO_MODE`: 演示模式开关

### 默认配置
- 默认温度参数：0.7
- 默认模型：配置文件中指定
- 流式输出延迟：模拟打字效果

## 错误处理

### 异常处理机制
- LLM调用异常捕获
- 演示模式降级处理
- JSON解析错误处理

### 降级策略
- 演示模式作为主要降级方案
- 预设响应保证核心功能
- 重试机制（在适用情况下）

## 扩展性考虑

### 1. 模块化设计
- LLM提供商抽象层
- 易于扩展新的AI功能

### 2. 配置化
- 可配置的模型参数
- 可配置的演示响应

### 3. 插件化架构
- 工具调用系统的扩展性
- 新功能的插件化支持
