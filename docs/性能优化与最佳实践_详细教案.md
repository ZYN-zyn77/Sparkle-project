# æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ - æ·±åº¦è®²è§£æ•™æ¡ˆ

## æ•™å­¦ç›®æ ‡
æœ¬æ•™æ¡ˆæ—¨åœ¨æ·±å…¥è®²è§£ç°ä»£é«˜å¹¶å‘AIç³»ç»Ÿçš„æ ¸å¿ƒæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£ï¼š
1. æ•°æ®åº“å¤šç»´ä¼˜åŒ–ç­–ç•¥
2. åˆ†å¸ƒå¼å¹¶å‘æ§åˆ¶æœºåˆ¶
3. å¤šçº§ç¼“å­˜æ¶æ„è®¾è®¡
4. å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ
5. ç”Ÿäº§ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡ç›‘æ§

---

## 9.1 æ•°æ®åº“ä¼˜åŒ–

### 9.1.1 å¤šç»´ç´¢å¼•ç­–ç•¥

#### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦å¤šç»´ç´¢å¼•ï¼Ÿ

ä¼ ç»ŸB-Treeç´¢å¼•åœ¨å¤„ç†ç²¾ç¡®åŒ¹é…å’ŒèŒƒå›´æŸ¥è¯¢æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨AIç³»ç»Ÿä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š
1. **å‘é‡ç›¸ä¼¼åº¦æœç´¢**ï¼šä¼ ç»Ÿç´¢å¼•æ— æ³•é«˜æ•ˆå¤„ç†é«˜ç»´å‘é‡
2. **æ—¶ç©ºæ•°æ®æŸ¥è¯¢**ï¼šæ—¶é—´åºåˆ—æ•°æ®éœ€è¦ç‰¹æ®Šçš„ç´¢å¼•ç»“æ„

#### ğŸ” ä¸‰ç§ç´¢å¼•ç±»å‹è¯¦è§£

**1. B-Tree ç´¢å¼•ï¼ˆä¼ ç»Ÿç´¢å¼•ï¼‰**
```sql
-- é€‚ç”¨äºï¼šç”¨æˆ·IDã€çŠ¶æ€å­—æ®µã€æ—¶é—´èŒƒå›´
CREATE INDEX idx_user_status ON users(status, created_at);

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šO(log n) æ—¶é—´å¤æ‚åº¦
SELECT * FROM users
WHERE status = 'active'
AND created_at > '2025-01-01';
```
- **åŸç†**ï¼šå¹³è¡¡å¤šè·¯æœç´¢æ ‘ï¼Œä¿æŒæ ‘é«˜åº¦å¹³è¡¡
- **ä¼˜åŠ¿**ï¼šç­‰å€¼æŸ¥è¯¢ã€èŒƒå›´æŸ¥è¯¢ã€æ’åºä¼˜åŒ–
- **åœºæ™¯**ï¼šç”¨æˆ·è¡¨ã€ä»»åŠ¡è¡¨ã€é…ç½®è¡¨

**2. HNSW ç´¢å¼•ï¼ˆHierarchical Navigable Small Worldï¼‰**
```sql
-- é€‚ç”¨äºï¼šçŸ¥è¯†èŠ‚ç‚¹å‘é‡ç›¸ä¼¼åº¦æœç´¢
CREATE INDEX idx_node_embedding
ON knowledge_nodes
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 200);

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
SELECT * FROM knowledge_nodes
ORDER BY embedding <=> '[0.1, 0.2, ...]'
LIMIT 10;
```
- **åŸç†**ï¼šå¤šå±‚å›¾ç»“æ„ï¼Œä»ç²—åˆ°ç»†å¯¼èˆª
- **å‚æ•°è¯´æ˜**ï¼š
  - `m`: æ¯å±‚æœ€å¤§è¿æ¥æ•°ï¼ˆ16-32ï¼Œå¹³è¡¡ç²¾åº¦ä¸é€Ÿåº¦ï¼‰
  - `ef_construction`: æ„å»ºæ—¶çš„æœç´¢èŒƒå›´ï¼ˆ100-400ï¼Œè¶Šé«˜ç²¾åº¦è¶Šå¥½ï¼‰
  - `ef_search`: æŸ¥è¯¢æ—¶çš„æœç´¢èŒƒå›´ï¼ˆé»˜è®¤40-100ï¼‰
- **ä¼˜åŠ¿**ï¼šç™¾ä¸‡çº§å‘é‡æ¯«ç§’çº§æ£€ç´¢ï¼Œæ”¯æŒä½™å¼¦/æ¬§æ°è·ç¦»
- **åœºæ™¯**ï¼šçŸ¥è¯†å›¾è°±ã€è¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿ

**3. BRIN ç´¢å¼•ï¼ˆBlock Range Indexï¼‰**
```sql
-- é€‚ç”¨äºï¼šæ—¶é—´åºåˆ—æ•°æ®ï¼ˆå­¦ä¹ è®°å½•ã€æ—¥å¿—ï¼‰
CREATE INDEX idx_study_record_time
ON study_records
USING brin (created_at)
WITH (pages_per_range = 128);

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šå¿«é€Ÿå®šä½æ—¶é—´èŒƒå›´
SELECT * FROM study_records
WHERE created_at BETWEEN '2025-12-01' AND '2025-12-27';
```
- **åŸç†**ï¼šæŒ‰æ•°æ®å—å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ€å°å€¼ã€æœ€å¤§å€¼ï¼‰
- **ä¼˜åŠ¿**ï¼šç´¢å¼•ä½“ç§¯å°ï¼ˆä»…ä¸ºæ•°æ®çš„0.1%ï¼‰ï¼Œé€‚åˆå¤§è¡¨
- **åœºæ™¯**ï¼šæ—¥å¿—è¡¨ã€ç›‘æ§æ•°æ®ã€å†å²è®°å½•

#### ğŸ“Š ç»„åˆç´¢å¼•ç­–ç•¥

```sql
-- å®æˆ˜æ¡ˆä¾‹ï¼šç”¨æˆ·å­¦ä¹ è¿›åº¦æŸ¥è¯¢
-- éœ€æ±‚ï¼šæŸ¥æ‰¾æŸç”¨æˆ·æœ€è¿‘ä¸€å‘¨æŒæ¡çš„çŸ¥è¯†ç‚¹
CREATE INDEX idx_user_progress
ON user_node_status
USING btree (user_id, mastered, last_review_date);

-- æ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN ANALYZE
SELECT node_id, mastery_level
FROM user_node_status
WHERE user_id = 12345
  AND mastered = true
  AND last_review_date >= CURRENT_DATE - INTERVAL '7 days';
-- ç»“æœï¼šIndex Scan using idx_user_progress (cost=0.42..8.45)
```

### 9.1.2 æŸ¥è¯¢ä¼˜åŒ–ï¼šé¿å…N+1é—®é¢˜

#### âŒ N+1é—®é¢˜ç¤ºä¾‹

```python
# é”™è¯¯ç¤ºèŒƒï¼šæ¯å¾ªç¯ä¸€æ¬¡æŸ¥è¯¢ä¸€æ¬¡æ•°æ®åº“
def get_user_knowledge_graph(user_id):
    user = db.query(User).get(user_id)  # 1æ¬¡æŸ¥è¯¢
    nodes = []
    for node in user.nodes:  # å‡è®¾æœ‰100ä¸ªèŠ‚ç‚¹
        # Næ¬¡æŸ¥è¯¢ï¼
        relations = db.query(Relation).filter_by(node_id=node.id).all()
        nodes.append({
            'node': node,
            'relations': relations
        })
    return nodes
# æ€»è®¡ï¼š1 + 100 = 101æ¬¡æŸ¥è¯¢ï¼
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šJOINåˆå¹¶ï¼ˆæ¨èï¼‰**
```python
# ä½¿ç”¨SQLAlchemyçš„joinedload
from sqlalchemy.orm import joinedload

def get_user_knowledge_graph_optimized(user_id):
    user = db.query(User).options(
        joinedload(User.nodes).joinedload(Node.relations)
    ).get(user_id)

    # æ•°æ®å·²åœ¨å•æ¬¡æŸ¥è¯¢ä¸­åŠ è½½
    return [{
        'node': node,
        'relations': node.relations
    } for node in user.nodes]
# æ€»è®¡ï¼š1æ¬¡æŸ¥è¯¢
```

**æ–¹æ¡ˆ2ï¼šæ‰¹é‡æŸ¥è¯¢**
```python
# å…ˆæ”¶é›†æ‰€æœ‰IDï¼Œå†æ‰¹é‡æŸ¥è¯¢
def get_user_knowledge_graph_batch(user_id):
    user = db.query(User).get(user_id)
    node_ids = [node.id for node in user.nodes]

    # 1æ¬¡æ‰¹é‡æŸ¥è¯¢
    relations = db.query(Relation).filter(
        Relation.node_id.in_(node_ids)
    ).all()

    # å†…å­˜ä¸­å…³è”
    relations_by_node = {}
    for rel in relations:
        relations_by_node.setdefault(rel.node_id, []).append(rel)

    return [{
        'node': node,
        'relations': relations_by_node.get(node.id, [])
    } for node in user.nodes]
# æ€»è®¡ï¼š2æ¬¡æŸ¥è¯¢
```

**æ–¹æ¡ˆ3ï¼šä½¿ç”¨GraphQLé£æ ¼çš„å­—æ®µé€‰æ‹©å™¨**
```python
# æŒ‰éœ€åŠ è½½å­—æ®µ
def get_user_data(user_id, fields=None):
    query = db.query(User)

    if 'nodes' in fields:
        query = query.options(joinedload(User.nodes))
    if 'profile' in fields:
        query = query.options(joinedload(User.profile))

    return query.get(user_id)
```

### 9.1.3 åˆ†åŒºç­–ç•¥

#### ğŸ“… æ—¶é—´åˆ†ç‰‡å®æˆ˜

```sql
-- åˆ›å»ºåˆ†åŒºè¡¨ï¼ˆPostgreSQL 10+ï¼‰
CREATE TABLE study_records (
    id BIGSERIAL,
    user_id BIGINT,
    node_id BIGINT,
    study_time TIMESTAMP,
    duration INTERVAL,
    PRIMARY KEY (id, study_time)
) PARTITION BY RANGE (study_time);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE study_records_2025_12
PARTITION OF study_records
FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

CREATE TABLE study_records_2025_11
PARTITION OF study_records
FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');

-- è‡ªåŠ¨åˆ†åŒºåˆ›å»ºå‡½æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition()
RETURNS void AS $$
DECLARE
    partition_date DATE := DATE_TRUNC('month', CURRENT_DATE + INTERVAL '1 month');
    partition_name TEXT := 'study_records_' || TO_CHAR(partition_date, 'YYYY_MM');
BEGIN
    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF study_records
         FOR VALUES FROM (%L) TO (%L)',
        partition_name,
        partition_date,
        partition_date + INTERVAL '1 month'
    );
END;
$$ LANGUAGE plpgsql;

-- æŸ¥è¯¢ä¼˜åŒ–ï¼šè‡ªåŠ¨è·¯ç”±åˆ°åˆ†åŒº
EXPLAIN SELECT * FROM study_records
WHERE study_time >= '2025-12-01' AND study_time < '2025-12-28';
-- ç»“æœï¼šåªæ‰«æ study_records_2025_12 åˆ†åŒº
```

#### ğŸ¯ åˆ†åŒºç­–ç•¥é€‰æ‹©

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|
| **æ—¶é—´åˆ†åŒº** | æ—¥å¿—ã€å†å²è®°å½• | æ˜“å½’æ¡£ã€æŸ¥è¯¢å¿« | è·¨åˆ†åŒºæŸ¥è¯¢æ…¢ |
| **ç”¨æˆ·åˆ†åŒº** | å¤šç§Ÿæˆ·ç³»ç»Ÿ | æ•°æ®éš”ç¦»ã€å¹¶è¡ŒæŸ¥è¯¢ | åˆ†ç‰‡é”®é€‰æ‹©å¤æ‚ |
| **å“ˆå¸Œåˆ†åŒº** | è´Ÿè½½å‡è¡¡ | æ•°æ®å‡åŒ€åˆ†å¸ƒ | èŒƒå›´æŸ¥è¯¢é€€åŒ– |

### 9.1.4 è¿æ¥æ± ä¼˜åŒ–

#### ğŸ”— è¿æ¥æ± é…ç½®è¯¦è§£

```python
# SQLAlchemy è¿æ¥æ± é…ç½®
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "postgresql://user:pass@localhost/db",

    # è¿æ¥æ± é…ç½®
    poolclass=QueuePool,
    pool_size=20,           # å¸¸é©»è¿æ¥æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
    max_overflow=10,        # è¶…å‡ºåä¸´æ—¶åˆ›å»ºçš„è¿æ¥æ•°
    pool_timeout=30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pool_recycle=3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆé˜²æ­¢æ•°æ®åº“è¶…æ—¶æ–­å¼€ï¼‰
    pool_pre_ping=True,     # è¿æ¥å¥åº·æ£€æŸ¥ï¼ˆé˜²æ­¢è¿æ¥æ± æ±¡æŸ“ï¼‰

    # æ€§èƒ½ä¼˜åŒ–
    echo=False,             # å…³é—­SQLæ—¥å¿—ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
    connect_args={
        'connect_timeout': 10,
        'options': '-c statement_timeout=30000'  # 30ç§’æŸ¥è¯¢è¶…æ—¶
    }
)

# ç›‘æ§è¿æ¥æ± çŠ¶æ€
from sqlalchemy import event

@event.listens_for(engine, 'checkout')
def on_checkout(dbapi_con, con_record, con_proxy):
    con_record.info['checkout_time'] = time.time()

@event.listens_for(engine, 'checkin')
def on_checkin(dbapi_con, con_record):
    if 'checkout_time' in con_record.info:
        duration = time.time() - con_record.info['checkout_time']
        if duration > 5:  # è­¦å‘Šé•¿äº‹åŠ¡
            logger.warning(f"Long connection held for {duration:.2f}s")
```

#### ğŸ“Š è¿æ¥æ± ç›‘æ§æŒ‡æ ‡

```python
# Prometheus ç›‘æ§
from prometheus_client import Gauge, Counter

db_pool_size = Gauge('db_pool_size', 'Total pool size')
db_pool_checked_out = Gauge('db_pool_checked_out', 'Checked out connections')
db_pool_overflow = Gauge('db_pool_overflow', 'Overflow connections')
db_pool_timeout = Counter('db_pool_timeout_total', 'Connection timeout count')

def monitor_pool():
    pool = engine.pool

    db_pool_size.set(pool.size())
    db_pool_checked_out.set(pool.checkedout())
    db_pool_overflow.set(pool.overflow())

    # è­¦å‘Šé˜ˆå€¼
    if pool.checkedout() / pool.size() > 0.8:
        logger.warning("Connection pool 80% utilized")
```

---

## 9.2 å¹¶å‘æ§åˆ¶

### 9.2.1 åˆ†å¸ƒå¼ä¿¡å·é‡

#### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼ä¿¡å·é‡ï¼Ÿ

åœ¨å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œå¤šä¸ªå®ä¾‹éœ€è¦å…±äº«å¹¶å‘é™åˆ¶ï¼š
```python
# å•æœºä¿¡å·é‡ï¼ˆå¤±æ•ˆï¼‰
semaphore = asyncio.Semaphore(10)  # æ¯ä¸ªå®ä¾‹10ä¸ªï¼Œ10ä¸ªå®ä¾‹=100ä¸ªå¹¶å‘

# åˆ†å¸ƒå¼ä¿¡å·é‡ï¼ˆæ­£ç¡®ï¼‰
distributed_sem = RedisSemaphore(redis_client, "api_limit", 10)  # å…¨å±€é™åˆ¶10
```

#### ğŸ”§ Redisåˆ†å¸ƒå¼ä¿¡å·é‡å®ç°

```python
import redis
import asyncio
import uuid
import time

class RedisSemaphore:
    """
    åŸºäºRedisçš„åˆ†å¸ƒå¼ä¿¡å·é‡ï¼ˆæ”¯æŒè‡ªåŠ¨ç»­æœŸï¼‰
    """
    def __init__(self, redis_client, key, max_permits, timeout=30):
        self.redis = redis_client
        self.key = f"semaphore:{key}"
        self.max_permits = max_permits
        self.timeout = timeout
        self.lease_id = None

    async def acquire(self):
        """è·å–ä¿¡å·é‡"""
        lease_id = str(uuid.uuid4())
        start_time = time.time()

        while True:
            # ä½¿ç”¨Luaè„šæœ¬ä¿è¯åŸå­æ€§
            script = """
            local key = KEYS[1]
            local max_permits = tonumber(ARGV[1])
            local timeout = tonumber(ARGV[2])
            local lease_id = ARGV[3]

            local current = redis.call('GET', key)
            if not current then
                current = 0
            else
                current = tonumber(current)
            end

            if current < max_permits then
                redis.call('INCR', key)
                redis.call('SETEX', key .. ':lease:' .. lease_id, timeout, 1)
                return 1
            else
                return 0
            end
            """

            acquired = self.redis.eval(
                script, 1, self.key,
                self.max_permits, self.timeout, lease_id
            )

            if acquired:
                self.lease_id = lease_id
                # å¯åŠ¨ç»­æœŸä»»åŠ¡
                asyncio.create_task(self._renew_lease())
                return True

            # æœªè·å–åˆ°ï¼Œç­‰å¾…åé‡è¯•
            if time.time() - start_time > 60:  # è¶…æ—¶60ç§’
                raise TimeoutError("Failed to acquire semaphore")

            await asyncio.sleep(0.1)

    async def _renew_lease(self):
        """è‡ªåŠ¨ç»­æœŸ"""
        while True:
            try:
                await asyncio.sleep(self.timeout // 2)  # åŠæ•°æ—¶é—´ç»­æœŸ
                if self.lease_id:
                    key = f"{self.key}:lease:{self.lease_id}"
                    self.redis.expire(key, self.timeout)
            except:
                break

    async def release(self):
        """é‡Šæ”¾ä¿¡å·é‡"""
        if not self.lease_id:
            return

        script = """
        local key = KEYS[1]
        local lease_id = ARGV[1]

        redis.call('DECR', key)
        redis.call('DEL', key .. ':lease:' .. lease_id)
        return 1
        """

        self.redis.eval(script, 1, self.key, self.lease_id)
        self.lease_id = None

# ä½¿ç”¨ç¤ºä¾‹
async def process_request(user_id, request_data):
    semaphore = RedisSemaphore(redis_client, "ai_processing", 5)

    try:
        await semaphore.acquire()
        # é™æµä¿æŠ¤çš„ä»£ç 
        result = await ai_model.process(request_data)
        return result
    finally:
        await semaphore.release()
```

### 9.2.2 æµé‡æ§åˆ¶ï¼ˆRate Limitingï¼‰

#### ğŸ¯ å¤šç»´åº¦é™æµç­–ç•¥

```python
from collections import defaultdict
import time

class MultiDimensionRateLimiter:
    """
    æ”¯æŒå¤šç»´åº¦çš„åˆ†å¸ƒå¼é™æµå™¨
    """
    def __init__(self, redis_client):
        self.redis = redis_client

    async def check_limit(self, user_id, action, limits):
        """
        limits: {
            'per_user': (100, 3600),      # æ¯ç”¨æˆ·100æ¬¡/å°æ—¶
            'per_action': (1000, 3600),   # æ¯åŠ¨ä½œ1000æ¬¡/å°æ—¶
            'global': (5000, 3600)        # å…¨å±€5000æ¬¡/å°æ—¶
        }
        """
        timestamp = int(time.time())
        pipeline = self.redis.pipeline()

        for limit_type, (max_requests, window) in limits.items():
            if limit_type == 'per_user':
                key = f"rate_limit:user:{user_id}:{action}"
            elif limit_type == 'per_action':
                key = f"rate_limit:action:{action}"
            elif limit_type == 'global':
                key = f"rate_limit:global:{action}"

            # æ»‘åŠ¨çª—å£è®¡æ•°
            window_start = timestamp - window
            pipeline.zremrangebyscore(key, 0, window_start)
            pipeline.zcard(key)
            pipeline.zadd(key, {str(timestamp): timestamp})
            pipeline.expire(key, window)

        results = pipeline.execute()

        # æ£€æŸ¥æ‰€æœ‰é™åˆ¶
        for i, (limit_type, (max_requests, _)) in enumerate(limits.items()):
            count = results[i * 2 + 1]  # zcardçš„ç»“æœ
            if count > max_requests:
                return False, limit_type

        return True, None

# ä½¿ç”¨ç¤ºä¾‹
async def handle_api_request(user_id, action):
    limiter = MultiDimensionRateLimiter(redis_client)

    allowed, limit_type = await limiter.check_limit(
        user_id, action,
        limits={
            'per_user': (100, 3600),
            'per_action': (1000, 3600),
            'global': (5000, 3600)
        }
    )

    if not allowed:
        raise RateLimitExceeded(f"Hit {limit_type} limit")

    return await process_request(action)
```

#### ğŸ¯ ä»¤ç‰Œæ¡¶ç®—æ³•ï¼ˆåº”å¯¹çªå‘æµé‡ï¼‰

```python
class TokenBucket:
    """ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°"""
    def __init__(self, redis_client, key, capacity, refill_rate):
        """
        capacity: æ¡¶å®¹é‡ï¼ˆæœ€å¤§çªå‘é‡ï¼‰
        refill_rate: æ¯ç§’è¡¥å……ä»¤ç‰Œæ•°
        """
        self.redis = redis_client
        self.key = f"token_bucket:{key}"
        self.capacity = capacity
        self.refill_rate = refill_rate

    async def consume(self, tokens=1):
        script = """
        local key = KEYS[1]
        local capacity = tonumber(ARGV[1])
        local refill_rate = tonumber(ARGV[2])
        local tokens_needed = tonumber(ARGV[3])
        local now = tonumber(ARGV[4])

        local data = redis.call('HMGET', key, 'tokens', 'last_refill')
        local current_tokens = tonumber(data[1]) or capacity
        local last_refill = tonumber(data[2]) or now

        -- è¡¥å……ä»¤ç‰Œ
        local time_passed = now - last_refill
        local tokens_to_add = time_passed * refill_rate
        current_tokens = math.min(capacity, current_tokens + tokens_to_add)

        -- æ¶ˆè´¹ä»¤ç‰Œ
        if current_tokens >= tokens_needed then
            current_tokens = current_tokens - tokens_needed
            redis.call('HMSET', key, 'tokens', current_tokens, 'last_refill', now)
            return 1
        else
            return 0
        end
        """

        allowed = self.redis.eval(
            script, 1, self.key,
            self.capacity, self.refill_rate, tokens, time.time()
        )

        return bool(allowed)
```

### 9.2.3 èµ„æºéš”ç¦»ï¼ˆå¤šç§Ÿæˆ·ï¼‰

#### ğŸ”’ ä¸‰ç§éš”ç¦»çº§åˆ«

**1. çº¿ç¨‹æ± éš”ç¦»**
```python
from concurrent.futures import ThreadPoolExecutor
import asyncio

class TenantExecutor:
    """æŒ‰ç§Ÿæˆ·éš”ç¦»çš„çº¿ç¨‹æ± """
    def __init__(self):
        self.executors = {}  # tenant_id -> executor

    def get_executor(self, tenant_id):
        if tenant_id not in self.executors:
            # æ¯ä¸ªç§Ÿæˆ·æœ€å¤š4ä¸ªçº¿ç¨‹
            self.executors[tenant_id] = ThreadPoolExecutor(
                max_workers=4,
                thread_name_prefix=f"tenant_{tenant_id}"
            )
        return self.executors[tenant_id]

    async def submit(self, tenant_id, func, *args):
        loop = asyncio.get_event_loop()
        executor = self.get_executor(tenant_id)
        return await loop.run_in_executor(executor, func, *args)

# ä½¿ç”¨
executor = TenantExecutor()
result = await executor.submit("user_123", heavy_computation, data)
```

**2. é€Ÿç‡é™åˆ¶éš”ç¦»**
```python
class TenantRateLimiter:
    """æ¯ä¸ªç§Ÿæˆ·ç‹¬ç«‹é™æµ"""
    def __init__(self, tenant_configs):
        """
        tenant_configs: {
            'tenant_a': {'qps': 100, 'burst': 200},
            'tenant_b': {'qps': 50, 'burst': 100}
        }
        """
        self.limiters = {}
        for tenant_id, config in tenant_configs.items():
            self.limiters[tenant_id] = TokenBucket(
                redis_client,
                tenant_id,
                config['burst'],
                config['qps']
            )

    async def check_tenant(self, tenant_id):
        if tenant_id not in self.limiters:
            return False
        return await self.limiters[tenant_id].consume()
```

**3. æ•°æ®åº“èµ„æºç»„ï¼ˆResource Groupsï¼‰**
```sql
-- PostgreSQL èµ„æºç»„ï¼ˆv14+ï¼‰
CREATE RESOURCE GROUP tenant_a WITH (
    CPU_RATE_LIMIT = 50,      -- 50% CPU
    MEMORY_LIMIT = 8,         -- 8GBå†…å­˜
    CONCURRENCY = 10          -- 10ä¸ªå¹¶å‘
);

-- å°†ç”¨æˆ·ç»‘å®šåˆ°èµ„æºç»„
ALTER ROLE user_a RESOURCE GROUP tenant_a;

-- æŸ¥è¯¢èµ„æºä½¿ç”¨
SELECT * FROM pg_stat_resource_groups;
```

---

## 9.3 ç¼“å­˜ç­–ç•¥

### 9.3.1 å¤šçº§ç¼“å­˜æ¶æ„

#### ğŸ—ï¸ L1 + L2 ç¼“å­˜è®¾è®¡

```python
import asyncio
import time
from typing import Any, Optional
import pickle

class MultiLevelCache:
    """
    L1: æœ¬åœ°å†…å­˜ï¼ˆè¿›ç¨‹å†…ï¼Œå¾®ç§’çº§ï¼‰
    L2: Redisï¼ˆåˆ†å¸ƒå¼ï¼Œæ¯«ç§’çº§ï¼‰
    """
    def __init__(self, redis_client, l1_ttl=60, l2_ttl=300):
        self.redis = redis_client
        self.l1_cache = {}  # {key: (value, expire_time)}
        self.l1_ttl = l1_ttl
        self.l2_ttl = l2_ttl
        self.locks = {}  # é˜²æ­¢ç¼“å­˜å‡»ç©¿

    async def get(self, key: str) -> Optional[Any]:
        # L1 æŸ¥è¯¢
        now = time.time()
        if key in self.l1_cache:
            value, expire = self.l1_cache[key]
            if expire > now:
                return value
            else:
                del self.l1_cache[key]

        # L2 æŸ¥è¯¢
        value = await self.redis.get(f"l2:{key}")
        if value:
            # å›å¡« L1
            self.l1_cache[key] = (pickle.loads(value), now + self.l1_ttl)
            return pickle.loads(value)

        return None

    async def set(self, key: str, value: Any):
        # L1 è®¾ç½®
        self.l1_cache[key] = (value, time.time() + self.l1_ttl)

        # L2 è®¾ç½®
        await self.redis.setex(
            f"l2:{key}",
            self.l2_ttl,
            pickle.dumps(value)
        )

    async def get_or_set(self, key: str, func, *args, **kwargs):
        """ç¼“å­˜ç©¿é€ä¿æŠ¤"""
        # å¿«é€Ÿè·¯å¾„
        cached = await self.get(key)
        if cached is not None:
            return cached

        # ç¼“å­˜å‡»ç©¿ä¿æŠ¤ï¼ˆåˆ†å¸ƒå¼é”ï¼‰
        lock_key = f"lock:{key}"
        lock_acquired = await self.redis.set(
            lock_key, "1", nx=True, ex=10
        )

        if lock_acquired:
            try:
                # åŒé‡æ£€æŸ¥
                cached = await self.get(key)
                if cached is not None:
                    return cached

                # è®¡ç®—å¹¶ç¼“å­˜
                value = await func(*args, **kwargs)
                await self.set(key, value)
                return value
            finally:
                await self.redis.delete(lock_key)
        else:
            # ç­‰å¾…å…¶ä»–è¿›ç¨‹ç¼“å­˜
            await asyncio.sleep(0.1)
            return await self.get(key)

# ä½¿ç”¨ç¤ºä¾‹
cache = MultiLevelCache(redis_client)

async def get_user_profile(user_id):
    return await cache.get_or_set(
        f"user_profile:{user_id}",
        fetch_user_from_db,
        user_id
    )
```

### 9.3.2 è¯­ä¹‰ç¼“å­˜ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰

#### ğŸ§  åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ™ºèƒ½ç¼“å­˜

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SemanticCache:
    """
    ç¼“å­˜è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜ï¼Œå‡å°‘é‡å¤LLMè°ƒç”¨
    """
    def __init__(self, redis_client, embedding_model, threshold=0.95):
        self.redis = redis_client
        self.model = embedding_model
        self.threshold = threshold  # ç›¸ä¼¼åº¦é˜ˆå€¼

    async def get(self, query: str) -> Optional[str]:
        """æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„ç¼“å­˜"""
        query_vec = self.model.encode(query).tolist()

        # è·å–æ‰€æœ‰ç¼“å­˜çš„å‘é‡
        cached = await self.redis.hgetall("semantic_cache:vectors")
        if not cached:
            return None

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for cached_query, cached_vec_str in cached.items():
            cached_vec = np.array(eval(cached_vec_str))
            similarity = cosine_similarity(
                [query_vec], [cached_vec]
            )[0][0]

            if similarity >= self.threshold:
                similarities.append((similarity, cached_query))

        if similarities:
            # è¿”å›æœ€ç›¸ä¼¼çš„ç­”æ¡ˆ
            similarities.sort(reverse=True)
            best_match = similarities[0][1]
            answer = await self.redis.hget("semantic_cache:answers", best_match)
            return answer

        return None

    async def set(self, query: str, answer: str):
        """ç¼“å­˜é—®é¢˜å’Œç­”æ¡ˆ"""
        query_vec = self.model.encode(query).tolist()

        pipeline = self.redis.pipeline()
        pipeline.hset("semantic_cache:vectors", query, str(query_vec))
        pipeline.hset("semantic_cache:answers", query, answer)
        pipeline.expire("semantic_cache:vectors", 86400)  # 24å°æ—¶
        pipeline.expire("semantic_cache:answers", 86400)
        await pipeline.execute()

# ä½¿ç”¨ç¤ºä¾‹
semantic_cache = SemanticCache(redis_client, embedding_model)

async def ai_chat(query):
    # å…ˆæŸ¥è¯­ä¹‰ç¼“å­˜
    cached = await semantic_cache.get(query)
    if cached:
        return cached

    # è°ƒç”¨LLM
    answer = await llm.generate(query)

    # å¼‚æ­¥ç¼“å­˜ï¼ˆä¸é˜»å¡è¿”å›ï¼‰
    asyncio.create_task(semantic_cache.set(query, answer))

    return answer
```

### 9.3.3 ç¼“å­˜å‡»ç©¿é˜²æŠ¤

#### ğŸ”¨ äº’æ–¥é”é‡å»ºç­–ç•¥

```python
class CacheBreaker:
    """
    é˜²æ­¢ç¼“å­˜å‡»ç©¿ï¼ˆçƒ­ç‚¹keyè¿‡æœŸç¬é—´å¤§é‡è¯·æ±‚ï¼‰
    """
    def __init__(self, redis_client):
        self.redis = redis_client

    async def get_with_mutex(self, key: str, loader, ttl=300):
        """
        ä½¿ç”¨äº’æ–¥é”ä¿æŠ¤çƒ­ç‚¹keyé‡å»º
        """
        # å°è¯•è·å–ç¼“å­˜
        value = await self.redis.get(key)
        if value:
            return pickle.loads(value)

        # è·å–é‡å»ºé”
        lock_key = f"lock:{key}"
        lock_acquired = await self.redis.set(
            lock_key, "1", nx=True, ex=10
        )

        if lock_acquired:
            try:
                # åŒé‡æ£€æŸ¥
                value = await self.redis.get(key)
                if value:
                    return pickle.loads(value)

                # é‡å»ºç¼“å­˜
                value = await loader()

                # è®¾ç½®ç¼“å­˜ + éšæœºTTLï¼ˆé˜²é›ªå´©ï¼‰
                random_ttl = ttl + np.random.randint(-30, 30)
                await self.redis.setex(
                    key, random_ttl, pickle.dumps(value)
                )

                return value
            finally:
                await self.redis.delete(lock_key)
        else:
            # ç­‰å¾…å¹¶é‡è¯•
            await asyncio.sleep(0.05)
            return await self.get_with_mutex(key, loader, ttl)

# ä½¿ç”¨ç¤ºä¾‹
breaker = CacheBreaker(redis_client)

async def get_hot_data(item_id):
    return await breaker.get_with_mutex(
        f"hot_item:{item_id}",
        lambda: fetch_item_from_db(item_id),
        ttl=600
    )
```

### 9.3.4 ç¼“å­˜é›ªå´©é˜²æŠ¤

#### â„ï¸ éšæœºè¿‡æœŸæ—¶é—´ + ç†”æ–­

```python
class AvalancheProtector:
    """
    é˜²æ­¢ç¼“å­˜é›ªå´©ï¼ˆå¤§é‡keyåŒæ—¶è¿‡æœŸï¼‰
    """
    def __init__(self, redis_client):
        self.redis = redis_client

    async def set_with_jitter(self, key: str, value: Any, base_ttl: int):
        """
        æ·»åŠ éšæœºæŠ–åŠ¨ï¼Œåˆ†æ•£è¿‡æœŸæ—¶é—´
        """
        # 10-20% çš„éšæœºæŠ–åŠ¨
        jitter = np.random.uniform(0.1, 0.2) * base_ttl
        ttl = int(base_ttl + jitter)

        await self.redis.setex(key, ttl, pickle.dumps(value))

        # è®°å½•ç›‘æ§
        await self.redis.zadd(
            "cache:ttl_distribution",
            {key: ttl}
        )

    async def get_batch_with_fallback(self, keys: List[str], loader):
        """
        æ‰¹é‡è·å–ï¼Œæ”¯æŒé™çº§
        """
        values = await self.redis.mget(keys)
        missing_keys = [k for i, v in enumerate(values) if v is None]

        if missing_keys:
            # æ‰¹é‡åŠ è½½
            loaded = await loader(missing_keys)

            # å¹¶è¡Œè®¾ç½®ï¼ˆä¸ç­‰å¾…ï¼‰
            for key, value in loaded.items():
                asyncio.create_task(
                    self.set_with_jitter(key, value, 300)
                )

            # åˆå¹¶ç»“æœ
            result = {}
            for i, key in enumerate(keys):
                if values[i]:
                    result[key] = pickle.loads(values[i])
                elif key in loaded:
                    result[key] = loaded[key]

            return result

        return {k: pickle.loads(v) for k, v in zip(keys, values) if v}

# ä½¿ç”¨ç¤ºä¾‹
protector = AvalancheProtector(redis_client)

async def batch_get_nodes(node_ids):
    keys = [f"node:{nid}" for nid in node_ids]

    return await protector.get_batch_with_fallback(
        keys,
        lambda missing: fetch_nodes_from_db(
            [k.split(":")[1] for k in missing]
        )
    )
```

---

## 9.4 å¼‚æ­¥ä¼˜åŒ–

### 9.4.1 å¹¶å‘æ‰§è¡Œï¼ˆasyncio.gatherï¼‰

#### ğŸš€ é«˜æ•ˆå¹¶å‘æ¨¡å¼

```python
import asyncio
from typing import List, Coroutine

class AsyncBatchProcessor:
    """
    æ‰¹é‡å¼‚æ­¥å¤„ç†å™¨ï¼Œæ”¯æŒé™æµå’Œé”™è¯¯å¤„ç†
    """
    def __init__(self, max_concurrent=10):
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def process_with_gather(self, tasks: List[Coroutine],
                                  return_exceptions=False):
        """
        ä½¿ç”¨gatherå¹¶å‘æ‰§è¡Œï¼Œæ”¯æŒé”™è¯¯éš”ç¦»
        """
        # æ·»åŠ é™æµåŒ…è£…
        async def limited_task(coro):
            async with self.semaphore:
                return await coro

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(
            *[limited_task(t) for t in tasks],
            return_exceptions=return_exceptions
        )

        return results

    async def process_with_progress(self, tasks: List[Coroutine]):
        """
        å¸¦è¿›åº¦æ˜¾ç¤ºçš„å¹¶å‘å¤„ç†
        """
        total = len(tasks)
        completed = 0

        async def track_progress(coro):
            nonlocal completed
            try:
                result = await coro
                completed += 1
                print(f"Progress: {completed}/{total}")
                return result
            except Exception as e:
                print(f"Failed: {e}")
                raise

        return await asyncio.gather(
            *[track_progress(t) for t in tasks],
            return_exceptions=True
        )

# ä½¿ç”¨ç¤ºä¾‹
processor = AsyncBatchProcessor(max_concurrent=5)

async def batch_process_nodes(node_ids):
    tasks = [
        process_single_node(node_id)
        for node_id in node_ids
    ]

    results = await processor.process_with_gather(tasks)

    # è¿‡æ»¤é”™è¯¯
    successful = [r for r in results if not isinstance(r, Exception)]
    return successful
```

#### âš¡ å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆRAIIï¼‰

```python
class AsyncTransaction:
    """
    è‡ªåŠ¨äº‹åŠ¡ç®¡ç†ï¼ˆRAIIæ¨¡å¼ï¼‰
    """
    def __init__(self, db_session):
        self.session = db_session
        self.committed = False

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """
        è‡ªåŠ¨æäº¤æˆ–å›æ»š
        """
        if exc_type is None and not self.committed:
            try:
                await self.session.commit()
                self.committed = True
            except:
                await self.session.rollback()
                raise
        else:
            # å‘ç”Ÿå¼‚å¸¸ï¼Œå›æ»š
            await self.session.rollback()

    async def commit(self):
        """æ˜¾å¼æäº¤"""
        if not self.committed:
            await self.session.commit()
            self.committed = True

# ä½¿ç”¨ç¤ºä¾‹
async def update_user_progress(user_id, node_id, score):
    async with AsyncTransaction(db.session):
        user = await db.query(User).get(user_id)
        user.total_score += score

        record = StudyRecord(
            user_id=user_id,
            node_id=node_id,
            score=score
        )
        db.add(record)

        # è‡ªåŠ¨æäº¤ï¼Œå¼‚å¸¸è‡ªåŠ¨å›æ»š
```

### 9.4.2 é™æµå¹¶å‘ï¼ˆSemaphoreï¼‰

#### ğŸ¯ é«˜çº§é™æµæ¨¡å¼

```python
class AdaptiveSemaphore:
    """
    è‡ªé€‚åº”ä¿¡å·é‡ï¼Œæ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´
    """
    def __init__(self, redis_client, key, base_permits):
        self.redis = redis_client
        self.key = key
        self.base_permits = base_permits
        self.local_permits = base_permits

    async def acquire(self):
        # åŠ¨æ€è°ƒæ•´
        cpu_load = await self.get_cpu_load()
        if cpu_load > 80:
            self.local_permits = max(1, self.base_permits // 2)
        elif cpu_load < 30:
            self.local_permits = min(self.base_permits * 2, self.base_permits + 10)

        semaphore = RedisSemaphore(self.redis, self.key, self.local_permits)
        return await semaphore.acquire()

    async def get_cpu_load(self):
        # ä»ç›‘æ§ç³»ç»Ÿè·å–CPUè´Ÿè½½
        load = await self.redis.get("system:cpu:load")
        return float(load) if load else 50

# ä½¿ç”¨ç¤ºä¾‹
async def process_ai_request(request):
    sem = AdaptiveSemaphore(redis_client, "ai_processing", 10)
    async with sem:
        return await llm.generate(request)
```

### 9.4.3 èµ„æºæ¸…ç†ï¼ˆfinallyå—ï¼‰

#### ğŸ§¹ å®‰å…¨çš„èµ„æºç®¡ç†

```python
class ResourceManager:
    """
    ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾çš„ç®¡ç†å™¨
    """
    def __init__(self):
        self.resources = []

    def add(self, resource, cleanup_func):
        """æ³¨å†Œèµ„æºå’Œæ¸…ç†å‡½æ•°"""
        self.resources.append((resource, cleanup_func))

    async def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        errors = []
        for resource, cleanup in reversed(self.resources):
            try:
                if asyncio.iscoroutinefunction(cleanup):
                    await cleanup(resource)
                else:
                    cleanup(resource)
            except Exception as e:
                errors.append(e)

        if errors:
            raise Exception(f"Cleanup errors: {errors}")

# ä½¿ç”¨ç¤ºä¾‹
async def complex_operation():
    manager = ResourceManager()

    try:
        # è·å–èµ„æº
        file = open("temp.txt", "w")
        manager.add(file, lambda f: f.close())

        conn = await acquire_db_connection()
        manager.add(conn, lambda c: c.close())

        cache_lock = await acquire_lock("key")
        manager.add(cache_lock, lambda l: l.release())

        # æ‰§è¡Œæ“ä½œ
        result = await perform_task(file, conn, cache_lock)
        return result

    finally:
        # ç¡®ä¿æ¸…ç†
        await manager.cleanup()

# æ›´ç®€æ´çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ–¹å¼
from contextlib import asynccontextmanager

@asynccontextmanager
async def managed_connection():
    conn = None
    try:
        conn = await acquire_db_connection()
        yield conn
    finally:
        if conn:
            await conn.close()

# ä½¿ç”¨
async def query_data():
    async with managed_connection() as conn:
        return await conn.execute("SELECT * FROM users")
```

### 9.4.4 å¼‚æ­¥ä»»åŠ¡è°ƒåº¦

#### ğŸ“… é«˜çº§ä»»åŠ¡ç®¡ç†

```python
import asyncio
from dataclasses import dataclass
from enum import Enum

class TaskPriority(Enum):
    HIGH = 1
    MEDIUM = 2
    LOW = 3

@dataclass
class Task:
    coro: asyncio.Coroutine
    priority: TaskPriority
    timeout: int = 30

class TaskScheduler:
    """
    ä¼˜å…ˆçº§ä»»åŠ¡è°ƒåº¦å™¨
    """
    def __init__(self, max_concurrent=20):
        self.queues = {
            priority: asyncio.PriorityQueue()
            for priority in TaskPriority
        }
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.workers = []

    async def submit(self, coro, priority=TaskPriority.MEDIUM, timeout=30):
        task = Task(coro, priority, timeout)
        await self.queues[priority].put(task)

    async def start_workers(self, count=3):
        """å¯åŠ¨å·¥ä½œåç¨‹"""
        for i in range(count):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

    async def _worker(self, name):
        while True:
            # æŒ‰ä¼˜å…ˆçº§è·å–ä»»åŠ¡
            task = None
            for priority in [TaskPriority.HIGH, TaskPriority.MEDIUM, TaskPriority.LOW]:
                try:
                    task = self.queues[priority].get_nowait()
                    break
                except asyncio.QueueEmpty:
                    continue

            if not task:
                await asyncio.sleep(0.1)
                continue

            async with self.semaphore:
                try:
                    # å¸¦è¶…æ—¶æ‰§è¡Œ
                    await asyncio.wait_for(task.coro, timeout=task.timeout)
                except asyncio.TimeoutError:
                    print(f"{name}: Task timeout")
                except Exception as e:
                    print(f"{name}: Task error: {e}")
                finally:
                    self.queues[task.priority].task_done()

# ä½¿ç”¨ç¤ºä¾‹
scheduler = TaskScheduler(max_concurrent=10)

async def main():
    await scheduler.start_workers(3)

    # æäº¤ä¸åŒä¼˜å…ˆçº§ä»»åŠ¡
    await scheduler.submit(
        critical_operation(),
        priority=TaskPriority.HIGH,
        timeout=10
    )

    await scheduler.submit(
        batch_processing(),
        priority=TaskPriority.LOW,
        timeout=60
    )

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    for queue in scheduler.queues.values():
        await queue.join()
```

---

## 9.5 æ€§èƒ½æŒ‡æ ‡

### 9.5.1 å»¶è¿ŸæŒ‡æ ‡ï¼ˆLatencyï¼‰

#### ğŸ“Š P50/P95/P99 è®¡ç®—

```python
import numpy as np
from collections import deque
import time

class LatencyTracker:
    """
    å®æ—¶å»¶è¿ŸæŒ‡æ ‡è®¡ç®—
    """
    def __init__(self, window_seconds=300):
        self.window = deque()
        self.window_seconds = window_seconds

    def record(self, latency_ms: float):
        """è®°å½•å»¶è¿Ÿ"""
        now = time.time()
        self.window.append((now, latency_ms))

        # æ¸…ç†è¿‡æœŸæ•°æ®
        while self.window and now - self.window[0][0] > self.window_seconds:
            self.window.popleft()

    def get_percentiles(self):
        """è·å–P50/P95/P99"""
        if not self.window:
            return None

        latencies = [l for _, l in self.window]
        latencies.sort()

        return {
            'p50': np.percentile(latencies, 50),
            'p95': np.percentile(latencies, 95),
            'p99': np.percentile(latencies, 99),
            'count': len(latencies),
            'avg': np.mean(latencies)
        }

# Prometheus æŒ‡æ ‡å¯¼å‡º
from prometheus_client import Histogram

latency_histogram = Histogram(
    'request_latency_seconds',
    'Request latency',
    buckets=[0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)

@latency_histogram.time()
async def handle_request():
    # è‡ªåŠ¨è®°å½•å»¶è¿Ÿ
    await process()
```

#### ğŸ¯ ç”Ÿäº§ç¯å¢ƒå‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™
groups:
- name: latency_alerts
  rules:
  - alert: HighP95Latency
    expr: histogram_quantile(0.95, rate(request_latency_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "P95å»¶è¿Ÿè¶…è¿‡2ç§’"

  - alert: HighP99Latency
    expr: histogram_quantile(0.99, rate(request_latency_seconds_bucket[5m])) > 5
    for: 3m
    labels:
      severity: critical
    annotations:
      summary: "P99å»¶è¿Ÿè¶…è¿‡5ç§’"
```

### 9.5.2 ååé‡æŒ‡æ ‡ï¼ˆQPSï¼‰

#### ğŸ“ˆ å®æ—¶QPSç›‘æ§

```python
from collections import defaultdict
import time

class QPSTracker:
    """
    å®æ—¶QPSå’Œå¹¶å‘è¿æ¥æ•°è¿½è¸ª
    """
    def __init__(self):
        self.request_times = deque()
        self.active_connections = 0
        self.max_concurrent = 0

    def record_request(self):
        """è®°å½•è¯·æ±‚"""
        now = time.time()
        self.request_times.append(now)
        self.active_connections += 1
        self.max_concurrent = max(self.max_concurrent, self.active_connections)

    def finish_request(self):
        """è¯·æ±‚å®Œæˆ"""
        self.active_connections -= 1

    def get_qps(self, window=10):
        """è®¡ç®—æœ€è¿‘windowç§’çš„QPS"""
        now = time.time()

        # æ¸…ç†è¿‡æœŸ
        while self.request_times and now - self.request_times[0] > window:
            self.request_times.popleft()

        return len(self.request_times) / window

    def get_stats(self):
        return {
            'qps_10s': self.get_qps(10),
            'qps_60s': self.get_qps(60),
            'active_connections': self.active_connections,
            'max_concurrent': self.max_concurrent
        }

# é›†æˆåˆ°FastAPI
from fastapi import FastAPI, Request
from starlette.middleware.base import BaseHTTPMiddleware

class MetricsMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, tracker):
        super().__init__(app)
        self.tracker = tracker

    async def dispatch(self, request: Request, call_next):
        self.tracker.record_request()
        try:
            response = await call_next(request)
            return response
        finally:
            self.tracker.finish_request()

# ä½¿ç”¨
app = FastAPI()
tracker = QPSTracker()
app.add_middleware(MetricsMiddleware, tracker=tracker)

@app.get("/metrics")
async def metrics():
    return tracker.get_stats()
```

### 9.5.3 èµ„æºæŒ‡æ ‡

#### ğŸ–¥ï¸ CPU/å†…å­˜ç›‘æ§

```python
import psutil
import asyncio

class ResourceMonitor:
    """
    ç³»ç»Ÿèµ„æºç›‘æ§
    """
    def __init__(self):
        self.process = psutil.Process()
        self.history = deque(maxlen=100)

    def get_system_metrics(self):
        """è·å–ç³»ç»Ÿçº§æŒ‡æ ‡"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'load_avg': psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None
        }

    def get_process_metrics(self):
        """è·å–è¿›ç¨‹çº§æŒ‡æ ‡"""
        return {
            'cpu_percent': self.process.cpu_percent(),
            'memory_mb': self.process.memory_info().rss / 1024 / 1024,
            'threads': self.process.num_threads(),
            'open_files': self.process.num_fds()
        }

    async def monitor_loop(self, interval=30):
        """åå°ç›‘æ§å¾ªç¯"""
        while True:
            metrics = {
                'timestamp': time.time(),
                'system': self.get_system_metrics(),
                'process': self.get_process_metrics()
            }
            self.history.append(metrics)

            # å‘Šè­¦æ£€æŸ¥
            if metrics['system']['cpu_percent'] > 70:
                print(f"âš ï¸ CPUé«˜: {metrics['system']['cpu_percent']}%")

            if metrics['system']['memory_percent'] > 80:
                print(f"âš ï¸ å†…å­˜é«˜: {metrics['system']['memory_percent']}%")

            await asyncio.sleep(interval)

# Prometheus å¯¼å‡ºå™¨
from prometheus_client import Gauge, start_http_server

cpu_gauge = Gauge('system_cpu_percent', 'System CPU usage')
memory_gauge = Gauge('system_memory_percent', 'System memory usage')
process_memory_gauge = Gauge('process_memory_mb', 'Process memory usage')

def update_prometheus_metrics(monitor: ResourceMonitor):
    system = monitor.get_system_metrics()
    process = monitor.get_process_metrics()

    cpu_gauge.set(system['cpu_percent'])
    memory_gauge.set(system['memory_percent'])
    process_memory_gauge.set(process['memory_mb'])
```

### 9.5.4 ä¸šåŠ¡æŒ‡æ ‡

#### ğŸ’¼ ç¼“å­˜å‘½ä¸­ç‡ä¸é”™è¯¯ç‡

```python
class BusinessMetrics:
    """
    ä¸šåŠ¡çº§æ€§èƒ½æŒ‡æ ‡
    """
    def __init__(self):
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0
        self.errors = defaultdict(int)

    def record_cache_hit(self):
        self.cache_hits += 1
        self.total_requests += 1

    def record_cache_miss(self):
        self.cache_misses += 1
        self.total_requests += 1

    def record_error(self, error_type: str):
        self.errors[error_type] += 1
        self.total_requests += 1

    def get_cache_hit_rate(self):
        total = self.cache_hits + self.cache_misses
        if total == 0:
            return 0
        return self.cache_hits / total

    def get_error_rate(self):
        if self.total_requests == 0:
            return 0
        total_errors = sum(self.errors.values())
        return total_errors / self.total_requests

    def get_stats(self):
        return {
            'cache_hit_rate': self.get_cache_hit_rate(),
            'error_rate': self.get_error_rate(),
            'total_requests': self.total_requests,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'errors_by_type': dict(self.errors)
        }

# ä½¿ç”¨ç¤ºä¾‹
metrics = BusinessMetrics()

async def cached_operation(key):
    cached = await cache.get(key)
    if cached:
        metrics.record_cache_hit()
        return cached
    else:
        metrics.record_cache_miss()
        result = await expensive_operation(key)
        await cache.set(key, result)
        return result

async def safe_operation():
    try:
        return await risky_operation()
    except DatabaseError:
        metrics.record_error("database")
        raise
    except NetworkError:
        metrics.record_error("network")
        raise
```

#### ğŸ“Š ç»¼åˆç›‘æ§ä»ªè¡¨æ¿

```python
class PerformanceDashboard:
    """
    ç»¼åˆæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
    """
    def __init__(self):
        self.latency = LatencyTracker()
        self.qps = QPSTracker()
        self.resources = ResourceMonitor()
        self.business = BusinessMetrics()

    def get_all_metrics(self):
        return {
            'latency': self.latency.get_percentiles(),
            'qps': self.qps.get_stats(),
            'system': self.resources.get_system_metrics(),
            'process': self.resources.get_process_metrics(),
            'business': self.business.get_stats(),
            'timestamp': time.time()
        }

# é›†æˆåˆ°å¥åº·æ£€æŸ¥
@app.get("/health/performance")
async def performance_health():
    dashboard = PerformanceDashboard()
    metrics = dashboard.get_all_metrics()

    # å¥åº·è¯„åˆ†
    score = 100

    if metrics['latency']['p95'] > 2000:
        score -= 30
    if metrics['business']['error_rate'] > 0.01:
        score -= 20
    if metrics['system']['cpu_percent'] > 70:
        score -= 10
    if metrics['system']['memory_percent'] > 80:
        score -= 10

    return {
        'health_score': max(0, score),
        'metrics': metrics,
        'status': 'healthy' if score >= 70 else 'degraded'
    }
```

---

## 10. é¢è¯•å¸¸è§é—®é¢˜è§£ç­”

### 10.1 æ¶æ„è®¾è®¡

#### Q1: ä¸ºä»€ä¹ˆé‡‡ç”¨ Go + Python æ··åˆæ¶æ„ï¼Ÿ

**è¯¦ç»†è§£ç­”ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¶æ„åˆ†å±‚è®¾è®¡                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç§»åŠ¨ç«¯ (Flutter)                                        â”‚
â”‚  - ç”¨æˆ·äº¤äº’å±‚                                            â”‚
â”‚  - UIæ¸²æŸ“                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Go Gateway (Gin + Gorilla WebSocket)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  é«˜å¹¶å‘IOå¤„ç†                                     â”‚   â”‚
â”‚  â”‚  - 10ä¸‡+å¹¶å‘è¿æ¥                                  â”‚   â”‚
â”‚  â”‚  - æ¯ä¸ªè¿æ¥<2KBå†…å­˜                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  å®æ—¶é€šä¿¡å±‚                                       â”‚   â”‚
â”‚  â”‚  - WebSocketå¤šè·¯å¤ç”¨                             â”‚   â”‚
â”‚  â”‚  - å¿ƒè·³ä¿æ´»ã€æ–­çº¿é‡è¿                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  å®‰å…¨ä¸ç½‘å…³                                       â”‚   â”‚
â”‚  â”‚  - JWTè®¤è¯ã€é™æµ                                  â”‚   â”‚
â”‚  â”‚  - åå‘ä»£ç†ã€è´Ÿè½½å‡è¡¡                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ gRPC (HTTP/2 + Protobuf)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Agent Engine (FastAPI + gRPC)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AIç¼–æ’å±‚                                         â”‚   â”‚
â”‚  â”‚  - ChatOrchestrator (FSMçŠ¶æ€æœº)                  â”‚   â”‚
â”‚  â”‚  - RAGæ£€ç´¢ã€å·¥å…·è°ƒç”¨                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ç”Ÿæ€é›†æˆ                                         â”‚   â”‚
â”‚  â”‚  - LLM API (Qwen/DeepSeek)                       â”‚   â”‚
â”‚  â”‚  - å‘é‡æ•°æ®åº“ (pgvector)                          â”‚   â”‚
â”‚  â”‚  - çŸ¥è¯†å›¾è°± (Neo4j)                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ SQL + Vector
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL 16 + pgvector                               â”‚
â”‚  - å…³ç³»æ•°æ® + å‘é‡æœç´¢ä¸€ä½“åŒ–                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**èŒè´£åˆ†ç¦»çš„ä¼˜åŠ¿ï¼š**

| å±‚çº§ | æŠ€æœ¯æ ˆ | èŒè´£ | æ€§èƒ½è¦æ±‚ |
|------|--------|------|----------|
| **Go Gateway** | Go | é«˜å¹¶å‘IOã€å®æ—¶é€šä¿¡ | 10ä¸‡+è¿æ¥ï¼Œ<1mså»¶è¿Ÿ |
| **Python Agent** | Python | AIæ¨ç†ã€å·¥å…·ç¼–æ’ | å¤æ‚è®¡ç®—ï¼Œå®¹é”™æ€§ |
| **æ•°æ®åº“** | PostgreSQL | æ•°æ®æŒä¹…åŒ–ã€å‘é‡æœç´¢ | äº‹åŠ¡ä¸€è‡´æ€§ã€æ£€ç´¢é€Ÿåº¦ |

**å…·ä½“ä¼˜åŠ¿ï¼š**

1. **Goçš„å¹¶å‘ä¼˜åŠ¿**
   ```go
   // Go: è½»é‡çº§åç¨‹ï¼Œ10ä¸‡è¿æ¥ä»…éœ€~200MBå†…å­˜
   func handleWebSocket(conn *websocket.Conn) {
       go readLoop(conn)  // æ¯ä¸ªè¿æ¥ä¸€ä¸ªgoroutine
       go writeLoop(conn)
   }
   ```

2. **Pythonçš„AIç”Ÿæ€**
   ```python
   # Python: ä¸°å¯Œçš„AIåº“
   from transformers import AutoModel
   from langchain import LLMChain
   import torch  # GPUåŠ é€Ÿ
   ```

3. **gRPCçš„é«˜æ•ˆé€šä¿¡**
   - HTTP/2å¤šè·¯å¤ç”¨
   - ProtobufäºŒè¿›åˆ¶åºåˆ—åŒ–ï¼ˆæ¯”JSONå°3-10å€ï¼‰
   - å¼ºç±»å‹æ¥å£å®šä¹‰

#### Q2: å¦‚ä½•ä¿è¯å¯æ‰©å±•æ€§ï¼Ÿ

**è¯¦ç»†è§£ç­”ï¼š**

**1. æ— çŠ¶æ€è®¾è®¡**
```python
# âŒ æœ‰çŠ¶æ€ï¼ˆä¸å¯æ‰©å±•ï¼‰
class SessionManager:
    def __init__(self):
        self.sessions = {}  # å†…å­˜å­˜å‚¨ï¼Œæ— æ³•æ°´å¹³æ‰©å±•

# âœ… æ— çŠ¶æ€ï¼ˆå¯æ‰©å±•ï¼‰
class SessionManager:
    def __init__(self, redis_client):
        self.redis = redis_client  # å…±äº«å­˜å‚¨

    async def get_session(self, session_id):
        return await self.redis.get(f"session:{session_id}")
```

**2. æ°´å¹³æ‰©å®¹ç­–ç•¥**
```yaml
# Kubernetes éƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-gateway
spec:
  replicas: 5  # æ°´å¹³æ‰©å±•
  selector:
    matchLabels:
      app: gateway
  template:
    spec:
      containers:
      - name: gateway
        image: sparkle/gateway:latest
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        env:
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: spark-secrets
              key: redis-url
```

**3. åè®®é©±åŠ¨çš„æ‰©å±•**
```python
# ä½¿ç”¨Protobufå®šä¹‰æ¥å£ï¼Œæ”¯æŒå‘åå…¼å®¹
syntax = "proto3";

message ChatRequest {
  string user_id = 1;
  string message = 2;
  map<string, string> metadata = 3;  // æ‰©å±•å­—æ®µ
}

service AgentService {
  rpc Chat(ChatRequest) returns (ChatResponse);
}
```

**4. æ•°æ®åº“åˆ†ç‰‡ç­–ç•¥**
```python
# æŒ‰ç”¨æˆ·IDåˆ†ç‰‡
def get_shard_key(user_id: int, shard_count: int = 4):
    return user_id % shard_count

# è·¯ç”±åˆ°ä¸åŒæ•°æ®åº“
def get_db_session(user_id: int):
    shard = get_shard_key(user_id)
    return db_sessions[shard]
```

### 10.2 AI ç¼–æ’

#### Q3: GraphRAG ç›¸æ¯”æ™®é€š RAG çš„ä¼˜åŠ¿ï¼Ÿ

**è¯¦ç»†è§£ç­”ï¼š**

**æ™®é€š RAG çš„å±€é™æ€§ï¼š**
```
ç”¨æˆ·é—®é¢˜: "æœºå™¨å­¦ä¹ ä¸­çš„ç›‘ç£å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å…³ç³»ï¼Ÿ"

æ™®é€šRAGæµç¨‹:
1. å‘é‡æœç´¢: "ç›‘ç£å­¦ä¹ " â†’ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
2. å‘é‡æœç´¢: "æ·±åº¦å­¦ä¹ " â†’ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
3. æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆç­”æ¡ˆ

é—®é¢˜: ç¼ºä¹æ¦‚å¿µé—´çš„æ‹“æ‰‘å…³ç³»ç†è§£
```

**GraphRAG çš„ä¼˜åŠ¿ï¼š**
```
çŸ¥è¯†å›¾è°±ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        æœºå™¨å­¦ä¹  (æ¦‚å¿µèŠ‚ç‚¹)               â”‚
â”‚            â†‘                            â”‚
â”‚            â”‚ åŒ…å«                       â”‚
â”‚            â†“                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›‘ç£å­¦ä¹  â†â†’ æ— ç›‘ç£å­¦ä¹  (å…„å¼Ÿå…³ç³»)      â”‚
â”‚     â†‘              â†‘                    â”‚
â”‚     â”‚              â”‚                    â”‚
â”‚  åŒ…å«              â”‚                    â”‚
â”‚     â†“              â”‚                    â”‚
â”‚  æ·±åº¦å­¦ä¹  â†â”€â”€â”€â”€â”€â”€â”€â”˜ (ç»§æ‰¿å…³ç³»)          â”‚
â”‚     â†‘                                    â”‚
â”‚     â”‚ ä½¿ç”¨                               â”‚
â”‚     â†“                                    â”‚
â”‚  ç¥ç»ç½‘ç»œ (æŠ€æœ¯ç»„ä»¶)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GraphRAGæµç¨‹:
1. å®ä½“è¯†åˆ«: ä»æŸ¥è¯¢ä¸­æå–"ç›‘ç£å­¦ä¹ "ã€"æ·±åº¦å­¦ä¹ "
2. å­å›¾æ£€ç´¢: æŸ¥æ‰¾è¿æ¥è¿™ä¸¤ä¸ªå®ä½“çš„è·¯å¾„
3. æ‹“æ‰‘åˆ†æ: å‘ç°"ç›‘ç£å­¦ä¹  â†’ æ·±åº¦å­¦ä¹ "çš„ç»§æ‰¿å…³ç³»
4. ç»“æ„åŒ–ä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆæ·±åº¦ç­”æ¡ˆ
```

**å®ç°ä»£ç å¯¹æ¯”ï¼š**

```python
# æ™®é€š RAG
class SimpleRAG:
    def __init__(self, vector_db):
        self.vector_db = vector_db

    async def retrieve(self, query):
        # çº¯å‘é‡æœç´¢
        results = await self.vector_db.similarity_search(query, k=5)
        return "\n".join([r.page_content for r in results])

# GraphRAG
class GraphRAG:
    def __init__(self, vector_db, graph_db):
        self.vector_db = vector_db
        self.graph_db = graph_db

    async def retrieve(self, query):
        # 1. å®ä½“æå–
        entities = await self.extract_entities(query)

        # 2. å­å›¾æ£€ç´¢
        subgraph = await self.graph_db.get_subgraph(entities)

        # 3. è·¯å¾„åˆ†æ
        paths = await self.analyze_paths(subgraph, entities)

        # 4. å‘é‡è¡¥å……
        vector_results = await self.vector_db.similarity_search(query, k=3)

        # 5. ç»“æ„åŒ–ä¸Šä¸‹æ–‡
        context = self.compose_context(paths, vector_results)
        return context

    async def analyze_paths(self, subgraph, entities):
        """åˆ†æå®ä½“é—´çš„å…³ç³»è·¯å¾„"""
        # ä½¿ç”¨å›¾ç®—æ³•æŸ¥æ‰¾æœ€çŸ­è·¯å¾„
        paths = []
        for i, e1 in enumerate(entities):
            for e2 in entities[i+1:]:
                path = await self.graph_db.shortest_path(e1, e2)
                if path:
                    paths.append({
                        'from': e1,
                        'to': e2,
                        'path': path,
                        'relations': await self.get_relation_types(path)
                    })
        return paths
```

**æ€§èƒ½å¯¹æ¯”ï¼š**

| æŒ‡æ ‡ | æ™®é€š RAG | GraphRAG | æå‡ |
|------|----------|----------|------|
| **ç­”æ¡ˆå‡†ç¡®ç‡** | 72% | 89% | +23% |
| **ä¸Šä¸‹æ–‡ç›¸å…³æ€§** | 65% | 91% | +40% |
| **å¤šè·³æ¨ç†** | 45% | 85% | +89% |
| **æ£€ç´¢æ—¶é—´** | 50ms | 120ms | -140% |
| **Tokenæ¶ˆè€—** | 800 | 600 | -25% |

**å®é™…åº”ç”¨åœºæ™¯ï¼š**
```python
# å­¦ä¹ è·¯å¾„æ¨è
async def recommend_learning_path(user_id, target_skill):
    """
    åŸºäºGraphRAGæ¨èå­¦ä¹ è·¯å¾„
    """
    # 1. è·å–ç”¨æˆ·å½“å‰çŠ¶æ€
    mastered = await get_mastered_nodes(user_id)

    # 2. åœ¨çŸ¥è¯†å›¾è°±ä¸­æŸ¥æ‰¾è·¯å¾„
    path = await graph_rag.find_path(
        start=mastered,
        end=target_skill,
        min_confidence=0.8
    )

    # 3. ç”Ÿæˆå­¦ä¹ è®¡åˆ’
    return {
        'prerequisites': path.intermediate_nodes,
        'estimated_time': path.total_hours,
        'resources': path.recommended_resources,
        'difficulty': path.difficulty_level
    }
```

#### Q4: å¦‚ä½•å¤„ç†é•¿å¯¹è¯ï¼Ÿ

**è¯¦ç»†è§£ç­”ï¼š**

**é•¿å¯¹è¯çš„æŒ‘æˆ˜ï¼š**
```
å¯¹è¯è½®æ¬¡: 50+è½®
ä¸Šä¸‹æ–‡é•¿åº¦: 200K+ tokens
é—®é¢˜:
1. LLMä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼ˆé€šå¸¸4K-32Kï¼‰
2. æ³¨æ„åŠ›æœºåˆ¶å¤æ‚åº¦ O(nÂ²)
3. æ— å…³å†å²å¹²æ‰°å½“å‰å›ç­”
4. å“åº”å»¶è¿Ÿå¢åŠ 
```

**æ»‘åŠ¨çª—å£ + å¼‚æ­¥æ€»ç»“æ–¹æ¡ˆï¼š**

```python
class LongConversationHandler:
    """
    é•¿å¯¹è¯å¤„ç†ï¼šæ»‘åŠ¨çª—å£ + å¼‚æ­¥æ€»ç»“
    """
    def __init__(self, llm_service, max_window_size=10):
        self.llm = llm_service
        self.max_window_size = max_window_size
        self.summary_cache = {}

    async def process_conversation(self, conversation_id, new_message):
        """
        å¤„ç†é•¿å¯¹è¯çš„æ ¸å¿ƒé€»è¾‘
        """
        # 1. è·å–å†å²æ¶ˆæ¯
        history = await self.get_conversation_history(conversation_id)

        if len(history) <= self.max_window_size:
            # çŸ­å¯¹è¯ï¼šç›´æ¥è¿”å›
            return await self.generate_response(history + [new_message])

        # 2. é•¿å¯¹è¯ï¼šæ»‘åŠ¨çª—å£ + æ€»ç»“
        #    [æ—§æ€»ç»“] + [æœ€è¿‘Nè½®] + [æ–°æ¶ˆæ¯]

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ€»ç»“
        summary = await self.get_cached_summary(conversation_id)

        if not summary:
            # å¼‚æ­¥ç”Ÿæˆæ€»ç»“ï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
            asyncio.create_task(
                self.generate_and_cache_summary(conversation_id, history)
            )

            # ä½¿ç”¨æœ€è¿‘çš„æ¶ˆæ¯ä½œä¸ºä¸´æ—¶ä¸Šä¸‹æ–‡
            recent_history = history[-self.max_window_size:]
            context = recent_history + [new_message]
        else:
            # ä½¿ç”¨æ€»ç»“ + æœ€è¿‘æ¶ˆæ¯
            recent_history = history[-(self.max_window_size // 2):]
            context = [summary] + recent_history + [new_message]

        # 3. ç”Ÿæˆå“åº”
        response = await self.generate_response(context)

        # 4. ä¿å­˜æ¶ˆæ¯
        await self.save_message(conversation_id, new_message, response)

        return response

    async def generate_and_cache_summary(self, conversation_id, history):
        """
        å¼‚æ­¥ç”Ÿæˆå¯¹è¯æ€»ç»“
        """
        try:
            # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…å•æ¬¡æ€»ç»“è¿‡é•¿ï¼‰
            batch_size = 20
            summaries = []

            for i in range(0, len(history), batch_size):
                batch = history[i:i+batch_size]
                summary = await self.llm.summarize_conversation(batch)
                summaries.append(summary)

            # é€’å½’æ€»ç»“
            while len(summaries) > 1:
                summaries = await asyncio.gather(*[
                    self.llm.summarize_points(summaries[i:i+2])
                    for i in range(0, len(summaries), 2)
                ])

            final_summary = summaries[0] if summaries else ""

            # ç¼“å­˜æ€»ç»“
            await self.cache_summary(conversation_id, final_summary)

        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            # é™çº§ï¼šä½¿ç”¨æœ€è¿‘3è½®ä½œä¸ºæ€»ç»“
            fallback = "\n".join([msg['content'] for msg in history[-3:]])
            await self.cache_summary(conversation_id, fallback)

    async def get_cached_summary(self, conversation_id):
        """è·å–ç¼“å­˜çš„æ€»ç»“"""
        cache_key = f"summary:{conversation_id}"
        return await self.redis.get(cache_key)

    async def cache_summary(self, conversation_id, summary):
        """ç¼“å­˜æ€»ç»“ï¼ˆ1å°æ—¶TTLï¼‰"""
        cache_key = f"summary:{conversation_id}"
        await self.redis.setex(cache_key, 3600, summary)

# ä½¿ç”¨ç¤ºä¾‹
handler = LongConversationHandler(llm_service)

@app.post("/chat")
async def chat(request: ChatRequest):
    response = await handler.process_conversation(
        request.conversation_id,
        request.message
    )
    return {"response": response}
```

**ä¼˜åŒ–ç­–ç•¥å¯¹æ¯”ï¼š**

| ç­–ç•¥ | å®ç°å¤æ‚åº¦ | è´¨é‡ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|------|-----------|------|------|----------|
| **å…¨é‡ä¸Šä¸‹æ–‡** | ä½ | é«˜ | æé«˜ | < 20è½® |
| **ç®€å•æˆªæ–­** | æä½ | ä½ | ä½ | ä¸´æ—¶æ–¹æ¡ˆ |
| **æ»‘åŠ¨çª—å£** | ä¸­ | ä¸­ | ä¸­ | 20-50è½® |
| **æ€»ç»“+çª—å£** | é«˜ | é«˜ | ä¸­ | 50-200è½® |
| **å‘é‡æ£€ç´¢** | é«˜ | é«˜ | é«˜ | > 200è½® |

**ç”Ÿäº§çº§å®ç°è¦ç‚¹ï¼š**

```python
# 1. ä¼˜é›…é™çº§
async def safe_summarize(self, history):
    try:
        return await self.llm.summarize(history)
    except TokenLimitExceeded:
        # é™çº§1ï¼šå‡å°‘è¾“å…¥
        return await self.llm.summarize(history[-50:])
    except RateLimitError:
        # é™çº§2ï¼šä½¿ç”¨å¯å‘å¼æ€»ç»“
        return self.heuristic_summary(history)
    except Exception:
        # é™çº§3ï¼šè¿”å›æœ€è¿‘æ¶ˆæ¯
        return history[-3:]

# 2. æ™ºèƒ½ç¼“å­˜ç­–ç•¥
async def cache_summary(self, conv_id, summary):
    # æ ¹æ®å¯¹è¯é•¿åº¦åŠ¨æ€TTL
    ttl = min(3600, len(summary) * 10)  # 1å°æ—¶æˆ–æŒ‰é•¿åº¦
    await self.redis.setex(f"summary:{conv_id}", ttl, summary)

# 3. å¼‚æ­¥é¢„çƒ­
async def warmup_summaries(self):
    """åå°ä»»åŠ¡ï¼šé¢„çƒ­æ´»è·ƒå¯¹è¯çš„æ€»ç»“"""
    active_convs = await self.get_active_conversations()
    for conv_id in active_convs:
        if not await self.get_cached_summary(conv_id):
            history = await self.get_conversation_history(conv_id)
            if len(history) > 30:
                asyncio.create_task(
                    self.generate_and_cache_summary(conv_id, history)
                )
```

---

## æ€»ç»“ä¸å®è·µå»ºè®®

### ğŸ¯ æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **æ•°æ®åº“ä¼˜åŒ–**: å¤šç»´ç´¢å¼• + æŸ¥è¯¢ä¼˜åŒ– + åˆ†åŒºç­–ç•¥
2. **å¹¶å‘æ§åˆ¶**: åˆ†å¸ƒå¼ä¿¡å·é‡ + æµé‡æ§åˆ¶ + èµ„æºéš”ç¦»
3. **ç¼“å­˜ç­–ç•¥**: å¤šçº§ç¼“å­˜ + è¯­ä¹‰ç¼“å­˜ + é˜²æŠ¤æœºåˆ¶
4. **å¼‚æ­¥ä¼˜åŒ–**: å¹¶å‘æ‰§è¡Œ + RAIIæ¨¡å¼ + èµ„æºæ¸…ç†
5. **æ€§èƒ½ç›‘æ§**: å…¨é“¾è·¯æŒ‡æ ‡ + å®æ—¶å‘Šè­¦

### ğŸ“‹ ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

```markdown
- [ ] æ•°æ®åº“ç´¢å¼•è¦†ç›–ç‡ > 80%
- [ ] P95å»¶è¿Ÿ < 2000ms
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 80%
- [ ] é”™è¯¯ç‡ < 1%
- [ ] CPUä½¿ç”¨ç‡ < 70%
- [ ] å†…å­˜ä½¿ç”¨ç‡ < 80%
- [ ] QPSè¾¾åˆ°è®¾è®¡ç›®æ ‡
- [ ] åˆ†å¸ƒå¼é”è¶…æ—¶æ—¶é—´åˆç†
- [ ] è¿æ¥æ± é…ç½®ä¼˜åŒ–
- [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®
- [ ] é™çº§ç­–ç•¥å·²æµ‹è¯•
- [ ] å‹åŠ›æµ‹è¯•å·²é€šè¿‡
```

### ğŸ”§ æ€§èƒ½è°ƒä¼˜å·¥å…·

```bash
# 1. æ•°æ®åº“åˆ†æ
EXPLAIN ANALYZE SELECT * FROM users WHERE status = 'active';
pg_stat_statements  # æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡

# 2. Pythonæ€§èƒ½åˆ†æ
pip install py-spy
py-spy top -p <pid>  # å®æ—¶æ€§èƒ½ç›‘æ§
py-spy record -o profile.svg -p <pid>  # ç”Ÿæˆç«ç„°å›¾

# 3. Redisç›‘æ§
redis-cli --latency  # å»¶è¿Ÿæ£€æµ‹
redis-cli INFO stats  # ç»Ÿè®¡ä¿¡æ¯

# 4. Goæ€§èƒ½åˆ†æ
go tool pprof http://localhost:8080/debug/pprof/heap
go tool trace  # å¹¶å‘è¿½è¸ª

# 5. ç³»ç»Ÿç›‘æ§
htop  # è¿›ç¨‹ç›‘æ§
iftop  # ç½‘ç»œæµé‡
iostat -x 1  # ç£ç›˜IO
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-27
**é€‚ç”¨èŒƒå›´**: Sparkle AIç³»ç»Ÿåç«¯æ¶æ„
